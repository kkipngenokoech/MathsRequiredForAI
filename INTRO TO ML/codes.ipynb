{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION WITH REGULARIZATION AND ONLINE UPDATES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions below to solve a regression problem using ridge regression with varying regularization\n",
    "parameters (λ) and analyze the bias-variance trade-off. Use only the ‘numpy’ library for computations and\n",
    "‘matplotlib’ for plotting. When uploading to Gradescope, you will need to produce a PDF version of your\n",
    "solutions and code. One way to do this is to use a notebook (https://jupyter.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Regularized Linear Regression\n",
    "In many real-world applications, predictive models often encounter challenges such as multicollinearity among features, small sample sizes, or noisy data. These challenges can lead to overfitting, where the model captures noise instead of meaningful patterns. To address this, regularized linear regression, such as ridge regression, introduces a penalty term to the loss function, which helps constrain the model complexity and improve generalization.\n",
    "\n",
    "Consider the following scenarios where regularized linear regression plays a critical role:\n",
    "\n",
    "- __Financial Forecasting__: Predicting stock prices or company revenues often involves highly correlated features, such as economic indicators or market trends. Ridge regression can reduce the impact of multicollinearity, ensuring stable and interpretable predictions.\n",
    "- __Healthcare Analytics__: In clinical studies with limited patient data, predictive models for treatment outcomes may suffer from high variance due to noise. Regularization helps to avoid overfitting by penalizing extreme weight values.\n",
    "- __Marketing Campaign Analysis__: Estimating the impact of various advertisement strategies on sales can involve sparse and noisy data. Regularization ensures that the model remains robust despite inconsistencies in the data.\n",
    "\n",
    "In this problem, you will explore ridge regression on synthetic data to understand its behavior and implications for real-world applications. Specifically, you will:\n",
    "\n",
    "- __Analyze the behavior of learned coefficients__: Observe how the learned coefficients vary as the regularization parameter λ changes, helping to illustrate how ridge regression stabilizes the model\n",
    "- __Assess model performance__: Measure the model’s performance using Root Mean Squared Error (RMSE) on validation data to identify the optimal regularization parameter λ\n",
    "- __Investigate the bias-variance trade-off__: Quantify and visualize how regularization influences bias and variance, providing insights into the trade-offs inherent in predictive modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SYNTHETIC DATA GENERATION\n",
    "\n",
    "Generate a synthetic dataset for training, validation, and testing. The dataset consists of N = 500 data points and d = 12 features. Each data point is generated as follows:\n",
    "\n",
    "$y = x^T + \\epsilon$\n",
    "\n",
    "where  $ x \\sim N(0, 1)$, $\\textbf{w}$ is a vector of length d with linearly spaced values between 1 and 5, and $\\epsilon \\sim N(0, 0.5^2)$\n",
    "\n",
    "split the data into:\n",
    "\n",
    "- __Training set (70%)__: For training the model.\n",
    "- __Validation set (15%)__: For selecting the optimal λ.\n",
    "-  __Test set (15%)__: For final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate synthetic data\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "N = 500 # Total data points\n",
    "d = 12 # Number of features\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "# Generate feature matrix and true weights\n",
    "X = np.random.normal(0, 1, (N, d))\n",
    "true_weights = np.linspace(1, 5, d) # Linearly spaced true weights\n",
    "epsilon = np.random.normal(0, 0.5, N) # Noise\n",
    "y = X @ true_weights + epsilon # Generate target values\n",
    "# Split data into train, validation, and test sets\n",
    "train_size = int(N * train_ratio)\n",
    "val_size = int(N * val_ratio)\n",
    "test_size = N - train_size - val_size\n",
    "X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]\n",
    "y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASKS\n",
    "\n",
    "### Plot coefficients  vrs $\\lambda$:\n",
    "\n",
    "- Train ridge regression models for $\\lambda \\in \\{a \\cdot 10^b : a \\in \\{1, 2, \\ldots, 9\\}, b \\in \\{-5, -4, \\ldots, 2\\}\\}$.\n",
    "- Plot the learned coefficients ($w_i$) for all 12 features and the bias term against $\\lambda$ (use a log scale on the vertical axis).\n",
    "\n",
    "### Validate RMSE vs $\\lambda$:\n",
    "\n",
    "- Calculate the RMSE (root mean squared error) on the validation set for each λ.\n",
    "- Plot the validation RMSE against λ (use a logarithmic scale on the vertical axis) and identify $\\lambda^*$ the value that minimizes the RMSE on the validation dataset.\n",
    "\n",
    "###  Predicted vs. True Values:\n",
    "\n",
    "- Use $\\lambda^*$ to train the model on the combined training and validation sets.\n",
    "- Plot the predicted values against the true values for the test set. Your result should be a scatter plot with each point representing a (predicted value, true value) pair for one data point in the test set.\n",
    "\n",
    "### Bias-Variance Trade-off:\n",
    "\n",
    "- Generate L = 20 independent training datasets of size $N_{sub}$ = 50 by sampling with replacement from the training data.\n",
    "- Train models for each dataset and calculate the bias and variance for each λ.\n",
    "- Plot the variance against λ (use a logarithmic scale for the vertical axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- Use gradient descent to solve ridge regression:\n",
    "\n",
    "$$\n",
    "w^* = \\arg\\min_w \\|y - Xw\\|_2^2 + \\lambda \\|w\\|_2^2\n",
    "$$\n",
    "\n",
    "- Initialize \\( w = 0 \\), and use a learning rate of 0.01 with a stopping criterion of \\( 10^{-6} \\).\n",
    "- For sampling with replacement, use `numpy.random.choice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Ridge regression functions\n",
    "def ridge_loss(w, X, y, lam):\n",
    "    \"\"\"Calculate the ridge regression loss.\"\"\"\n",
    "    residuals = y - X @ w\n",
    "    return NotImplemented\n",
    "def ridge_gradient(w, X, y, lam):\n",
    "    \"\"\"Calculate the gradient of the ridge regression loss.\"\"\"\n",
    "    residuals = y - X @ w\n",
    "    grad = NotImplemented\n",
    "    return grad\n",
    "def gradient_descent(loss_fn, grad_fn, w_init, X, y, lam, lr=0.01, tol=1e-6, max_iters=1000):\n",
    "    \"\"\"Perform gradient descent to minimize the ridge regression loss.\"\"\"\n",
    "    w = w_init\n",
    "    for i in range(max_iters):\n",
    "        grad = grad_fn(w, X, y, lam)\n",
    "        w_new = w - lr * grad\n",
    "        if np.linalg.norm(w_new - w, ord=2) < tol:\n",
    "            break\n",
    "        w = w_new\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Variance and bias calculation\n",
    "def calculate_bias_variance(X_train, y_train, X_val, y_val, lambdas, num_datasets=20, sub_sample_size=50):\n",
    "    \"\"\"\n",
    "    Calculate the bias and variance for ridge regression models trained on multiple datasets.\n",
    "    \"\"\"\n",
    "    biases, variances = [], []\n",
    "    for lam in lambdas:\n",
    "        predictions = []\n",
    "        for _ in range(num_datasets):\n",
    "            # Sample with replacement\n",
    "            indices = np.random.choice(len(X_train), size=sub_sample_size, replace=True)\n",
    "            X_sample, y_sample = X_train[indices], y_train[indices]\n",
    "            # Train ridge regression\n",
    "            w_init = np.zeros(d)\n",
    "            w = gradient_descent(ridge_loss, ridge_gradient, w_init, X_sample, y_sample, lam)\n",
    "            # Predict on validation data\n",
    "            predictions.append(X_val @ w)\n",
    "        # Average predictions\n",
    "        predictions = np.array(predictions)\n",
    "        mean_prediction = np.mean(predictions, axis=0)\n",
    "        bias = np.mean((mean_prediction - y_val)**2)\n",
    "        variance = np.mean(np.var(predictions, axis=0))\n",
    "        biases.append(bias)\n",
    "        variances.append(variance)\n",
    "    return biases, variances\n",
    "\n",
    "# Empty sections for students to complete\n",
    "def plot_coefficients_vs_lambda():\n",
    "    pass\n",
    "def plot_rmse_vs_lambda():\n",
    "    pass\n",
    "def plot_predicted_vs_true():\n",
    "    pass\n",
    "def plot_bias_variance_tradeoff():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "\n",
    "- Python code for all tasks, including plots.\n",
    "-  Saved figures:\n",
    "    - coefficients vs lambda.png\n",
    "    - rmse vs lambda.png\n",
    "    - predicted vs true.png\n",
    "    - bias variance tradeoff.png\n",
    "- Analysis discussing:\n",
    "    - How coefficients behave as λ increases.\n",
    "    - The trade-off between RMSE and λ.\n",
    "    - Observations from the bias-variance trade-off plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "- [10 points] Correct implementation of ridge regression and bias-variance analysis.\n",
    "- [4 points] Visualization of coefficients with respect to λ.\n",
    "- [6 points] Validation RMSE plot and identification of $\\lambda^*$\n",
    "- [4 points] Scatter plot for predictions versus true values.\n",
    "- [6 points] Plot and meaningful analysis of bias-variance trade-off with respect to λ.\n",
    "- *Make sure that all figures, plots, and diagrams referenced in your work are embedded directly in the PDF file you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvMaths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
