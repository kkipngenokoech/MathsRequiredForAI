\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{positioning, automata}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage[a4paper, margin=1in]{geometry}


\newcommand{\R}{\mathbb{R}}

\pagestyle{fancy}
\begin{document}


\author{kipngeno koech - bkoech}
\title{Homework 4 - Applied Stochastic Processes}   
\maketitle

\medskip

\section*{Question 1: MoM, MLE, Bias and Consistency (20 points)}

Consider a normal distribution defined by the probability density function (PDF):

\[
f(y; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y - \mu)^2}{2\sigma^2}}, \quad -\infty < y < \infty,
\]

\noindent where \(\mu\) is the mean and \(\sigma^2\) is the variance. Given a random sample \(Y = \{ y_1, y_2, \ldots, y_n \}\) drawn from this normal distribution, perform the following tasks:

\begin{enumerate}
    \item[(a)] \textbf{(5 points)} Use the method of moments to derive the estimators for \(\mu\) and \(\sigma^2\).
    \\ The mean and the variance of the normal distribution are given by:
    \[
    \mu = E[Y], \quad \sigma^2 = \text{Var}(Y)
    \]
    The method of moments estimates are obtained by setting the sample moments equal to the population moments:
    \[
    \bar{y} = \frac{1}{n} \sum_{i=1}^n y_i = \mu
    \]
    \[
    \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 = \sigma^2
    \]
    Thus, the method of moments estimators are:
    \[
    \hat{\mu}_{\text{MoM}} = \bar{y}, \quad \hat{\sigma}^2_{\text{MoM}} = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2
    \]
    
    
     \item[(b)] \textbf{(5 points)} Derive the Maximum Likelihood Estimators (MLE) for \(\mu\) and \(\sigma^2\).
    the likelihood function is given by:
    \[
    L(\mu, \sigma^2) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_i - \mu)^2}{2\sigma^2}}
    \]
    expanding the likelihood function:
    \[
    \ell(\mu, \sigma^2) = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n e^{-\frac{1}{2\sigma^2} \sum_{i=1}^{n} (y_i - \mu)^2}
    \]
    \[
    = \left( \frac{1}{\sqrt{2\pi\sigma^2}} \right)^n e^{-\frac{1}{2\sigma^2} \left( \sum_{i=1}^{n} y_i^2 - 2\mu \sum_{i=1}^{n} y_i + n\mu^2 \right)}
    \]
    \[
    \ell(\mu, \sigma^2) = -\frac{n}{2} \ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - \mu)^2
    \]
    Derivative with respect to \( \mu \):
    \[
    \frac{\partial \ell}{\partial \mu} = -\frac{1}{2\sigma^2} \cdot 2 \sum_{i=1}^n (y_i - \mu) = -\frac{1}{\sigma^2} \sum_{i=1}^n (y_i - \mu) = 0
    \]
    \[
    \sum_{i=1}^n y_i - n\mu = 0 \implies \mu = \frac{1}{n} \sum_{i=1}^n y_i = \bar{y}
    \]
    Derivative with respect to \( \sigma^2 \):
    \[
    \frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (y_i - \mu)^2 = 0
    \]
    \[
    \sigma^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \mu)^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2
    \]
    \[
    \hat{\mu}_{\text{MLE}} = \bar{y}, \quad \hat{\sigma}^2_{\text{MLE}} = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2
    \]

     \item[(c)] \textbf{(3 points)} Calculate the bias of the MoM estimators \(\hat{\mu}_{\text{MoM}}\) and \(\hat{\sigma}^2_{\text{MoM}}\).
     The bias of \( \hat{\sigma}^2_{\text{MoM}} \) is:

\[
\text{Bias}(\hat{\sigma}^2_{\text{MoM}}) = \mathbb{E}[\hat{\sigma}^2_{\text{MoM}}] - \sigma^2
\]


\[
\mathbb{E}[\hat{\sigma}^2_{\text{MoM}}] = \sigma^2 \cdot \frac{n-1}{n}
\]

Thus, the bias is:

\[
\text{Bias}(\hat{\sigma}^2_{\text{MoM}}) = \sigma^2 \cdot \frac{n-1}{n} - \sigma^2 = -\frac{\sigma^2}{n}
\]

Hence, \( \hat{\sigma}^2_{\text{MoM}} \) is biased and underestimates \( \sigma^2 \).

\item[(d)] \textbf{(3 points)} Calculate the bias of the MLE estimators \(\hat{\mu}_{\text{MLE}}\) and \(\hat{\sigma}^2_{\text{MLE}}\).
\[
\text{Bias}(\hat{\mu}_{\text{MLE}}) = \mathbb{E}[\hat{\mu}_{\text{MLE}}] - \mu
\]
\[
\text{Bias}(\hat{\sigma}^2_{\text{MLE}}) = \mathbb{E}[\hat{\sigma}^2_{\text{MLE}}] - \sigma^2
\]
\[
\mathbb{E}[\hat{\mu}_{\text{MLE}}] = \mu, \quad \mathbb{E}[\hat{\sigma}^2_{\text{MLE}}] = \sigma^2
\]
\[
\text{Bias}(\hat{\mu}_{\text{MLE}}) = 0, \quad \text{Bias}(\hat{\sigma}^2_{\text{MLE}}) = 0
\]
    \item[(e)] \textbf{(4 points)} Show that both the MoM and MLE estimators are consistent, meaning that as \(n \to \infty\), \(\hat{\mu}_{\text{MoM}} \to \mu\), \(\hat{\sigma}^2_{\text{MoM}} \to \sigma^2\), \(\hat{\mu}_{\text{MLE}} \to \mu\), and \(\hat{\sigma}^2_{\text{MLE}} \to \sigma^2\) in probability.
\end{enumerate}
for the MoM estimators:
\[
\lim_{n \to \infty} \hat{\mu}_{\text{MoM}} = \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n y_i = \mu
\]
\[
\lim_{n \to \infty} \hat{\sigma}^2_{\text{MoM}} = \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n (y_i - \hat{\mu}_{\text{MoM}})^2 = \sigma^2
\]
for the MLE estimators:
\[
\lim_{n \to \infty} \hat{\mu}_{\text{MLE}} = \lim_{n \to \infty} \bar{y} = \mu
\]
\[
\lim_{n \to \infty} \hat{\sigma}^2_{\text{MLE}} = \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 = \sigma^2
\]
Thus, both the MoM and MLE estimators are consistent.
    
  








\section*{Question 2: Spam-Ham Detection Using MLE and MAP (30 points)}
In digital communication, distinguishing spam from ham (non-spam) is crucial for email security. Statistical techniques such as Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation are effective for classification. This section aims to build a spam-ham classifier using both MLE and MAP methods.

\subsection*{Part 1: MLE/MAP on Toy Dataset (10 points)}
You are provided with a mini dataset containing six SMS messages labeled as either spam or ham. A single feature, ``offer,'' indicates the presence (1) or absence (0) of the word ``offer'' in each message.

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|l|c|c|}
        \hline
        \textbf{Message ID} & \textbf{Message Content} & \textbf{``Offer'' (X)} & \textbf{Class (Y)} \\
        \hline
        1 & ``Special offer now!'' & 1 & 1 (Spam) \\
        2 & ``Meeting at 10 AM'' & 0 & 0 (Ham) \\
        3 & ``Claim your offer'' & 1 & 1 (Spam) \\
        4 & ``Lunch tomorrow?'' & 0 & 0 (Ham) \\
        5 & ``Free offer available!'' & 1 & 1 (Spam) \\
        6 & ``Hello, how are you?'' & 0 & 0 (Ham) \\
        \hline
    \end{tabular}
    \caption{Mini Dataset for Spam-Ham Detection}
\end{table}

\noindent Calculate the following MLE estimates:
\begin{itemize}
    \item[(a)] \textbf{(1 point)} $\pi = P(Y = 1)$: Probability that a message is spam.
    \[
    \hat{\pi}_{\text{MLE}} = \frac{\text{Number of spam messages}}{\text{Total number of messages}} = \frac{3}{6} = \mathbf{\frac{1}{2}}
    \]
    \item[(b)] \textbf{(1 point)} $\theta_{\text{spam}} = P(X = 1 \mid Y = 1)$: Probability that ``offer'' appears in a spam message.
    \[
    \hat{\theta}_{\text{spam}} = \frac{\text{Number of spam messages with ``offer''}}{\text{Total number of spam messages}} = \frac{3}{3} = \mathbf{1}
    \]
    \item[(c)] \textbf{(1 point)} $\theta_{\text{ham}} = P(X = 1 \mid Y = 0)$: Probability that ``offer'' appears in a ham message.
    \[
    \hat{\theta}_{\text{ham}} = \frac{\text{Number of ham messages with ``offer''}}{\text{Total number of ham messages}} = \frac{0}{3} = \mathbf{0}
    \]
    \item[(d)] \textbf{(2 points)} Derive the likelihood function and maximize it to find the parameter estimates. 
    \[
    \hat{\pi}_{\text{MLE}} = \frac{3}{6} = \mathbf{\frac{1}{2}}, \quad \hat{\theta}_{\text{spam}} = 1, \quad \hat{\theta}_{\text{ham}} = 0
    \]
\end{itemize}

\noindent Assume Beta priors:
\begin{itemize}
    \item[(e)] \textbf{(2 points)} $\pi \sim \text{Beta}(2, 2)$
    \[
    P(\pi) = \frac{1}{B(2, 2)} \pi^{2-1} (1 - \pi)^{2-1} = 6\pi(1 - \pi)
    \]
    \[
    P(\pi \mid \text{data}) \propto P(\text{data} \mid \pi) P(\pi) = \pi^3 (1 - \pi)^3
    \]
    \[
    \hat{\pi}_{\text{MAP}} = \frac{3}{6} = \mathbf{\frac{1}{2}}
    \]
    \item[(f)] \textbf{(2 points)} $\theta_{\text{spam}} \sim \text{Beta}(2, 1)$
    \[
    P(\theta_{\text{spam}}) = \frac{1}{B(2, 1)} \theta_{\text{spam}}^{2-1} (1 - \theta_{\text{spam}})^{1-1} = 2\theta_{\text{spam}}
    \]
    \[
    P(\theta_{\text{spam}} \mid \text{data}) \propto P(\text{data} \mid \theta_{\text{spam}}) P(\theta_{\text{spam}}) = \theta_{\text{spam}}^3 (1 - \theta_{\text{spam}})
    \]
    \[
    \hat{\theta}_{\text{spam, MAP}} = \frac{3}{3} = \mathbf{1}
    \]
    \item[(g)] \textbf{(2 points)} $\theta_{\text{ham}} \sim \text{Beta}(1, 2)$
    \[
    P(\theta_{\text{ham}}) = \frac{1}{B(1, 2)} \theta_{\text{ham}}^{1-1} (1 - \theta_{\text{ham}})^{2-1} = 2(1 - \theta_{\text{ham}})
    \]
    \[
    P(\theta_{\text{ham}} \mid \text{data}) \propto P(\text{data} \mid \theta_{\text{ham}}) P(\theta_{\text{ham}}) = (1 - \theta_{\text{ham}})^3
    \]
    \[
    \hat{\theta}_{\text{ham, MAP}} = \frac{0}{3} = \mathbf{0}
    \]
\end{itemize}

\noindent Calculate the MAP estimates for $\pi$, $\theta_{\text{spam}}$, and $\theta_{\text{ham}}$ using prior information. 


\subsection*{Part 2: Practical Implications (4 points)}
Discuss the following:
\begin{enumerate}
    \item[(a)] \textbf{(1 point)} How do different prior choices affect MAP estimates?
    \\\\ A prior that aligns with the true underlying distribution can lead to more accurate estimates, while a less informative prior may introduce bias or uncertainty in the estimates.
    \item[(b)] \textbf{(1 point)} Why might MLE overfit with small datasets?
    \\\\ MLE tends to overfit with small datasets because it maximizes the likelihood of the observed data, which can lead to high variance and poor generalization to new data.
    \item[(c)] \textbf{(1 point)} In what scenarios would MLE or MAP perform better?
    \\\\ MLE is preferred when prior information is limited or unnecessary, while MAP is beneficial when incorporating prior knowledge can improve estimation accuracy.
    \item[(d)] \textbf{(1 point)} What is the bias-variance trade-off between MLE and MAP?
    \\\\ MLE tends to have lower bias but higher variance, while MAP can introduce bias but reduce variance by incorporating prior information.
\end{enumerate}

\subsection*{Part 3: Real-World Implementation (10 points)}
In this exercise, you will classify messages as either "spam" or "ham" (not spam) using a Naive Bayes classifier. You will implement two different estimation methods. Use the ``SMS Spam Collection'' dataset, available at \href{https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection}{this link}, to implement a spam-ham detection classifier using:

\begin{itemize}
    \item \textbf{Maximum Likelihood Estimation (MLE)}: Estimates the parameters based solely on the training data without prior beliefs about the parameters.
    \item \textbf{Maximum A Posteriori (MAP)}: Incorporates prior beliefs about the parameters into the estimation, using Laplace smoothing to handle zero probabilities.
\end{itemize}

"

\subsection*{Tasks}

\subsubsection*{Step 1: Data Loading and Preprocessing}
\begin{enumerate}
    \item \textbf{Load the Dataset}:
    \begin{itemize}
        \item Download the SMS Spam Collection dataset and load it into your environment using pandas.
        \item Ensure the dataset is read correctly, with columns labeled "label" for spam/ham and "message" for the text content.
    \end{itemize}
    
    \item \textbf{Preprocess the Text Messages}:
    \begin{itemize}
        \item Convert all text to lowercase.
        \item Remove punctuation and special characters.
        \item Tokenize the messages (split the messages into words).
    \end{itemize}
    Example Preprocessing Steps:
    \begin{itemize}
        \item Lowercasing: Convert ``Hello!'' to ``hello''.
        \item Remove punctuation: Convert ``This is spam!!!'' to ``this is spam''.
    \end{itemize}
    
    \item \textbf{Split the Dataset}:
    \begin{itemize}
        \item Divide the dataset into training and test sets (e.g., 80\% training, 20\% testing).
        \item Ensure that both sets maintain the same class distribution.
    \end{itemize}
\end{enumerate}

\subsubsection*{Step 2: Implement Maximum Likelihood Estimator (MLE)}
\begin{enumerate}
    \item \textbf{Calculate Probabilities}:
    \begin{itemize}
        \item For each class (spam and ham), calculate the probability of each word appearing in that class based on the training data.
    \end{itemize}
    
    \item \textbf{Implement Prediction Function}:
    \begin{itemize}
        \item Create a function to classify messages using the calculated probabilities and the prior probabilities of each class.
    \end{itemize}
    
    \item \textbf{Evaluate the Classifier}:
    \begin{itemize}
        \item Use metrics such as accuracy, precision, recall, and F1 score to evaluate the performance of the MLE classifier on the test set.
    \end{itemize}
\end{enumerate}

\subsubsection*{Step 3: Implement Maximum A Posteriori (MAP)}
\begin{enumerate}
    \item \textbf{Implement MAP Estimator}:
    \begin{itemize}
        \item Calculate the same probabilities as in MLE but include Laplace smoothing to avoid zero probabilities.
    \end{itemize}
    
    \item \textbf{Implement Prediction Function}:
    \begin{itemize}
        \item Create a prediction function similar to MLE but using the MAP probabilities.
    \end{itemize}
    
    \item \textbf{Evaluate the Classifier}:
    \begin{itemize}
        \item Again, use accuracy, precision, recall, and F1 score to evaluate the MAP classifier's performance on the test set.
    \end{itemize}
\end{enumerate}

\subsubsection*{Step 4: Compare Results}
\begin{enumerate}
    \item \textbf{Performance Comparison}:
    \begin{itemize}
        \item Create a comparison table that summarizes the accuracy, precision, recall, and F1 score for both classifiers.
    \end{itemize}
    
    \item \textbf{Discussion}:
    \begin{itemize}
        \item Reflect on the differences in performance:
        \begin{itemize}
            \item How did incorporating prior knowledge in MAP affect the predictions?
            \item Were there any significant changes in the classification of messages between MLE and MAP?
            \item What factors might account for any differences in the performance metrics?
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsubsection*{Step 5: Vary the Prior (MAP)}
\begin{enumerate}
    \item \textbf{Experiment with Different Alpha Values}:
    \begin{itemize}
        \item Run the MAP classifier with varying values of the Laplace smoothing parameter (alpha) such as 0.1, 0.5, 1, and 5.
        \item Observe how these variations affect the results and the evaluation metrics.
    \end{itemize}
    
    \item \textbf{Discussion of Findings}:
    \begin{itemize}
        \item Summarize your observations regarding the impact of varying the prior on the classification performance.
    \end{itemize}
\end{enumerate}

\subsection*{Deliverables}
Prepare a Jupyter Notebook containing the following:
\begin{itemize}
    \item Data loading and preprocessing steps.
    \item MLE and MAP classifier implementations with detailed comments.
    \item Evaluation metrics for both classifiers.
    \item A comparison table summarizing the performance metrics.
    \item A discussion section reflecting on your observations and insights gained from the exercise.
\end{itemize}

\section*{Reflection}
Conclude the exercise by reflecting on the differences between MLE and MAP:
\begin{itemize}
    \item Discuss scenarios where one might prefer MLE over MAP and vice versa.
    \item Reflect on the impact of incorporating prior information into the estimation process.
    \item Consider how understanding these differences might be beneficial in real-world applications.
\end{itemize}










\section*{Question 3: Blind Estimation of a Corrupted Variable (20 points)}
In signal processing, estimating a hidden signal corrupted by noise is a common challenge. This problem focuses on estimating a random variable \( X \), which is corrupted by Gaussian noise \( N \). The observed variable \( Y \) is given by:
\[
Y = X + N,
\]
where \( N \sim N(0, \sigma^2) \) is Gaussian noise with zero mean and variance \( \sigma^2 \), and is uncorrelated with \( X \).
\subsection*{Part 1: Analytical Derivations (10 points)}
\begin{enumerate}
    \item[(a)] \textbf{(2 points)} Derive the mean and variance of \(Y\).
    \\ the mean of \(Y\) is given by:
    \[
    E[Y] = E[X + N] = E[X] + E[N] = E[X] + 0 = E[X]
    \]
    \[
     = \textbf{E[X]}
    \]
    the variance of \(Y\) is given by:
    \[
    \text{Var}(Y) = \text{Var}(X + N) = \text{Var}(X) + \text{Var}(N) = \text{Var}(X) + \sigma^2
    \]
    \item[(b)] \textbf{(3 points)} Derive the Best Linear Estimator of \( X \)
    \[
    \hat{X} = aY + b
    \]
    where \( a \) and \( b \) are constants to be determined.
    \\ the best linear estimator is given by:
    \[
    \text{MSE}(\hat{X}) = E[(X - \hat{X})^2] = E[(X - aY - b)^2] 
    \]
    \[
    = E[X^2 - 2aXY - 2bX + a^2Y^2 + 2abY + b^2]
    \]
    
    to determine the constants \( a \) and \( b \), we minimize the MSE by taking the derivative with respect to \( a \) and \( b \) and setting them to zero.
    derivative with respect to \( a \):
    \[
    \frac{\partial \text{MSE}}{\partial a} = -2E[XY] + 2aE[Y^2] + 2bE[Y] = 0
    \]
    derivative with respect to \( b \):
    \[
    \frac{\partial \text{MSE}}{\partial b} = -2E[X] + 2aE[Y] + 2b = 0
    \]
    solving the above equations, we get:
    \[
    a = \frac{E[XY] - E[X]E[Y]}{E[Y^2] - E[Y]^2}, \quad b = E[X] - aE[Y]
    \]
    \[
    \hat{X} = \frac{E[XY] - E[X]E[Y]}{E[Y^2] - E[Y]^2}Y + E[X] - \frac{E[XY] - E[X]E[Y]}{E[Y^2] - E[Y]^2}E[Y]
    \]

    
    \item[(c)] \textbf{(2 points)} The orthogonality principle states that the error \( \epsilon = X - \hat{X} \) must be uncorrelated with \( Y \), i.e.:
    
    Substitute \( \hat{X} \) into the expression and derive equations for \( a \) and \( b \) that minimize the Mean Squared Error (MSE).
    The orthogonality principle states that the error $\epsilon = X - \hat{X}$ must be uncorrelated with $Y$:

    \[E[\epsilon Y] = 0\]
    
    Substituting $\hat{X} = aY + b$ into the error expression:
    
    \[\epsilon = X - (aY + b)\]
    
    Now, we can write the orthogonality condition:
    
    \[E[(X - (aY + b))Y] = 0\]
    
    Expanding this equation:
    
    \[E[XY - aY^2 - bY] = 0\]
    
    This can be rewritten as:
    
    \[E[XY] - aE[Y^2] - bE[Y] = 0 \quad \text{(Equation 1)}\]
    
    For minimizing MSE, we need another equation. We can use the fact that the expected value of the error should be zero:
    
    \[E[X - (aY + b)] = 0\]
    
    This simplifies to:
    
    \[E[X] - aE[Y] - b = 0 \quad \text{(Equation 2)}\]
    
    Now we have two equations to solve for $a$ and $b$:
    
    \begin{align*}
    \text{Equation 1:} & \quad E[XY] - aE[Y^2] - bE[Y] = 0 \\
    \text{Equation 2:} & \quad E[X] - aE[Y] - b = 0
    \end{align*}
    
    From Equation 2:
    \[b = E[X] - aE[Y]\]
    
    Substituting this into Equation 1:
    
    \[E[XY] - aE[Y^2] - (E[X] - aE[Y])E[Y] = 0\]
    
    \[E[XY] - aE[Y^2] - E[X]E[Y] + aE[Y]^2 = 0\]
    
    \[E[XY] - E[X]E[Y] = a(E[Y^2] - E[Y]^2)\]
    
    Solving for $a$:
    
    \[a = \frac{E[XY] - E[X]E[Y]}{E[Y^2] - E[Y]^2}\]
    
    And then we can find $b$:
    
    \[b = E[X] - aE[Y]\]
    
    Therefore, the equations that minimize the MSE are:
    
    \begin{align*}
    a &= \frac{E[XY] - E[X]E[Y]}{E[Y^2] - E[Y]^2} \\
    b &= E[X] - aE[Y]
    \end{align*}
    
    \item[(d)] \textbf{(1 point)} Define bias as:
    \[
    \text{Bias}(\hat{X}) = E[\hat{X}] - E[X].
    \]


    
    Show that the estimator \( \hat{X} \) derived above is unbiased.
\[\text{Bias}(\hat{X}) = E[\hat{X}] - E[X]\]

To show $\hat{X} = aY + b$ is unbiased, we prove $\text{Bias}(\hat{X}) = 0$.


\begin{align*}
E[\hat{X}] &= E[aY + b] \\
&= aE[Y] + b \\
&= \frac{E[XY] - E[X]E[Y]}{E[Y^2] - E[Y]^2} E[Y] + E[X] - \frac{E[XY] - E[X]E[Y]}{E[Y^2] - E[Y]^2} E[Y] \\
&= E[X]
\end{align*}

Therefore,
\[\text{Bias}(\hat{X}) = E[\hat{X}] - E[X] = E[X] - E[X] = 0\]


Thus, the estimator $\hat{X}$ is unbiased.    
\item (2 points ) Define the Mean Squared Error (MSE) as:
\[
    MSE(\hat{X}) = E[(X - \hat{X})^2].
\]
Using the derived values of a and b, calculate the MSE and verify that it is minimized for the
derived linear estimator

\begin{align*}
MSE(\hat{X}) &= E[(X - (aY + b))^2] \\
&= E[X^2] - 2aE[XY] - 2bE[X] + a^2E[Y^2] + 2abE[Y] + b^2
\end{align*}

Minimizing MSE:
\begin{align*}
\frac{\partial MSE}{\partial a} &= -2E[XY] + 2aE[Y^2] + 2bE[Y] = 0 \\
\frac{\partial MSE}{\partial b} &= -2E[X] + 2aE[Y] + 2b = 0
\end{align*}

These yield our previously derived optimal $a$ and $b$. Substituting back:

\[MSE(\hat{X}) = Var(X) - \frac{Cov(X,Y)^2}{Var(Y)}\]

\end{enumerate}

\subsection*{Part 2: Practical Implementation with the California Housing Dataset (10 points)}

\subsection*{Dataset Overview}
The California Housing Dataset is a widely used dataset that contains information about various attributes of houses in California, including median house values, median income, housing age, and more. We will focus on estimating the original value of a key variable that has been corrupted by artificial noise.

\subsection*{Step-by-Step Implementation}

\textbf{Load the Dataset:}
\begin{itemize}
    \item Use the \texttt{sklearn.datasets} module to load the California Housing Dataset.
\end{itemize}

\textbf{Introduce Noise:}
\begin{itemize}
    \item Select a variable, such as the "Average Number of Rooms" (AveRooms), and add Gaussian noise to simulate corruption. 
    \item Use a Gaussian noise distribution with mean $\mu = 0$ and standard deviation $\sigma = 0.5$.
    \item The observed variable $Y$ can be represented as:
    \[
    Y = X + N, \quad \text{where } N \sim N(0, 0.5^2)
    \]
\end{itemize}

\textbf{Downsample the Data:}
\begin{itemize}
    \item After splitting the dataset, randomly select 200 points from the test set to reduce clutter in visualizations and focus on key trends. 
\end{itemize}

\textbf{Train-Test Split:}
\begin{itemize}
    \item Split the data into training and testing sets using an 80-20 split ratio. This means 80\% of the data will be used for training and 20\% for testing.
\end{itemize}

\textbf{Apply the Best Linear Estimator:}
\begin{itemize}
    \item Implement the best linear estimator (e.g., linear regression) to estimate the original values of the corrupted variable.
\end{itemize}

\textbf{Evaluate the Estimator:}
\begin{itemize}
    \item Calculate the bias and Mean Squared Error (MSE) of the estimator. Analyze how close the estimated values are to the original values.
\end{itemize}

\textbf{Visualize the Results:}
\begin{itemize}
    \item Plot the original, noise-corrupted, and estimated values of the selected variable. The plots should clearly distinguish between these values to facilitate comparison.
\end{itemize}

\subsection*{Discussion}
In this implementation, consider the following questions to guide your discussion:

\begin{itemize}
    \item \textbf{Bias and MSE:} What do the calculated bias and Mean Squared Error (MSE) reveal about the accuracy of our estimates? How might these metrics inform our understanding of model performance?

    \item \textbf{Impact of Noise:} In what ways does the introduction of Gaussian noise reflect real-world measurement errors? How do the estimates change as a result of this noise, and what implications does this have for data integrity?

    \item \textbf{Model Limitations:} What are the limitations of using linear regression in this context? Are there scenarios where a more complex model might provide better estimates? What factors should be considered when choosing a modeling approach?
\end{itemize}
 
\[
\textbf{[ in the notebook ]}
\]




\section*{Question 4: Mixture Models and the EM Algorithm (30 points)}

\begin{enumerate}
    \item Consider a dataset that is generated from a mixture of two Gaussian distributions. The first component has mean \(\mu_1 = 0\) and variance \(\sigma_1^2 = 1\), and the second component has mean \(\mu_2 = 5\) and variance \(\sigma_2^2 = 2\). The mixing proportions are \(\pi_1 = 0.3\) and \(\pi_2 = 0.7\).
    \begin{enumerate}
        \item[(a)] \textbf{(2 points)} Write down the probability density function of the mixture model.
        \[
        f(x) = \pi_1 \mathcal{N}(x | \mu_1, \sigma_1^2) + \pi_2 \mathcal{N}(x | \mu_2, \sigma_2^2)
        \]
        \[
        f(x) = 0.3 \mathcal{N}(x | 0, 1) + 0.7 \mathcal{N}(x | 5, 2)
        \]
        \[
        f(x) = 0.3 \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{x^2}{2}} + 0.7 \frac{1}{\sqrt{4\pi\sigma^2}} e^{-\frac{(x-5)^2}{8}}
        \]
        \item[(b)] \textbf{(4 points} Derive the Expectation-Maximization (EM) algorithm steps for estimating the parameters of this mixture model.
        \[
        \text{E-step: } Q(\theta | \theta^{(t)}) = E_{Z|X, \theta^{(t)}}[\log L(\theta; X, Z)] = \sum_{Z} P(Z | X, \theta^{(t)}) \log L(\theta; X, Z)
        \]
        \[
        \text{E-step: } P(Z | X, \theta^{(t)}) = \frac{P(X | Z, \theta^{(t)}) P(Z | \theta^{(t)})}{P(X | \theta^{(t)})}
        \]
        where \( Z \) is the latent variable representing the component from which the data point \( X \) is generated.
        \\ for the first component:
        \[
        P(Z = 1 | X, \theta^{(t)}) = \frac{\pi_1 \mathcal{N}(X | \mu_1, \sigma_1^2)}{\pi_1 \mathcal{N}(X | \mu_1, \sigma_1^2) + \pi_2 \mathcal{N}(X | \mu_2, \sigma_2^2)}
        \]
        \[
        = \frac{0.3 \mathcal{N}(X | 0, 1)}{0.3 \mathcal{N}(X | 0, 1) + 0.7 \mathcal{N}(X | 5, 2)}
        \]
        and for the second component:
        \[
        P(Z = 2 | X, \theta^{(t)}) = \frac{\pi_2 \mathcal{N}(X | \mu_2, \sigma_2^2)}{\pi_1 \mathcal{N}(X | \mu_1, \sigma_1^2) + \pi_2 \mathcal{N}(X | \mu_2, \sigma_2^2)}
        \]
        \[
        = \frac{0.7 \mathcal{N}(X | 5, 2)}{0.3 \mathcal{N}(X | 0, 1) + 0.7 \mathcal{N}(X | 5, 2)}
        \]
        The M-step involves maximizing the expected log-likelihood with respect to the parameters:
        \[
        \text{M-step: } \theta^{(t+1)} = \arg\max_{\theta} Q(\theta | \theta^{(t)})
        \]
        \[
        \text{M-step: } \pi_k = \frac{1}{n} \sum_{i=1}^{n} P(Z = k | X_i, \theta^{(t)})
        \]
        \[
        \text{M-step: } \mu_k = \frac{\sum_{i=1}^{n} P(Z = k | X_i, \theta^{(t)}) X_i}{\sum_{i=1}^{n} P(Z = k | X_i, \theta^{(t)})}
        \]
        \[
        \text{M-step: } \sigma_k^2 = \frac{\sum_{i=1}^{n} P(Z = k | X_i, \theta^{(t)}) (X_i - \mu_k)^2}{\sum_{i=1}^{n} P(Z = k | X_i, \theta^{(t)})}
        \]
        so, for the first component:
        \[
        \pi_1 = \frac{1}{n} \sum_{i=1}^{n} P(Z = 1 | X_i, \theta^{(t)})
        \]
        \[
        \mu_1 = \frac{\sum_{i=1}^{n} P(Z = 1 | X_i, \theta^{(t)}) X_i}{\sum_{i=1}^{n} P(Z = 1 | X_i, \theta^{(t)})}
        \]
        \[
        \sigma_1^2 = \frac{\sum_{i=1}^{n} P(Z = 1 | X_i, \theta^{(t)}) (X_i - \mu_1)^2}{\sum_{i=1}^{n} P(Z = 1 | X_i, \theta^{(t)})}
        \]
        and for the second component:
        \[
        \pi_2 = \frac{1}{n} \sum_{i=1}^{n} P(Z = 2 | X_i, \theta^{(t)})
        \]
        \[
        \mu_2 = \frac{\sum_{i=1}^{n} P(Z = 2 | X_i, \theta^{(t)}) X_i}{\sum_{i=1}^{n} P(Z = 2 | X_i, \theta^{(t)})}
        \]
        \[
        \sigma_2^2 = \frac{\sum_{i=1}^{n} P(Z = 2 | X_i, \theta^{(t)}) (X_i - \mu_2)^2}{\sum_{i=1}^{n} P(Z = 2 | X_i, \theta^{(t)})}
        \]
        \item[(c)] \textbf{(6 points)} Generate a synthetic dataset of 1000 samples from this mixture model using Python and implement the EM algorithm in Python and estimate the parameters of the mixture model from the synthetic dataset.
        \[
            \textbf{[ in the notebook ]}
        \]
        \item[(d)] \textbf{(2 points)}(2 points) Compare the estimated parameters with the true parameters and discuss the results.
    \end{enumerate}
    
    \item \textbf{(6 points)} Consider a set of data points \(x = \{1, 2, 3, 6, 7, 8\}\) which we assume come from a mixture of two Gaussian distributions. Use the Expectation-Maximization (EM) algorithm to fit a Gaussian Mixture Model (GMM) to this data. Start with the following initial parameters:

    \begin{itemize}
        \item Means: \(\mu_1 = 2\), \(\mu_2 = 7\)
        \item Variances: \(\sigma_1^2 = 1\), \(\sigma_2^2 = 1\)
        \item Mixing coefficients: \(\pi_1 = 0.5\), \(\pi_2 = 0.5\)
    \end{itemize}

    Perform the EM algorithm manually for three iterations and report the updated parameters \(\mu_1\), \(\mu_2\), \(\sigma_1^2\), \(\sigma_2^2\), \(\pi_1\), and \(\pi_2\) after each iteration. Use the following Gaussian probability density function for calculations:
    
    \begin{align*}
    \mathcal{N}(x | \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)
    \end{align*}
for each iteration, I need to compute:
    \begin{itemize}
        \item E-step: Compute the probability that each data point belongs to each cluster.
        \item M-step: Update the parameters based on the expected cluster assignments.
        \item Repeat the E-step and M-step for a specified number of iterations or until convergence.
    \end{itemize}

    \textbf{Iteration 1:}
x = 1:
the probability that x belongs to cluster 1 is given by:
\[
P(Z = 1 | X = 1) = \frac{\pi_1 \mathcal{N}(1 | \mu_1, \sigma_1^2)}{\pi_1 \mathcal{N}(1 | \mu_1, \sigma_1^2) + \pi_2 \mathcal{N}(1 | \mu_2, \sigma_2^2)}
\]
from the above formula we need to calculate three things:
\[
\mathcal{N}(1 | 2, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(1-2)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}} = \textbf{0.24197}
\]
\[
\mathcal{N}(1 | 7, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(1-7)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{36}{2}} = \frac{1}{\sqrt{2\pi}} e^{-18} = \textbf{0}
\]
\[
P(Z = 1 | X = 1) = \frac{0.5 \times 0.24197}{0.5 \times 0.24197 + 0.5 \times 0} = \frac{0.120985}{0.120985 + 0} = \textbf{1}
\]
Then, the probability that x belongs to cluster 2 is given by:
\[
P(Z = 2 | X = 1) = \frac{\pi_2 \mathcal{N}(1 | \mu_2, \sigma_2^2)}{\pi_1 \mathcal{N}(1 | \mu_1, \sigma_1^2) + \pi_2 \mathcal{N}(1 | \mu_2, \sigma_2^2)}
\]
\[
P(Z = 2 | X = 1) = \frac{0.5 \times 0}{0.5 \times 0.24197 + 0.5 \times 0} = \frac{0}{0.120985 + 0} = \textbf{0}
\]
so, the probability that x belongs to cluster 1 is 1 and to cluster 2 is 0.
\[
\gamma_{11}  = \textbf{1}, \quad \gamma_{12}  = \textbf{0}
\]

For x = 2:
\\ let us calculate the PMFs:
\[
\mathcal{N}(2 | 2, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(2-2)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{0}{2}} = \frac{1}{\sqrt{2\pi}} e^{0} = \frac{1}{\sqrt{2\pi}} = \textbf{0.398942}
\]
\[
\mathcal{N}(2 | 7, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(2-7)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{25}{2}} = \frac{1}{\sqrt{2\pi}} e^{-12.5} = \textbf{0.00000148656}
\]
so, the probability that x=2 belongs to cluster 1 is:
\[
P(Z = 1 | X = 2) = \frac{0.5 \times 0.398942}{0.5 \times 0.398942 + 0.5 \times 0.00000148656} = \frac{0.199471}{0.199471 + 0.00000074328} = \textbf{0.999996}
\]
\[
P(Z = 2 | X = 2) = \frac{0.5 \times 0.00000148656}{0.5 \times 0.398942 + 0.5 \times 0.00000148656} = \frac{0.00000148656}{0.199471 + 0.00000074328} = \textbf{0.00002439}
\]
so: $\gamma_{21} = \textbf{0.999996}, \quad \gamma_{22} = \textbf{0.00002439}$

For x = 3:
\\ we find the PMFs:
\[
\mathcal{N}(3 | 2, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(3-2)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}} = \textbf{0.241945}
\]
\[
\mathcal{N}(3 | 7, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(3-7)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{16}{2}} = \frac{1}{\sqrt{2\pi}} e^{-8} = \textbf{0.0001338}
\]
so, the probability that x=3 belongs to cluster 1 is:
\[
P(Z = 1 | X = 3) = \frac{0.5 \times 0.241945}{0.5 \times 0.241945 + 0.5 \times 0.0001338} = \frac{0.120985}{0.120985 + 0.00006691473} = 0.99943
\]
and, the probability that x=3 belongs to cluster 2 is:
\[
P(Z = 2 | X = 3) = \frac{0.5 \times 0.0001338}{0.5 \times 0.241945 + 0.5 \times 0.0001338} = \frac{0.00006691473}{0.120985 + 0.00006691473} = 0.00055
\]
so: $\gamma_{31} = \textbf{0.99943}, \quad \gamma_{32} = \textbf{0.00055}$
\\ for x = 6:
\\ we find the PMFs:
\[
\mathcal{N}(6 | 2, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(6-2)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{16}{2}} = \frac{1}{\sqrt{2\pi}} e^{-8} = \textbf{0.00013382946}
\]
\[
\mathcal{N}(6 | 7, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(6-7)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}} = \textbf{0.24197}
\]  
so, the probability that x=6 belongs to cluster 1 is:
\[
P(Z = 1 | X = 6) = \frac{0.5 \times 0.00013382946}{0.5 \times 0.00013382946 + 0.5 \times 0.24197} = \frac{0.00006691473}{0.00006691473 + 0.120985} = \textbf{0.00055}
\]
and, the probability that x=6 belongs to cluster 2 is:
\[
P(Z = 2 | X = 6) = \frac{0.5 \times 0.24197}{0.5 \times 0.00013382946 + 0.5 \times 0.24197} = \frac{0.120985}{0.00006691473 + 0.120985} = \textbf{0.99943}
\]
so: $\gamma_{41} = \textbf{0.00055}, \quad \gamma_{42} = \textbf{0.99943}$
\\ for x = 7:
\\ we find the PMFs:
\[
\mathcal{N}(7 | 2, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(7-2)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{25}{2}} = \frac{1}{\sqrt{2\pi}} e^{-12.5} = \textbf{0.00000148656}
\]
\[
\mathcal{N}(7 | 7, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(7-7)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{0}{2}} = \frac{1}{\sqrt{2\pi}} e^{0} = \frac{1}{\sqrt{2\pi}} = \textbf{0.398942}
\]
so, the probability that x=7 belongs to cluster 1 is:   
\[
P(Z = 1 | X = 7) = \frac{0.5 \times 0.00000503046}{0.5 \times 0.00000503046 + 0.5 \times 0.398942} = \frac{0.00000251523}{0.00000251523 + 0.199471} = \textbf{0.00002439}
\]
and, the probability that x=7 belongs to cluster 2 is:
\[
P(Z = 2 | X = 7) = \frac{0.5 \times 0.398942}{0.5 \times 0.00000503046 + 0.5 \times 0.398942} = \frac{0.199471}{0.00000251523 + 0.199471} = \textbf{0.398942}
\]
so, $\gamma_{51} = \textbf{0.00002439}, \quad \gamma_{52} = \textbf{0.999996}$
\\ for x = 8:
\\ we find the PMFs:
\[
\mathcal{N}(8 | 2, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(8-2)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{36}{2}} = \frac{1}{\sqrt{2\pi}} e^{-18} = 0
\]
\[
\mathcal{N}(8 | 7, 1) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(8-7)^2}{2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}} = 0.24197
\]
so, the probability that x=8 belongs to cluster 1 is:
\[
P(Z = 1 | X = 8) = \frac{0.5 \times 0}{0.5 \times 0 + 0.5 \times 0.24197} = \frac{0}{0 + 0.120985} = 0
\]
and, the probability that x=8 belongs to cluster 2 is:
\[
P(Z = 2 | X = 8) = \frac{0.5 \times 0.24197}{0.5 \times 0 + 0.5 \times 0.24197} = \frac{0.120985}{0 + 0.120985} = 1
\]
so, $\gamma_{61} = \textbf{0}, \quad \gamma_{62} = \textbf{1}$
\\ Here is a table of the probabilities (responsibilities) that each data point belongs to each cluster:
\[
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Data Point} & \textbf{Cluster 1} & \textbf{Cluster 2} \\
\hline
1 & 1 & 0 \\

2 & 0.999996 & 0.00002439 \\
3 & 0.99943 & 0.00055 \\
6 & 0.00055 & 0.99943 \\
7 & 0.00002439 & 0.999996 \\
8 & 0 & 1 \\
sum & 3.00000 & 3.00000 \\
\hline
\end{tabular}
\]
\\ now, we can calculate the updated parameters after the first iteration:
\[
\pi_1 = \frac{1}{6} \times (1 + 0.999996 + 0.99943 + 0.00055 + 0.00002439 + 0) = \frac{2.999426}{6} = \textbf{0.500004}
\]
\[
\pi_2 = \frac{1}{6} \times (0 + 0.00002439 + 0.00055 + 0.99943 + 0.999996 + 1) = \frac{2.999996}{6} = \textbf{0.500004}
\]
\[
\mu_1 = \frac{1}{2.999426} \times (1 \times 1 + 0.999996 \times 2 + 0.99943 \times 3 + 0.00055 \times 6 + 0.00002439 \times 7 + 0 \times 8) 
\]
\[
    = \frac{1 + 1.999992 + 2.99829 + 0.0033 + 0.00017517}{3.0000} = \frac{6.000175}{3.0000} = \textbf{2.000058}
\]
\[
\mu_2 = \frac{1}{2.999996} \times (0 \times 1 + 0.00002439 \times 2 + 0.00055 \times 3 + 0.99943 \times 6 + 0.999996 \times 7 + 1 \times 8)
\]
\[
= \frac{0.00048 + 0.0000243900165 + 5.99658 + 6.999972 + 8}{3.0000} = \frac{20.997}{3.0000} = \textbf{6.999}
\]
\[
\sigma_1^2 = \frac{1}{2.999426} \times (1 \times (1 - 2.000058)^2 + 0.999996 \times (2 - 2.000058)^2 + 0.99943 \times (3 - 2.000058)^2 + 0.00055 \times (6 - 2.000058)^2 
\]
\[
    + 0.00002439 \times (7 - 2.000058)^2 + 0 \times (8 - 2.000058)^2)
\]
\[
= \frac{1 \times 1.00116 + 0.999996 \times 0.00000000 + 0.99943 \times 0.999884 + 0.00055 \times 15.9999 + 0.00002439 \times 24.9999}{3.0000} 
\]
\[
    = \frac{1.00116 + 0.999314 + 0.008799945 + 0.000024341 + 0.0006097}{3.0000} = \textbf{0.669786}
\]
\[
\sigma_2^2 = \frac{1}{3.00000} \times (0 \times (1 - 6.999)^2 + 0.00002439 \times (2 - 6.999)^2 + 0.00055 \times (3 - 6.999)^2 + 0.99943 \times (6 - 6.999)^2
\]
\[
    + 0.999996 \times (7 - 6.999)^2 + 1 \times (8 - 6.999)^2)
\]
\[
= \frac{0 + 0.00006097 + 0.0087956 + 0.997432 + 0.00000099999 + 1.002001}{3.000} = \textbf{0.6694}
\]

\textbf{Iteration 2:}
\\ we repeat the same process as in iteration 1, but this time we use the updated parameters from iteration 1.
\\ for x = 1:
\[
\mathcal{N}(1 | 2.000058, 0.669786) = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{(1-2.000058)^2}{2 \times 0.669786}} = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{1.000116}{1.339572}} = \textbf{0.2376}
\]
\[
\mathcal{N}(1 | 6.999, 0.6694) = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{(1-6.999)^2}{2 \times 0.6694}} = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{36.996}{1.3388}} = \textbf{0}
\]
so, the probability that x=1 belongs to cluster 1 is:
\[
P(Z = 1 | X = 1) = \frac{0.500004 \times 0.2376}{0.500004 \times 0.2376 + 0.500004 \times 0} = \frac{0.1188}{0.1188 + 0} = \textbf{1}
\]
and, the probability that x=1 belongs to cluster 2 is:
\[
P(Z = 2 | X = 1) = \frac{0.500004 \times 0}{0.500004 \times 0.2376 + 0.500004 \times 0} = \frac{0}{0.1188 + 0} = \textbf{0}
\]
so, $\gamma_{11} = \textbf{1}, \quad \gamma_{12} = \textbf{0}$
\\ for x = 2:
\[
\mathcal{N}(2 | 2.000058, 0.669786) = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{(2-2.000058)^2}{2 \times 0.669786}} = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{0.000116}{1.339572}} = \textbf{0.2376}
\]
\[
\mathcal{N}(2 | 6.999, 0.6694) = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{(2-6.999)^2}{2 \times 0.6694}} = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{20.996}{1.3388}} = \textbf{0}
\]
so, the probability that x=2 belongs to cluster 1 is:
\[
P(Z = 1 | X = 2) = \frac{0.500004 \times 0.2376}{0.500004 \times 0.2376 + 0.500004 \times 0} = \frac{0.1188}{0.1188 + 0} = \textbf{1}
\]
and, the probability that x=2 belongs to cluster 2 is:
\[
P(Z = 2 | X = 2) = \frac{0.500004 \times 0}{0.500004 \times 0.2376 + 0.500004 \times 0} = \frac{0}{0.1188 + 0} = \textbf{0}
\]
so, $\gamma_{21} = \textbf{1}, \quad \gamma_{22} = \textbf{0}$
\\ for x = 3:
\[
\mathcal{N}(3 | 2.000058, 0.669786) = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{(3-2.000058)^2}{2 \times 0.669786}} = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{1.000116}{1.339572}} = \textbf{0.2376}
\]
\[
\mathcal{N}(3 | 6.999, 0.6694) = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{(3-6.999)^2}{2 \times 0.6694}} = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{9.996}{1.3388}} = \textbf{0}
\]
so, the probability that x=3 belongs to cluster 1 is:
\[
P(Z = 1 | X = 3) = \frac{0.500004 \times 0.2376}{0.500004 \times 0.2376 + 0.500004 \times 0} = \frac{0.1188}{0.1188 + 0} = \textbf{1}
\]
\[
P(Z = 2 | X = 3) = \frac{0.500004 \times 0}{0.500004 \times 0.2376 + 0.500004 \times 0} = \frac{0}{0.1188 + 0} = \textbf{0}
\]
so, $\gamma_{31} = \textbf{1}, \quad \gamma_{32} = \textbf{0}$
\\ for x = 6:
\[
\mathcal{N}(6 | 2.000058, 0.669786) = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{(6-2.000058)^2}{2 \times 0.669786}} = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{15.996}{1.339572}} = \textbf{0}
\]
\[
\mathcal{N}(6 | 6.999, 0.6694) = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{(6-6.999)^2}{2 \times 0.6694}} = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{0.996}{1.3388}} = \textbf{0.2376}
\]
so, the probability that x=6 belongs to cluster 1 is:
\[
P(Z = 1 | X = 6) = \frac{0.500004 \times 0}{0.500004 \times 0 + 0.500004 \times 0.2376} = \frac{0}{0 + 0.1188} = \textbf{0}
\]
\[
P(Z = 2 | X = 6) = \frac{0.500004 \times 0.2376}{0.500004 \times 0 + 0.500004 \times 0.2376} = \frac{0.1188}{0 + 0.1188} = \textbf{1}
\]
so, $\gamma_{41} = \textbf{0}, \quad \gamma_{42} = \textbf{1}$
\\ for x = 7:
\[
\mathcal{N}(7 | 2.000058, 0.669786) = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{(7-2.000058)^2}{2 \times 0.669786}} = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{24.996}{1.339572}} = \textbf{0}
\]
\[
\mathcal{N}(7 | 6.999, 0.6694) = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{(7-6.999)^2}{2 \times 0.6694}} = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{0.996}{1.3388}} = \textbf{0.2376}
\]
so, the probability that x=7 belongs to cluster 1 is:
\[
P(Z = 1 | X = 7) = \frac{0.500004 \times 0}{0.500004 \times 0 + 0.500004 \times 0.2376} = \frac{0}{0 + 0.1188} = \textbf{0}
\]
\[
P(Z = 2 | X = 7) = \frac{0.500004 \times 0.2376}{0.500004 \times 0 + 0.500004 \times 0.2376} = \frac{0.1188}{0 + 0.1188} = \textbf{1}
\]
so, $\gamma_{51} = \textbf{0}, \quad \gamma_{52} = \textbf{1}$
\\ for x = 8:
\[
\mathcal{N}(8 | 2.000058, 0.669786) = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{(8-2.000058)^2}{2 \times 0.669786}} = \frac{1}{\sqrt{2\pi \times 0.669786}} e^{-\frac{36.996}{1.339572}} = \textbf{0}
\]
\[
\mathcal{N}(8 | 6.999, 0.6694) = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{(8-6.999)^2}{2 \times 0.6694}} = \frac{1}{\sqrt{2\pi \times 0.6694}} e^{-\frac{0.001}{1.3388}} = \textbf{0.2376}
\]
so, the probability that x=8 belongs to cluster 1 is:
\[
P(Z = 1 | X = 8) = \frac{0.500004 \times 0}{0.500004 \times 0 + 0.500004 \times 0.2376} = \frac{0}{0 + 0.1188} = \textbf{0}
\]
\[
P(Z = 2 | X = 8) = \frac{0.500004 \times 0.2376}{0.500004 \times 0 + 0.500004 \times 0.2376} = \frac{0.1188}{0 + 0.1188} = \textbf{1}
\]
so, $\gamma_{61} = \textbf{0}, \quad \gamma_{62} = \textbf{1}$
\\ Here is a table of the probabilities (responsibilities) that each data point belongs to each cluster:
\[
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Data Point} & \textbf{Cluster 1} & \textbf{Cluster 2} \\
\hline
1 & 1 & 0 \\
2 & 1 & 0 \\
3 & 1 & 0 \\
6 & 0 & 1 \\
7 & 0 & 1 \\
8 & 0 & 1 \\
sum & 3 & 3 \\
\hline
\end{tabular}
\]
\\ now, we can calculate the updated parameters after the second iteration:
\[
\pi_1 = \frac{1}{6} \times (1 + 1 + 1 + 0 + 0 + 0) = \frac{3}{6} = \textbf{0.5}
\]
\[
\pi_2 = \frac{1}{6} \times (0 + 0 + 0 + 1 + 1 + 1) = \frac{3}{6} = \textbf{0.5}
\]

\[
\mu_1 = \frac{1}{3} \times (1 + 2 + 3) = \frac{6}{3} = \textbf{2}
\]
\[
\mu_2 = \frac{1}{3} \times (6 + 7 + 8) = \frac{21}{3} = \textbf{7}
\]
\[
\sigma_1^2 = \frac{1}{3} \times (1 \times (1 - 2)^2 + 2 \times (2 - 2)^2 + 3 \times (3 - 2)^2) = \frac{1 + 0 + 3}{3} = \textbf{1.3333}
\]
\[
\sigma_2^2 = \frac{1}{3} \times (6 \times (6 - 7)^2 + 7 \times (7 - 7)^2 + 8 \times (8 - 7)^2) = \frac{6 + 0 + 8}{3} = \textbf{4.6667}
\]
so the updated parameters after the second iteration are:
\[
\pi_1 = \textbf{0.5}, \quad \pi_2 = \textbf{0.5}, \quad \mu_1 = \textbf{2}, \quad \mu_2 = \textbf{7}, \quad \sigma_1^2 = \textbf{1.3333}, \quad \sigma_2^2 = \textbf{4.6667}
\]


\textbf{Iteration 3:}
\\ we repeat the same process as in iteration 2, but this time we use the updated parameters from iteration 2.
\\ for x = 1:
\[
\mathcal{N}(1 | 2, 1.3333) = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{(1-2)^2}{2 \times 1.3333}} = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{1}{2.6666}} = \textbf{0.23748}
\]
\[
\mathcal{N}(1 | 7, 4.6667) = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{(1-7)^2}{2 \times 4.6667}} = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{36}{9.3334}} = \textbf{0.0039019}
\]
so, the probability that x=1 belongs to cluster 1 is:
\[
P(Z = 1 | X = 1) = \frac{0.5 \times 0.23748}{0.5 \times 0.23748 + 0.5 \times 0.0039019} = \frac{0.11874}{0.11874 + 0.00195095} = \textbf{0.983}
\]
and, the probability that x=1 belongs to cluster 2 is:
\[
P(Z = 2 | X = 1) = \frac{0.5 \times 0.0039019}{0.5 \times 0.23748 + 0.5 \times 0.0039019} = \frac{0.00195095}{0.11874 + 0.00195095} = \textbf{0.016}
\]
so, $\gamma_{11} = \textbf{0.983}, \quad \gamma_{12} = \textbf{0.016}$
\\ for x = 2:
\[
\mathcal{N}(2 | 2, 1.3333) = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{(2-2)^2}{2 \times 1.3333}} = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{0}{2.6666}} = \textbf{0.3455}
\]
\[
\mathcal{N}(2 | 7, 4.6667) = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{(2-7)^2}{2 \times 4.6667}} = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{25}{9.3334}} = \textbf{0}
\]
so, the probability that x=2 belongs to cluster 1 is:
\[
P(Z = 1 | X = 2) = \frac{0.5 \times 0.3455}{0.5 \times 0.3455 + 0.5 \times 0} = \frac{0.17275}{0.17275 + 0} = \textbf{1}
\]
and, the probability that x=2 belongs to cluster 2 is:
\[
P(Z = 2 | X = 2) = \frac{0.5 \times 0}{0.5 \times 0.3455 + 0.5 \times 0} = \frac{0}{0.17275 + 0} = \textbf{0}
\]
so, $\gamma_{21} = \textbf{1}, \quad \gamma_{22} = \textbf{0}$
\\ for x = 3:
\[
\mathcal{N}(3 | 2, 1.3333) = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{(3-2)^2}{2 \times 1.3333}} = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{1}{2.6666}} = \textbf{0.23748}
\]
\[
\mathcal{N}(3 | 7, 4.6667) = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{(3-7)^2}{2 \times 4.6667}} = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{16}{9.3334}} = \textbf{0.03326}
\]
so, the probability that x=3 belongs to cluster 1 is:
\[
P(Z = 1 | X = 3) = \frac{0.5 \times 0.23748}{0.5 \times 0.23748 + 0.5 \times 0.03326} = \frac{0.11874}{0.11874 + 0.01663} = \textbf{0.877}
\]
and, the probability that x=3 belongs to cluster 2 is:
\[
P(Z = 2 | X = 3) = \frac{0.5 \times 0.03326}{0.5 \times 0.23748 + 0.5 \times 0.03326} = \frac{0.01663}{0.11874 + 0.01663} = \textbf{0.122}
\]
so, $\gamma_{31} = \textbf{0.877}, \quad \gamma_{32} = \textbf{0.122}$
\\ for x = 6:
\[
\mathcal{N}(6 | 2, 1.3333) = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{(6-2)^2}{2 \times 1.3333}} = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{16}{2.6666}} = \textbf{0.00085628}
\]
\[
\mathcal{N}(6 | 7, 4.6667) = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{(6-7)^2}{2 \times 4.6667}} = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{1}{9.3334}} = \textbf{0.165934112}
\]
so, the probability that x=6 belongs to cluster 1 is:
\[
P(Z = 1 | X = 6) = \frac{0.5 \times 0.00085628}{0.5 \times 0.00085628 + 0.5 \times 0.165934112} = \frac{0.00042814}{0.00042814 + 0.082967056} = \textbf{0.005}
\]
and, the probability that x=6 belongs to cluster 2 is:
\[
P(Z = 2 | X = 6) = \frac{0.5 \times 0.165934112}{0.5 \times 0.00085628 + 0.5 \times 0.165934112} = \frac{0.082967056}{0.00042814 + 0.082967056} = \textbf{0.995}
\]
so, $\gamma_{41} = \textbf{0.005}, \quad \gamma_{42} = \textbf{0.995}$
\\ for x = 7:
\[
\mathcal{N}(7 | 2, 1.3333) = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{(7-2)^2}{2 \times 1.3333}} = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{25}{2.6666}} = \textbf{0.0000292978}
\]
\[
\mathcal{N}(7 | 7, 4.6667) = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{(7-7)^2}{2 \times 4.6667}} = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{0}{9.3334}} = \textbf{0.1847}
\]
so, the probability that x=7 belongs to cluster 1 is:
\[
P(Z = 1 | X = 7) = \frac{0.5 \times 0.0000292978}{0.5 \times 0.0000292978 + 0.5 \times 0.1847} = \frac{0.0000146489}{0.0000146489 + 0.09235} = \textbf{0.00016}
\]
\[
P(Z = 2 | X = 7) = \frac{0.5 \times 0.1847}{0.5 \times 0.0000292978 + 0.5 \times 0.1847} = \frac{0.09235}{0.0000146489 + 0.09235} = \textbf{0.9998}
\]
so, $\gamma_{51} = \textbf{0.00016}, \quad \gamma_{52} = \textbf{0.9998}$
\\ for x = 8:
\[
\mathcal{N}(8 | 2, 1.3333) = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{(8-2)^2}{2 \times 1.3333}} = \frac{1}{\sqrt{2\pi \times 1.3333}} e^{-\frac{36}{2.6666}} = \textbf{0}
\]
\[
\mathcal{N}(8 | 7, 4.6667) = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{(8-7)^2}{2 \times 4.6667}} = \frac{1}{\sqrt{2\pi \times 4.6667}} e^{-\frac{1}{9.3334}} = \textbf{0.165934112}
\]
so, the probability that x=8 belongs to cluster 1 is:
\[
P(Z = 1 | X = 8) = \frac{0.5 \times 0}{0.5 \times 0 + 0.5 \times 0.165934112} = \frac{0}{0 + 0.082967056} = \textbf{0}
\]
\[
P(Z = 2 | X = 8) = \frac{0.5 \times 0.165934112}{0.5 \times 0 + 0.5 \times 0.165934112} = \frac{0.082967056}{0 + 0.082967056} = \textbf{1}
\]
so, $\gamma_{61} = \textbf{0}, \quad \gamma_{62} = \textbf{1}$
\\ Here is a table of the probabilities (responsibilities) that each data point belongs to each cluster:
\[
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\textbf{Data Point} & \textbf{Cluster 1} & \textbf{Cluster 2} \\
\hline
1 & 0.983 & 0.016 \\
2 & 1 & 0 \\
3 & 0.877 & 0.122 \\
6 & 0.005 & 0.995 \\
7 & 0.00016 & 0.9998 \\
8 & 0 & 1 \\
sum & 2.86516 & 3.1328 \\
\hline
\end{tabular}
\]
\\ now, we can calculate the updated parameters after the third iteration:
\[
\pi_1 = \frac{1}{6} \times (0.983 + 1 + 0.877 + 0.005 + 0.00016) = \frac{2.86516}{5.99896} = \textbf{0.4777}
\]
\[
\pi_2 = \frac{1}{6} \times (0.016 + 0 + 0.122 + 0.995 + 0.9998) = \frac{3.1328}{5.99896} = \textbf{0.5223}
\]
\[
\mu_1 = \frac{1}{2.86516} \times (0.983 \times 1 + 1 \times 2 + 0.877 \times 3 + 0.005 \times 6 + 0.00016 \times 7) = \frac{5.64512}{2.86516} = \textbf{1.97}
\]
\[
\mu_2 = \frac{1}{3.1328} \times (0.016 \times 1 + 0 \times 2 + 0.122 \times 3 + 0.995 \times 6 + 0.9998 \times 7 + 8 \times 1) = \frac{14.352}{3.1328} = \textbf{6.81519}
\]
\[
\sigma_1^2 = \frac{1}{2.86516} \times (0.983 \times (1 - 1.97)^2 + 1 \times (2 - 1.97)^2 + 0.877 \times (3 - 1.97)^2 + 0.005 \times (6 - 1.97)^2 + 0.00016 \times (7 - 1.97)^2)
\]
\[
 = \frac{1}{2.86516} \times (0.9249 + 0.0009 + 0.9304093 + 0.0812045 + 0.0004048) = \frac{1.9370923}{2.86516} = \textbf{0.6776}
\]
\[
\sigma_2^2 = \frac{1}{3.1328} \times (0.016 \times (1 - 6.81519)^2 + 0 \times (2 - 6.81519)^2 + 0.122 \times (3 - 6.81519)^2 + 0.995 \times (6 - 6.81519)^2 
\]
\[
    + 0.9998 \times (7 - 6.81519)^2)
\]
\[
    = \frac{1}{3.1328} \times (0.54106 + 0 + 1.77579 + 0.66121 + 0.0341 + 8) = \frac{11.0122}{3.1328} = \textbf{3.515}
\]
\item \textbf{(10 points)} The Iris dataset contains 150 samples of iris flowers, each with four features: sepal length, sepal width, petal length, and petal width. The dataset also includes the species of the iris flowers, but we'll ignore this information for clustering.

    Use the Expectation-Maximization (EM) algorithm to fit a Gaussian Mixture Model (GMM) to this data. Assume the data comes from a mixture of three Gaussian distributions. Perform the following steps:
    \[
    \textbf{[ in the notebook ]}
    \]
    \begin{enumerate}
        \item Preprocess the data by standardizing each feature.

        \item Initialize the parameters of the GMM (means, variances, and mixing coefficients) randomly.
        \item Implement the EM algorithm to fit the GMM to the data.
        
        \item Perform the EM algorithm for a maximum of 100 iterations or until convergence.
        \item Report the final parameters (means, variances, and mixing coefficients).
        \item Visualize the clustering results on the first two principal components of the data.
    \end{enumerate}
\end{enumerate}


\section*{Question 5: Random Processes and Markov Chains (40 points)}
\subsection*{Problem 1: Simple Random Walk Process (8 points)}
Imagine a scenario where a delivery robot moves along a straight path, starting from the origin (0 meters). At each time step, the robot has an equal chance of moving one meter to the left (backwards) or one meter to the right (forwards). This model can help us understand how the robot's position changes over time, which can be useful in logistics and supply chain management. Let \( X_n \) denote the position of the robot at time \( n \).
\begin{itemize}
    \item[(a)] \textbf{(2 points)} Derive the first-order probability mass function for the position of the robot at time \( n \), i.e., \( P(X_n = x) \), where \( x \) is an integer representing the robot's position.
    \[
    P(X_n = x) = \begin{cases}
    \frac{1}{2} P(X_{n-1} = x-1) + \frac{1}{2} P(X_{n-1} = x+1), & \text{if } x \neq 0, \\
    \frac{1}{2} P(X_{n-1} = 1), & \text{if } x = 0.
    \end{cases}
    \]
    so the first order probability mass function is given by:
    \[
    P(X_n = x) = \frac{1}{2} P(X_{n-1} = x-1) + \frac{1}{2} P(X_{n-1} = x+1), \quad \text{for } x \in \mathbb{Z}.
    \]
    \item[(b)] \textbf{(2 points)} Calculate the probability that the robot, starting at the origin \( X_0 = 0 \), is located at \( X_4 = -2 \) after 4 steps. \textbf{\textit{Hint:}} Use the binomial distribution, considering the number of steps to the left and right.
    \\ the binomial distribution formula is given by:
    \[
    P(X_n = x) = \binom{n}{k} p^k (1-p)^{n-k}
    \]
    where, according to this setup, \( n = 4, \text{total steps}\), \( k = \text{number of steps to the right} \), $n-k = $ number of steps to the left, and \( p = \frac{1}{2} \).
    \\ to be at position $-2$ :
    \[
    k - (n-k) = -2 \Rightarrow 2k - n = -2 \Rightarrow 2k = n - 2 \Rightarrow k = \frac{n-2}{2} = \frac{4-2}{2} = 1
    \]
    so we need to find all the possible ways the robot can take 1 step to the right and 3 steps to the left, which is given by the binomial distribution formula
    \\so the probability that the robot is located at \( X_4 = -2 \) after 4 steps is:
    \[
    P(X_4 = -2) = \binom{4}{3} \left(\frac{1}{2}\right)^3 \left(\frac{1}{2}\right)^1 = 4 \times \frac{1}{8} \times \frac{1}{2} = \mathbf{\frac{1}{4}}
    \]
    \item[(c)] \textbf{(2 points)} Derive the expressions for the mean \( E[X_n] \) and variance \( \text{Var}(X_n) \) of the random walk at any time \( n \in \mathbb{N} \).
    \\ At every step, the robot has an equal probability of moving one meter to the left or right. Therefore, the expected value of the position after \( n \) steps is:
    \[
    E[X_n] = 1 \cdot p + (-1) \cdot (1-p) = 1 \cdot \frac{1}{2} + (-1) \cdot \frac{1}{2} = 0
    \]
    \[
    E[X_n] = \textbf{0}
    \]
    The variance of the position after \( n \) steps is:
    \[
    \text{Var}(X_n) = E[X_n^2] - (E[X_n])^2 = 1^2 \cdot p + (-1)^2 \cdot (1-p) - 0 = 1 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} = 1 \text{ per step}  
    \]
    so the variance after \( n \) steps is:
    \[
    \text{Var}(X_n) = n \cdot 1 = \mathbf{n}
    \]
    \item[(d)] \textbf{(2 points)} Discuss whether the mean and variance depend on the initial position \( X_0 \).
    \\\\ The mean depends on the initial position, as the expected value of the position after \( n \) steps is 0 when starting from the origin. However, the variance does not depend on the initial position, as it is solely determined by the number of steps taken.
\end{itemize}



\subsection*{Problem 2: Wide Sense Stationarity (WSS) (8 points)}

Consider a discrete-time process \( \{X_n\} \) with mean \( \mu \) and autocovariance function \( \gamma(k) \), where \( k \) is the lag.

\begin{itemize}
    \item[(a)] \textbf{(2 points)} Derive the mathematical conditions that must be satisfied for the process \( \{X_n\} \) to be classified as Wide Sense Stationary (WSS). Clearly define what it means for the mean and autocovariance function to be time-invariant.
    \[
    \text{WSS conditions:} \quad \mu = \text{constant},\quad  \sigma^2 = \text{constant} ,\quad \gamma(k) = \text{constant}, \quad \forall k
    \]
    mean must be time-invariant and  remains constant over time, i.e.,:
    \[
    \text{Time-invariant mean:} \quad E[X_n] = \mu, \quad \forall n
    \]
    variance must be time-invariant and remains constant over time, i.e.,:
    \[
    \text{Time-invariant variance:} \quad \text{Var}(X_n) = \sigma^2, \quad \forall n
    \]
    autocovariance function must be time-invariant if it only depends on the lag \( k \) and not on the time index \( n \), i.e.,:
    \[
    \text{Time-invariant autocovariance:} \quad \gamma(k) = \text{Cov}(X_n, X_{n+k}), \quad \forall k
    \]
    \item[(b)] \textbf{(2 points)} Let \( X_n = a X_{n-1} + W_n \), where \( W_n \) is white noise with mean 0 and variance \( \sigma^2 \), and \( |a| < 1 \). Show that the process \( \{X_n\} \) is WSS by deriving its mean, variance, and autocovariance function, ensuring that the conditions of WSS are satisfied.
    \\ let's start by calculating the mean:
    \[
    E[X_n] = E[aX_{n-1} + W_n] = aE[X_{n-1}] + E[W_n] 
    \]
    since \( W_n \) is white noise with mean 0
    \[
    E[X_n] = aE[X_{n-1}] + 0 = aE[X_{n-1}]
    \]
    let us take the expectation of \( X_{n-1} \) to be used in the next step, since fo WSS the mean must be time-invariant:
    \[
    E[X_{n-1}] = aE[X_{n-2}]
    \]
    \[
    E[X_{n-2}] = aE[X_{n-3}]
    \]
    \[
    \vdots
    \]
    \[
    E[X_{n-1}] = a^n E[X_{0}]
    \]
    since $|a| < 1$, as \( n \) approaches infinity, \( a^n \) approaches 0, so the mean is time-invariant:
    \[
    E[X_n] = a^n E[X_{0}] = 0
    \]
    the variance formula is given by:
    \[
    \text{Var}(X_n) = E[X_n^2] - (E[X_n])^2
    \]
    the expectation of \( X_n^2 \) is:
    \[
    E[X_n^2] = E[(aX_{n-1} + W_n)^2] = E[a^2X_{n-1}^2 + 2aX_{n-1}W_n + W_n^2]
    \]
    \[
    E[X_n^2] = a^2E[X_{n-1}^2] + 2aE[X_{n-1}W_n] + E[W_n^2]
    \]
    since \( W_n \) is white noise with mean 0 and variance \( \sigma^2 \), the expectation of \( W_n^2 \) is \( \sigma^2 \), and the expectation of \( X_{n-1}W_n \) is 0, so:
    \[
    E[X_n^2] = a^2E[X_{n-1}^2] + \sigma^2
    \]
    the expectation of \( X_{n-1}^2 \) is:
    \[
    E[X_{n-1}^2] = E[(aX_{n-2} + W_{n-1})^2] = E[a^2X_{n-2}^2 + 2aX_{n-2}W_{n-1} + W_{n-1}^2]
    \]
    \[
    E[X_{n-1}^2] = a^2E[X_{n-2}^2] + \sigma^2
    \]
    \[
    E[X_{n-2}^2] = a^2E[X_{n-3}^2] + \sigma^2
    \]
    \[
    \vdots
    \]
    \[
    E[X_{n-1}^2] = a^2E[X_{n-2}^2] + \sigma^2
    \]
    for WSS, the variance must be time-invariant, so the variance $E[X_n^2]$ must be equal to the variance $E[X_{n-1}^2]$, so we can represent this as a recursive formula with the variance as $y$:
    \[
    y = a^2y + \sigma^2 \Rightarrow y(1 - a^2) = \sigma^2 \Rightarrow y = \frac{\sigma^2}{1 - a^2}
    \]
    so the variance is:
    \[
    \text{Var}(X_n) = \mathbf{\frac{\sigma^2}{1 - a^2}}
    \]
    The autocovariance function is given by:
    \[
    \gamma(k) = \text{Cov}(X_n, X_{n+k}) = E[(X_n - E[X_n])(X_{n+k} - E[X_{n+k}])]
    \]
    we can substitute the values of \( X_n \) and \( X_{n+k} \) to get:
    \[
    \gamma(k) = E[(aX_{n-1} + W_n)(aX_{n+k-1} + W_{n+k})]
    \]
    we know that the expectation of \( W_nW_{n+k} \) is 0, so the autocovariance function is:
    \[
    \gamma(k) = E[a^2X_{n-1}X_{n+k-1}] = a^2E[X_{n-1}X_{n+k-1}]
    \]
    we can substitute the values of \( X_{n-1} \) and \( X_{n+k-1} \) to get:
    \[
    \gamma(k) = a^2E[X_{n-2}X_{n+k-2}] = a^2E[X_{n-3}X_{n+k-3}]
    \]
    \[
    \vdots
    \]
    this can be generalized as:
    \[
    \gamma(k) = a^{k}E[X_{n}X_{n}]
    \]
    \[
    \gamma(k) = a^{k}E[X_{n}^2]
    \]
    \[
    \gamma(k) = a^{k}y
    \]
    \[
    \gamma(k) = a^{k}\frac{\sigma^2}{1 - a^2}
    \]
    \item[(c)] \textbf{(2 points)} Discuss how the parameter \( a \) influences the behavior of the process, particularly in terms of its stability and variance over time.
    \\\\ The parameter \( a \) influences the stability of the process, as the process is WSS only when \( |a| < 1 \). If \( |a| \geq 1 \), the process is not WSS, and the variance may increase over time, leading to instability. The parameter \( a \) determines the impact of the previous value on the current value, by scaling the previous value by \( a \). If \( |a| \) is close to 1, the process may exhibit slow convergence or divergence, affecting the stability and variance over time.
    \item[(d)] \textbf{(2 points)} Consider the case where \( |a| \geq 1 \) and explain why the process may no longer be WSS under such conditions.
    \\\\ When \( |a| \geq 1 \), the process may no longer be WSS because the process becomes unstable, with the variance increasing over time. The mean may not remain constant, violating the WSS conditions. The process may exhibit explosive behavior, diverging to infinity or oscillating without convergence, leading to non-stationarity.
\end{itemize}



\section*{Problem 3: Gambler’s Ruin Problem as a Markov Chain (24 points)}

Consider a gambling game where, at each turn, a gambler either wins 1 dollar with probability 0.4 or loses 1 dollar with probability 0.6. The gambler starts with an initial wealth of 1 dollar and stops playing when the total wealth reaches \( N = 5 \) dollars or falls to 0 dollars (ruin). Let \( X_n \) denote the wealth of the gambler after the \( n \)-th play.


\begin{itemize}
    \item[(a)] \textbf{(2 points)} Show that the process \( X_n \) is a Markov chain, explaining how it satisfies the Markov property 
    \\ the gambler begins with 1 dollar in state 1
    \\ the next state can be either 0 or 2, with probabilities 0.6 and 0.4, respectively
    \\ \begin{tikzpicture}[shorten >=1pt, node distance=2cm, auto, scale=1.5]

        % Nodes representing the states
        \node[state, fill=red!30] (A) {0};
        \node[state, right of=A] (B) {1};
        \node[state, right of=B] (C) {2};
        \node[state, right of=C] (D) {3};
        \node[state, right of=D] (E) {4};
        \node[state, fill=green!30, right of=E] (F) {5};
    
          % Transitions between states with different arrow types
    \path[->, thick] (B) edge [bend left=45, above] node {0.4} (C);
    \path[->, thick] (B) edge [bend right=45, below] node {0.6} (A);
    \path[->, thick] (C) edge [bend left=45, above] node {0.4} (D);
    \path[->, thick] (C) edge [bend right=45, below] node {0.6} (B);
    \path[->, thick] (D) edge [bend left=45, above] node {0.4} (E);
    \path[->, thick] (D) edge [bend right=45, below] node {0.6} (C);
    \path[->, thick] (E) edge [bend left=45, above] node {0.4} (F);
    \path[->, thick] (E) edge [bend right=45, below] node {0.6} (D);
    
    
        % Absorbing states
        \node[draw, circle, below=0.5cm of A] (AbsA) {Absorbing};
        \node[draw, circle, below=0.5cm of F] (AbsF) {Absorbing};
        
        % Final connections
        \path[->] (A) edge [loop left, above] node {1.0} (A);
        \path[->] (F) edge [loop right, above] node {1.0} (F);
    \end{tikzpicture}
    
    The process \( X_n \) is a Markov chain because the probability of transitioning to the next state depends only on the amount of wealth at the current state, not on the history of previous states. The Markov property holds because the future state depends only on the current state, making it a memoryless process.
    
    \item[(b)] \textbf{(4 points)} Construct the transition probability matrix for the Markov chain, ensuring that it captures both the winning and losing probabilities. Explicitly show the entries corresponding to the absorbing states and transient states.
\[
P =
\begin{array}{c|cccccc}
\text{State} & 0   & 1   & 2   & 3   & 4   & 5   \\
\hline
0 & 1   & 0   & 0   & 0   & 0   & 0   \\
1 & 0.6 & 0   & 0.4 & 0   & 0   & 0   \\
2 & 0   & 0.6 & 0   & 0.4 & 0   & 0   \\
3 & 0   & 0   & 0.6 & 0   & 0.4 & 0   \\
4 & 0   & 0   & 0   & 0.6 & 0   & 0.4 \\
5 & 0   & 0   & 0   & 0   & 0   & 1   \\
\end{array}
\]    
\item[(c)] \textbf{(5 points)} Derive the probability that the gambler reaches the absorbing state at \( N = 5 \) dollars before reaching 0 dollars (ruin).
    \\ let \( P \) be the probability of reaching the absorbing state at \( N = 5 \) dollars before reaching 0 dollars (ruin) given the current wealth of the gambler is \( i \), where \( i \) is the current state of the Markov chain. The probability \( P \) can be expressed as:
    \[
    P = \begin{cases}
    1, & \text{if } i = 5, \\
    0, & \text{if } i = 0, \\
    0.6P_{i-1} + 0.4P_{i+1}, & \text{otherwise}.
    \end{cases}
    \]
    The probability \( P \) can be solved using the system of linear equations:
    \[
    \begin{aligned}
    P_0 & = 0, \\
    P_5 & = 1, \\
    P_1 & = 0.6P_0 + 0.4P_2, \\
    P_2 & = 0.6P_1 + 0.4P_3, \\
    P_3 & = 0.6P_2 + 0.4P_4, \\
    P_4 & = 0.6P_3 + 0.4P_5.
    \end{aligned}
    \]
    Solving the system of equations, we find that the probability \( P \) is:
    \[
    P = \begin{cases}
    0, & \text{if } i = 0, \\
    \frac{1}{5}, & \text{if } i = 1, 2, 3, 4, \\
    1, & \text{if } i = 5.
    \end{cases}
    \]
    Therefore, the probability that the gambler reaches the absorbing state at \( N = 5 \) dollars before reaching 0 dollars (ruin) is \( \frac{1}{5} \) for all states \( i = 1, 2, 3, 4 \).

    \item[(d)] \textbf{(3 points)} Discuss how the probabilities change as \( p \) varies, particularly focusing on the expected duration of the game and the likelihood of reaching 5 dollars versus 0 dollars.
    \\\\ The probability of reaching the absorbing state at \( N = 5 \) dollars before reaching 0 dollars (ruin) depends on the winning probability \( p \). As \( p \) increases, the gambler is more likely to reach 5 dollars before reaching 0 dollars. The expected duration of the game decreases as \( p \) increases, as the gambler is more likely to win and reach the target wealth of 5 dollars. Conversely, as \( p \) decreases, the gambler is more likely to lose and reach 0 dollars, leading to a longer duration of the game.
    \item[(e)] \textbf{(10 points)} Simulate a gambling game where you either win or lose money based on specific probabilities based on the above parameters. 




\subsection*{Simulation Steps}

\begin{enumerate}
    
    \item \textbf{Initialize Simulation:}
    \begin{itemize}
        \item Define the number of trials (\texttt{num\_trials = 10}) to run, starting wealth for the player, probability of winning each round, and the target wealth to reach.
    \end{itemize}
    
    \item \textbf{Conduct Trials:}
    \begin{itemize}
        \item For each trial:
        \begin{itemize}
            \item Start with the initial wealth.
            \item Track the wealth changes throughout the game.
            \item Simulate rounds of play:
            \begin{itemize}
                \item Continue playing until either the target wealth is reached or the player loses all their money.
                \item In each round, determine the outcome based on the defined probability, updating the wealth accordingly.
            \end{itemize}
        \end{itemize}
    \end{itemize}
    
    \item \textbf{Store Results:}
    \begin{itemize}
        \item Keep a record of the wealth progression for each trial.
    \end{itemize}
    
    \item \textbf{Visualize Outcomes:}
    \begin{itemize}
        \item Create plots to illustrate the wealth changes over the course of several trials.
        \item Mark significant states, such as the target wealth and the point of ruin.
    \end{itemize}
    
    \item \textbf{Analyze Results:}
    \begin{itemize}
        \item Calculate the number of times the player wins or loses across all trials.
        \item Determine the overall winning rate and the average number of rounds played per trial.
        \item Present these statistics for review.
    \end{itemize}
\end{enumerate}

\subsection*{Discussion Points}
\begin{itemize}
    \item Reflect on how the probabilities affect the game's dynamics.
    \item Consider the implications of reaching either absorbing state and the significance of randomness in this process.
    \item Discuss how this simulation relates to concepts of Markov chains and random walks.
\end{itemize}


\end{itemize}




\end{document}