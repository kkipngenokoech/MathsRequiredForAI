{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from scipy import linalg\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1A: Kalman Filters in Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter:\n",
    "    def __init__(self, alpha, q, r, initial_state, initial_variance):\n",
    "        \"\"\"\n",
    "        Initializes the Kalman filter with parameters.\n",
    "        \n",
    "        Args:\n",
    "            alpha (float): State transition coefficient.\n",
    "            q (float): Process noise standard deviation.\n",
    "            r (float): Measurement noise standard deviation.\n",
    "            initial_state (float): Initial state estimate.\n",
    "            initial_variance (float): Initial error covariance.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.q = q\n",
    "        self.r = r\n",
    "        self.mu = initial_state\n",
    "        self.sigma = initial_variance\n",
    "\n",
    "    def filter(self, data):\n",
    "        \"\"\"\n",
    "        Applies the Kalman filter to the input data.\n",
    "        \n",
    "        Args:\n",
    "            data (np.array): The data array to filter.\n",
    "        \n",
    "        Returns:\n",
    "            mu_filtered (np.array): Array of filtered state estimates.\n",
    "            sigma_filtered (np.array): Array of filtered variances.\n",
    "        \n",
    "        TODO:\n",
    "        - Implement the prediction and update steps of the Kalman filter.\n",
    "        \"\"\"\n",
    "        T = len(data)\n",
    "        mu_filtered = np.zeros(T)\n",
    "        sigma_filtered = np.zeros(T)\n",
    "        mu_filtered[0] = self.mu\n",
    "        sigma_filtered[0] = self.sigma\n",
    "\n",
    "        for t in range(1, T):\n",
    "            # TODO: Implement prediction step\n",
    "            mu_prior = NotImplemented  # Replace with: μ_prior_t = α * μ_t-1\n",
    "            sigma_prior = NotImplemented  # Replace with: σ_prior_t = q^2 + (α^2) * σ_t-1\n",
    "\n",
    "            # TODO: Implement update step\n",
    "            K = NotImplemented  # Replace with: σ_prior_t / (σ_prior_t + r^2)\n",
    "            mu_filtered[t] = NotImplemented  # Replace with: μ_t = μ_prior_t + K_t * (data_t - μ_prior_t)\n",
    "            sigma_filtered[t] = NotImplemented  # Replace with: σ_t = (1 - K_t) * σ_prior_t\n",
    "\n",
    "        return mu_filtered, sigma_filtered\n",
    "\n",
    "    def plot_results(self, dates, data, mu_filtered, sigma_filtered):\n",
    "        \"\"\"\n",
    "        Plots the original and filtered data along with the confidence interval.\n",
    "        \n",
    "        Args:\n",
    "            dates (pd.Index): Date index for the data.\n",
    "            data (np.array): Original data.\n",
    "            mu_filtered (np.array): Filtered state estimates.\n",
    "            sigma_filtered (np.array): Filtered variances.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(dates, data, label='Original Avg Temp', color='red', alpha=0.5)\n",
    "        plt.plot(dates, mu_filtered, label='Filtered Avg Temp (Kalman)', color='blue')\n",
    "        plt.fill_between(dates, mu_filtered + sigma_filtered, mu_filtered - sigma_filtered, color='blue', alpha=0.2, label='Confidence Interval')\n",
    "        plt.title('Kalman Filter Applied to Avg Temp Data')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Temperature (°C)')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Main function with TODOs for students to complete\n",
    "if __name__ == \"__main__\":\n",
    "    # TODO: Load the dataset and preprocess it\n",
    "    weather_df = NotImplemented # TODO Load the data using the 'parse_dates' and 'index_col' arguments\n",
    "    weather_df = NotImplemented  # TODO: Select 'temp_max', 'temp_min', and 'weather' columns and drop NaN values\n",
    "    \n",
    "    # Calculate the average temperature\n",
    "    weather_df['temp_avg'] = NotImplemented  # TODO: Calculate the average temperature \n",
    "    \n",
    "    # Plot the original average temperature data\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(weather_df.index, weather_df['temp_avg'], label='Original Avg Temp', color='black')\n",
    "    plt.title('Original Avg Temperature Data')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # TODO: Set Kalman filter parameters\n",
    "    alpha = NotImplemented  # Replace with appropriate value\n",
    "    q = NotImplemented  # Replace with appropriate value\n",
    "    r = NotImplemented  # Replace with appropriate value\n",
    "    initial_state = weather_df['temp_avg'].values[0]  # Initial state estimate\n",
    "    initial_variance = 1  # Initial error covariance\n",
    "\n",
    "    # Initialize the Kalman filter\n",
    "    kf = KalmanFilter(alpha, q, r, initial_state, initial_variance)\n",
    "\n",
    "    # Run the Kalman filter on the temperature data\n",
    "    temp_avg_data = weather_df['temp_avg'].values\n",
    "    mu_filtered, sigma_filtered = kf.filter(temp_avg_data)\n",
    "\n",
    "    # Plot results\n",
    "    kf.plot_results(weather_df.index, temp_avg_data, mu_filtered, sigma_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1B: Hidden Markov Models (HMMs) in Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkovModel:\n",
    "    def __init__(self, num_states, num_observations):\n",
    "        \"\"\"\n",
    "        Initializes the HMM with random transition, emission, and initial state probabilities.\n",
    "        \n",
    "        Args:\n",
    "            num_states (int): Number of hidden states.\n",
    "            num_observations (int): Number of unique observations.\n",
    "        \"\"\"\n",
    "        self.num_states = num_states\n",
    "        self.num_observations = num_observations\n",
    "        self.A = np.random.dirichlet(np.ones(num_states), num_states)  # Transition matrix\n",
    "        self.B = np.random.dirichlet(np.ones(num_observations), num_states)  # Emission matrix\n",
    "        self.pi = np.random.dirichlet(np.ones(num_states))  # Initial state distribution\n",
    "        \n",
    "        # Log-space versions to avoid underflow\n",
    "        self.A_log = np.log(self.A + 1e-10)\n",
    "        self.B_log = np.log(self.B + 1e-10)\n",
    "        self.pi_log = np.log(self.pi + 1e-10)\n",
    "\n",
    "    def forward_algorithm_log(self, O):\n",
    "        \"\"\"\n",
    "        Forward algorithm in log-space.\n",
    "        \n",
    "        Args:\n",
    "            O (np.array): Observation sequence (integers).\n",
    "        \n",
    "        Returns:\n",
    "            alpha_log (np.array): Log-probability matrix of forward probabilities.\n",
    "        \n",
    "        TODO:\n",
    "        - Implement the forward algorithm initialization and recursion in log-space.\n",
    "        \"\"\"\n",
    "        T = len(O)\n",
    "        N = self.num_states\n",
    "        alpha_log = np.zeros((T, N))\n",
    "\n",
    "        # TODO: Initialization step\n",
    "        alpha_log[0] = NotImplemented  # Replace with initialization logic: log(α_0) = log(π) + log(B[:, O_0])\n",
    "\n",
    "        # TODO: Recursion step\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                alpha_log[t, j] = NotImplemented  # Replace with recursion logic\n",
    "\n",
    "        return alpha_log\n",
    "\n",
    "    def backward_algorithm_log(self, O):\n",
    "        \"\"\"\n",
    "        Backward algorithm in log-space.\n",
    "        \n",
    "        Args:\n",
    "            O (np.array): Observation sequence (integers).\n",
    "        \n",
    "        Returns:\n",
    "            beta_log (np.array): Log-probability matrix of backward probabilities.\n",
    "        \n",
    "        TODO:\n",
    "        - Implement the backward algorithm initialization and recursion in log-space.\n",
    "        \"\"\"\n",
    "        T = len(O)\n",
    "        N = self.num_states\n",
    "        beta_log = np.zeros((T, N))\n",
    "\n",
    "        # TODO: Initialization step\n",
    "        beta_log[-1] = 0  # log(1) = 0\n",
    "\n",
    "        # TODO: Recursion step\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            for i in range(N):\n",
    "                beta_log[t, i] = NotImplemented  # Replace with recursion logic\n",
    "\n",
    "        return beta_log\n",
    "\n",
    "    def baum_welch_log(self, O, max_iter=100, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        Baum-Welch algorithm for training HMM in log-space.\n",
    "        \n",
    "        Args:\n",
    "            O (np.array): Observation sequence (integers).\n",
    "            max_iter (int): Maximum number of iterations.\n",
    "            epsilon (float): Small value to prevent division by zero.\n",
    "        \n",
    "        Returns:\n",
    "            Trained transition, emission, and initial state distributions.\n",
    "        \n",
    "        TODO:\n",
    "        - Implement the update steps for transition and emission probabilities.\n",
    "        \"\"\"\n",
    "        T = len(O)\n",
    "        \n",
    "        for iteration in range(max_iter):\n",
    "            # TODO: Call forward_algorithm_log and backward_algorithm_log\n",
    "            alpha_log = NotImplemented  # Call forward algorithm\n",
    "            beta_log = NotImplemented  # Call backward algorithm\n",
    "\n",
    "            # Compute gamma and xi in log-space\n",
    "            gamma_log = alpha_log + beta_log - np.logaddexp.reduce(alpha_log[-1])\n",
    "            xi_log = np.zeros((T - 1, self.num_states, self.num_states))\n",
    "\n",
    "            for t in range(T - 1):\n",
    "                denom_log = NotImplemented  # Replace with logic for computing the denominator\n",
    "                for i in range(self.num_states):\n",
    "                    for j in range(self.num_states):\n",
    "                        xi_log[t, i, j] = NotImplemented  # Replace with logic for computing xi_log\n",
    "\n",
    "            # TODO: Update A_log, B_log, and pi_log\n",
    "            self.A_log = NotImplemented  # Replace with logic for updating A_log\n",
    "            self.B_log = NotImplemented  # Replace with logic for updating B_log\n",
    "            self.pi_log = NotImplemented  # Replace with logic for updating pi_log\n",
    "\n",
    "        return np.exp(self.A_log), np.exp(self.B_log), np.exp(self.pi_log)\n",
    "\n",
    "    def viterbi_algorithm_log(self, O):\n",
    "        \"\"\"\n",
    "        Viterbi algorithm for finding the most likely state sequence in log-space.\n",
    "        \n",
    "        Args:\n",
    "            O (np.array): Observation sequence (integers).\n",
    "        \n",
    "        Returns:\n",
    "            states (np.array): Most likely state sequence.\n",
    "        \n",
    "        TODO:\n",
    "        - Implement the Viterbi algorithm's initialization and recursion steps.\n",
    "        \"\"\"\n",
    "        T = len(O)\n",
    "        N = self.num_states\n",
    "        delta_log = np.zeros((T, N))\n",
    "        psi = np.zeros((T, N), dtype=int)\n",
    "\n",
    "        # TODO: Initialization step\n",
    "        delta_log[0] = NotImplemented  # Replace with initialization logic\n",
    "\n",
    "        # TODO: Recursion step\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                delta_log[t, j] = NotImplemented  # Replace with recursion logic\n",
    "                psi[t, j] = NotImplemented  # Replace with logic for tracking the path\n",
    "\n",
    "        # TODO: Path backtracking\n",
    "        states = np.zeros(T, dtype=int)\n",
    "        states[-1] = NotImplemented  # Replace with logic for the final state\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            states[t] = NotImplemented  # Replace with path backtracking logic\n",
    "\n",
    "        return states\n",
    "\n",
    "# Main function with TODOs for students to complete\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the weather dataset\n",
    "    weather_df = NotImplemented # TODO Load the data using the 'parse_dates' and 'index_col' arguments\n",
    "    weather_df = NotImplemented  # TODO: Select 'temp_max', 'temp_min', and 'weather' columns and drop NaN values\n",
    "  \n",
    "\n",
    "    # Calculate the average temperature and encode the 'weather' column\n",
    "    weather_df['temp_avg'] = NotImplemented #TODO Calculate the average temperature\n",
    "\n",
    "    weather_mapping = {label: idx for idx, label in enumerate(weather_df['weather'].unique())}\n",
    "    reverse_weather_mapping = {v: k for k, v in weather_mapping.items()}\n",
    "    weather_df['weather_encoded'] = weather_df['weather'].map(weather_mapping)\n",
    "\n",
    "    # Convert average temperature to integer values for the observation sequence\n",
    "    temp_min = weather_df['temp_avg'].min()\n",
    "    O = (weather_df['temp_avg'] - temp_min).astype(int).values  # Observation sequence\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    train_size = int(0.8 * len(O))\n",
    "    O_train = O[:train_size]\n",
    "    O_test = O[train_size:]\n",
    "    actual_train = weather_df['weather_encoded'].values[:train_size]\n",
    "    actual_test = weather_df['weather_encoded'].values[train_size:]\n",
    "\n",
    "    # Initialize and train the HMM\n",
    "    num_states = len(weather_mapping)\n",
    "    num_observations = O.max() + 1\n",
    "    hmm = HiddenMarkovModel(num_states, num_observations)\n",
    "\n",
    "    # TODO: Train the HMM using Baum-Welch on training data\n",
    "    A_trained, B_trained, pi_trained = NotImplemented  \n",
    "\n",
    "    # TODO: Use the Viterbi algorithm for decoding the most likely state sequence\n",
    "    train_predicted_states = NotImplemented  \n",
    "    test_predicted_states = NotImplemented  \n",
    "\n",
    "    # Decode predicted states\n",
    "    train_decoded_states = NotImplemented  \n",
    "    test_decoded_states = NotImplemented  \n",
    "\n",
    "    # TODO: Create and display comparison DataFrames for train and test sets\n",
    "\n",
    "    # TODO: Evaluate accuracy for training and testing sets\n",
    "    train_accuracy = NotImplemented  \n",
    "    test_accuracy = NotImplemented  \n",
    "\n",
    "    print(\"\\nHMM Performance:\")\n",
    "    print(f\"Training Set Accuracy: {train_accuracy:.2%}\")\n",
    "    print(f\"Testing Set Accuracy: {test_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class HiddenMarkovModel:\n",
    "    def __init__(self, num_states, num_observations):\n",
    "        \"\"\"\n",
    "        Initializes the HMM with random transition, emission, and initial state probabilities.\n",
    "        \n",
    "        Args:\n",
    "            num_states (int): Number of hidden states.\n",
    "            num_observations (int): Number of unique observations.\n",
    "        \"\"\"\n",
    "        self.num_states = num_states\n",
    "        self.num_observations = num_observations\n",
    "        self.A = np.random.dirichlet(np.ones(num_states), num_states)  # Transition matrix\n",
    "        self.B = np.random.dirichlet(np.ones(num_observations), num_states)  # Emission matrix\n",
    "        self.pi = np.random.dirichlet(np.ones(num_states))  # Initial state distribution\n",
    "        \n",
    "        # Log-space versions to avoid underflow\n",
    "        self.A_log = np.log(self.A + 1e-10)\n",
    "        self.B_log = np.log(self.B + 1e-10)\n",
    "        self.pi_log = np.log(self.pi + 1e-10)\n",
    "\n",
    "    def forward_algorithm_log(self, O):\n",
    "        T = len(O)\n",
    "        N = self.num_states\n",
    "        alpha_log = np.zeros((T, N))\n",
    "\n",
    "        # Initialization\n",
    "        alpha_log[0] = self.pi_log + self.B_log[:, O[0]]\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                alpha_log[t, j] = np.logaddexp.reduce(alpha_log[t - 1] + self.A_log[:, j]) + self.B_log[j, O[t]]\n",
    "\n",
    "        return alpha_log\n",
    "\n",
    "    def backward_algorithm_log(self, O):\n",
    "        T = len(O)\n",
    "        N = self.num_states\n",
    "        beta_log = np.zeros((T, N))\n",
    "\n",
    "        # Initialization\n",
    "        beta_log[-1] = 0  # log(1) = 0\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            for i in range(N):\n",
    "                beta_log[t, i] = np.logaddexp.reduce(\n",
    "                    self.A_log[i] + self.B_log[:, O[t + 1]] + beta_log[t + 1]\n",
    "                )\n",
    "\n",
    "        return beta_log\n",
    "\n",
    "    def baum_welch_log(self, O, max_iter=100, epsilon=1e-6):\n",
    "        T = len(O)\n",
    "        for iteration in range(max_iter):\n",
    "            alpha_log = self.forward_algorithm_log(O)\n",
    "            beta_log = self.backward_algorithm_log(O)\n",
    "\n",
    "            # Compute gamma and xi in log-space\n",
    "            gamma_log = alpha_log + beta_log - np.logaddexp.reduce(alpha_log[-1])\n",
    "            xi_log = np.zeros((T - 1, self.num_states, self.num_states))\n",
    "\n",
    "            for t in range(T - 1):\n",
    "                xi_log[t] = self.A_log + self.B_log[:, O[t + 1]] + beta_log[t + 1] + alpha_log[t].reshape(-1, 1)\n",
    "                xi_log[t] -= np.logaddexp.reduce(xi_log[t].flatten())\n",
    "\n",
    "            # Update transition, emission, and initial probabilities\n",
    "            self.A_log = np.logaddexp.reduce(xi_log, axis=0) - np.logaddexp.reduce(gamma_log[:-1], axis=0).reshape(-1, 1)\n",
    "            self.B_log = np.zeros_like(self.B_log)\n",
    "            for k in range(self.num_observations):\n",
    "                mask = (O == k)\n",
    "                self.B_log[:, k] = np.logaddexp.reduce(gamma_log[mask], axis=0) - np.logaddexp.reduce(gamma_log, axis=0)\n",
    "            self.pi_log = gamma_log[0]\n",
    "\n",
    "        return np.exp(self.A_log), np.exp(self.B_log), np.exp(self.pi_log)\n",
    "\n",
    "    def viterbi_algorithm_log(self, O):\n",
    "        T = len(O)\n",
    "        N = self.num_states\n",
    "        delta_log = np.zeros((T, N))\n",
    "        psi = np.zeros((T, N), dtype=int)\n",
    "\n",
    "        # Initialization\n",
    "        delta_log[0] = self.pi_log + self.B_log[:, O[0]]\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(N):\n",
    "                scores = delta_log[t - 1] + self.A_log[:, j]\n",
    "                delta_log[t, j] = np.max(scores) + self.B_log[j, O[t]]\n",
    "                psi[t, j] = np.argmax(scores)\n",
    "\n",
    "        # Path backtracking\n",
    "        states = np.zeros(T, dtype=int)\n",
    "        states[-1] = np.argmax(delta_log[-1])\n",
    "        for t in range(T - 2, -1, -1):\n",
    "            states[t] = psi[t + 1, states[t + 1]]\n",
    "\n",
    "        return states\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess the weather dataset\n",
    "    weather_df = pd.read_csv(\"weather.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "    weather_df = weather_df[[\"temp_max\", \"temp_min\", \"weather\"]].dropna()\n",
    "\n",
    "    # Calculate the average temperature and encode the 'weather' column\n",
    "    weather_df[\"temp_avg\"] = (weather_df[\"temp_max\"] + weather_df[\"temp_min\"]) / 2\n",
    "    weather_mapping = {label: idx for idx, label in enumerate(weather_df[\"weather\"].unique())}\n",
    "    reverse_weather_mapping = {v: k for k, v in weather_mapping.items()}\n",
    "    weather_df[\"weather_encoded\"] = weather_df[\"weather\"].map(weather_mapping)\n",
    "\n",
    "    # Convert average temperature to integer values for the observation sequence\n",
    "    temp_min = weather_df[\"temp_avg\"].min()\n",
    "    O = (weather_df[\"temp_avg\"] - temp_min).astype(int).values\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    train_size = int(0.8 * len(O))\n",
    "    O_train, O_test = O[:train_size], O[train_size:]\n",
    "    actual_train = weather_df[\"weather_encoded\"].values[:train_size]\n",
    "    actual_test = weather_df[\"weather_encoded\"].values[train_size:]\n",
    "\n",
    "    # Initialize and train the HMM\n",
    "    num_states = len(weather_mapping)\n",
    "    num_observations = O.max() + 1\n",
    "    hmm = HiddenMarkovModel(num_states, num_observations)\n",
    "    A_trained, B_trained, pi_trained = hmm.baum_welch_log(O_train)\n",
    "\n",
    "    # Decode the most likely state sequence\n",
    "    train_predicted_states = hmm.viterbi_algorithm_log(O_train)\n",
    "    test_predicted_states = hmm.viterbi_algorithm_log(O_test)\n",
    "\n",
    "    # Decode predicted states to weather labels\n",
    "    train_decoded_states = [reverse_weather_mapping[s] for s in train_predicted_states]\n",
    "    test_decoded_states = [reverse_weather_mapping[s] for s in test_predicted_states]\n",
    "\n",
    "    # Evaluate accuracy\n",
    "    train_accuracy = np.mean(train_predicted_states == actual_train)\n",
    "    test_accuracy = np.mean(test_predicted_states == actual_test)\n",
    "\n",
    "    print(\"\\nHMM Performance:\")\n",
    "    print(f\"Training Set Accuracy: {train_accuracy:.2%}\")\n",
    "    print(f\"Testing Set Accuracy: {test_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Markov Chain for Stock Price Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataAnalysis:\n",
    "    def __init__(self, ticker, start_date, end_date, n_states=5):\n",
    "        \"\"\"\n",
    "        Initialize the StockDataAnalysis with stock parameters and state count.\n",
    "        \n",
    "        Args:\n",
    "            ticker (str): Stock ticker symbol.\n",
    "            start_date (datetime): Start date for data retrieval.\n",
    "            end_date (datetime): End date for data retrieval.\n",
    "            n_states (int): Number of quantile-based states.\n",
    "        \"\"\"\n",
    "        self.ticker = ticker\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.n_states = n_states\n",
    "        self.df = None\n",
    "        self.transition_matrix = None\n",
    "\n",
    "    def get_stock_data(self):\n",
    "        \"\"\"\n",
    "        Download historical stock data from Yahoo Finance and preprocess it.\n",
    "        \"\"\"\n",
    "        stock = yf.Ticker(self.ticker)\n",
    "        df = stock.history(start=self.start_date, end=self.end_date)\n",
    "        df['Daily_Return'] = NotImplemented #TODO Extract the close price and calculate daily returns\n",
    "        self.df = NotImplemented  #TODO Remove NaN values\n",
    "        print(f\"Stock data for {self.ticker} loaded successfully.\")\n",
    "\n",
    "    def assign_states(self):\n",
    "        \"\"\"\n",
    "        Assign states based on return quantiles and add descriptive labels.\n",
    "        \"\"\"\n",
    "        self.df['State'] = pd.qcut(self.df['Daily_Return'], q=self.n_states, labels=False)\n",
    "        state_labels = NotImplemented #TODO Initialize state lables in a list in the order described in the question\n",
    "        self.df['State_Description'] = pd.Categorical(\n",
    "            [state_labels[int(i)] for i in self.df['State']]\n",
    "        )\n",
    "        print(\"States assigned based on quantiles.\")\n",
    "\n",
    "    def get_transition_matrix(self):\n",
    "        \"\"\"\n",
    "        Construct the transition probability matrix from the sequence of states.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Transition matrix (dimensions: n_states x n_states).\n",
    "        \n",
    "        TODO:\n",
    "        - Ensure that states are properly handled as integers without NaNs.\n",
    "        - Implement a loop or vectorized operation to count transitions between states.\n",
    "        - Normalize the rows of the transition matrix to get probabilities.\n",
    "        \"\"\"\n",
    "        # TODO: Extract the 'State' column from the DataFrame as a numpy array.\n",
    "        states = NotImplemented # Ensure this is done without NaNs if needed.\n",
    "\n",
    "        # Initialize a matrix to store transition counts.\n",
    "        transitions = NotImplemented \n",
    "\n",
    "        # TODO: Iterate through the array of states and populate the transition matrix.\n",
    "        # Hint: Use a loop to increment the count for transitions from states[i] to states[i + 1].\n",
    "        for i in range(len(states) - 1):\n",
    "            current_state = NotImplemented  # Ensure valid integer index\n",
    "            next_state = NotImplemented  # Ensure valid integer index\n",
    "            transitions[current_state][next_state] += 1  # Increment the count for observed transitions.\n",
    "\n",
    "        \n",
    "        epsilon = 1e-8\n",
    "        row_sums = transitions.sum(axis=1) + epsilon\n",
    "\n",
    "        # TODO: Normalize each row of the transition matrix to create probabilities.\n",
    "        # Hint: Divide each element in a row by the sum of the row to ensure each row sums to 1.\n",
    "        self.transition_matrix = NotImplemented\n",
    "\n",
    "        return self.transition_matrix\n",
    "\n",
    "\n",
    "    def get_stationary_distribution(self):\n",
    "        \"\"\"\n",
    "        Calculate the stationary distribution of the Markov chain.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Stationary distribution (dimensions: n_states).\n",
    "        \"\"\"\n",
    "        eigenvals, eigenvects = NotImplemented # TODO Calculate the eigenvalues and eigenvectors\n",
    "        stationary = eigenvects[:, np.where(np.isclose(eigenvals, 1))[0][0]].real\n",
    "        stationary = NotImplemented #TODO Normalize the stationary distribution\n",
    "        print(\"Stationary distribution calculated.\")\n",
    "        return stationary\n",
    "\n",
    "    def analyze_markov_chain(self):\n",
    "        \"\"\"\n",
    "        Analyze and visualize Markov chain properties.\n",
    "        \"\"\"\n",
    "        stationary_dist = NotImplemented # TODO Calculate the stationary distribution\n",
    "        state_labels = NotImplemented #TODO Initialize state lables in a list in the order described in the question\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        sns.heatmap(self.transition_matrix, annot=True, fmt='.2f', cmap='YlOrRd', ax=ax1,\n",
    "                    xticklabels=state_labels, yticklabels=state_labels)\n",
    "        ax1.set_title('Transition Probability Matrix')\n",
    "        ax1.set_xlabel('Next State')\n",
    "        ax1.set_ylabel('Current State')\n",
    "\n",
    "        bars = ax2.bar(state_labels, stationary_dist)\n",
    "        ax2.set_title('Stationary Distribution')\n",
    "        ax2.set_xlabel('State')\n",
    "        ax2.set_ylabel('Probability')\n",
    "        ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax2.text(bar.get_x() + bar.get_width() / 2., height,\n",
    "                     f'{height * 100:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nMarkov Chain Analysis Complete.\")\n",
    "        print(\"\\nTransition Matrix:\")\n",
    "        transition_df = pd.DataFrame(self.transition_matrix,\n",
    "                                     columns=state_labels,\n",
    "                                     index=state_labels)\n",
    "        print(transition_df)\n",
    "\n",
    "        print(\"\\nStationary Distribution:\")\n",
    "        stationary_df = pd.DataFrame({\n",
    "            'State': state_labels,\n",
    "            'Probability': stationary_dist\n",
    "        })\n",
    "        print(stationary_df)\n",
    "\n",
    "    def simulate_price_path(self, initial_price, days):\n",
    "        \"\"\"\n",
    "        Simulate a single price path using the Markov chain model.\n",
    "        \n",
    "        Args:\n",
    "            initial_price (float): Starting price for the simulation.\n",
    "            days (int): Number of days to simulate.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: Simulated price path.\n",
    "        \n",
    "        TODO:\n",
    "        - Implement the logic for price path simulation based on state transitions.\n",
    "        \"\"\"\n",
    "        prices = [initial_price]\n",
    "        current_state = np.random.choice(self.n_states)\n",
    "\n",
    "        for _ in range(days):\n",
    "            next_state = np.random.choice(self.n_states, p=self.transition_matrix[current_state])\n",
    "            return_pct = np.random.choice(self.df[self.df['State'] == next_state]['Daily_Return'])\n",
    "            next_price = prices[-1] * (1 + return_pct / 100)\n",
    "            prices.append(next_price)\n",
    "            current_state = next_state\n",
    "\n",
    "        return np.array(prices)\n",
    "\n",
    "    def perform_simulations(self, n_simulations=1000, forecast_days=100):\n",
    "        \"\"\"\n",
    "        Perform multiple price simulations using the Markov model.\n",
    "        \n",
    "        TODO:\n",
    "        - Store and return simulation results.\n",
    "        \"\"\"\n",
    "        initial_price = self.df['Close'].iloc[-1] \n",
    "\n",
    "        simulations = np.zeros((n_simulations, forecast_days + 1))\n",
    "\n",
    "        for i in range(n_simulations):\n",
    "            simulations[i] = NotImplemented #TODO call the simulate_price_path method\n",
    "\n",
    "        print(f\"Performed {n_simulations} simulations for {forecast_days} days.\")\n",
    "        return simulations\n",
    "\n",
    "    def backtest(df, transition_matrix, n_simulations=100):\n",
    "        \"\"\"\n",
    "        Perform walk-forward backtesting using the Markov model.\n",
    "        \n",
    "        TODO:\n",
    "        - Implement rolling window logic to create a dynamic transition matrix for each backtest step.\n",
    "        - Ensure return distributions are created for each state within the rolling window.\n",
    "        - Simulate next-day prices using the dynamic transition matrix and return distributions.\n",
    "        \"\"\"\n",
    "        window_size = 252  # One trading year\n",
    "        n_states = transition_matrix.shape[0]\n",
    "        predictions = np.zeros((len(df), n_simulations))\n",
    "\n",
    "        # TODO: Iterate through the dataset, starting from the end of the initial rolling window.\n",
    "        for i in range(window_size, len(df)):\n",
    "            # TODO: Extract a rolling window of the data to calculate the transition matrix.\n",
    "            # Hint: Use `iloc` to get rows from `i - window_size` to `i`.\n",
    "            window_data = NotImplemented  \n",
    "\n",
    "            # Check if the window data has enough valid states to build a transition matrix.\n",
    "            if window_data['State'].isna().any() or len(window_data['State'].unique()) < n_states:  # Replace with a condition to check for data sufficiency.\n",
    "                continue\n",
    "\n",
    "            # TODO: Create a transition matrix from the rolling window data.\n",
    "            # Hint: Call a function like `get_transition_matrix` with the states in `window_data`.\n",
    "            window_transitions = NotImplemented  \n",
    "\n",
    "            # Create return distributions for each state, ensuring non-empty arrays\n",
    "            returns_by_state = [\n",
    "               window_data[window_data['State'] == state]['Daily_Return'].values\n",
    "                for state in range(n_states)\n",
    "            ]\n",
    "\n",
    "            # TODO: Simulate next-day prices using the current price and the transition matrix.\n",
    "            # Hint: Use `simulate_price_path` for the simulation.\n",
    "            current_price = df['Close'].iloc[i - 1]\n",
    "            for sim in range(n_simulations):\n",
    "                sim_path = NotImplemented  \n",
    "                predictions[i, sim] = NotImplemented  \n",
    "\n",
    "        # TODO: Return or process the predictions as needed for analysis.\n",
    "        valid_indices = np.where(predictions.sum(axis=1) != 0)[0]\n",
    "        prediction_mean = np.mean(predictions[valid_indices], axis=1)\n",
    "        prediction_std = NotImplemented\n",
    "        lower_bound = NotImplemented\n",
    "        upper_bound = NotImplemented\n",
    "\n",
    "        return prediction_mean, lower_bound, upper_bound\n",
    "\n",
    "\n",
    "    def plot_results(df, simulations, backtest_results):\n",
    "        \"\"\"\n",
    "        Create visualization of historical prices, backtest results, and future simulations.\n",
    "        \"\"\"\n",
    "        prediction_mean, lower_bound, upper_bound = backtest_results\n",
    "        \n",
    "        # Adjust the prediction arrays to match the index length\n",
    "        valid_index = len(prediction_mean)\n",
    "        df_index_subset = df.index[-valid_index:]  # Ensure the x-axis matches the length of the prediction arrays\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "        \n",
    "        # Plot backtesting results\n",
    "        ax1.plot(df_index_subset, df['Close'].iloc[-valid_index:], color='black', label='Actual Prices')\n",
    "        ax1.plot(df_index_subset, prediction_mean, color='blue', linestyle='--', label='Model Prediction')\n",
    "        ax1.fill_between(df_index_subset, lower_bound, upper_bound, color='blue', alpha=0.1, label='95% Confidence Interval')\n",
    "        ax1.set_title('Backtesting Results')\n",
    "        ax1.set_xlabel('Date')\n",
    "        ax1.set_ylabel('Price ($)')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # Plot future simulations\n",
    "        future_dates = pd.date_range(\n",
    "            df.index[-1], \n",
    "            periods=simulations.shape[1],\n",
    "            freq='B'\n",
    "        )\n",
    "        \n",
    "        # Plot individual simulations\n",
    "        for sim in simulations[:50]:  # Plot first 50 simulations for clarity\n",
    "            ax2.plot(future_dates, sim, alpha=0.1, color='gray')\n",
    "        \n",
    "        # Plot confidence intervals for simulations\n",
    "        sim_mean = np.mean(simulations, axis=0)\n",
    "        sim_std = np.std(simulations, axis=0)\n",
    "        ax2.plot(future_dates, sim_mean, color='red', label='Mean Forecast')\n",
    "        ax2.fill_between(\n",
    "            future_dates,\n",
    "            sim_mean - 1.96 * sim_std,\n",
    "            sim_mean + 1.96 * sim_std,\n",
    "            color='red', alpha=0.1,\n",
    "            label='95% Confidence Interval'\n",
    "        )\n",
    "        \n",
    "        # Plot historical prices for context\n",
    "        ax2.plot(df.index[-30:], df['Close'][-30:], color='black', label='Historical Prices')\n",
    "        \n",
    "        ax2.set_title('Future Price Simulations')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.set_ylabel('Price ($)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def generate_summary_stats(df, simulations, current_price):\n",
    "        \"\"\"\n",
    "        Generate and display summary statistics for future price simulations using tabulate.\n",
    "        \"\"\"\n",
    "        # Calculate average predicted price\n",
    "        final_prices = simulations[:, -1]  # Last price from each simulation\n",
    "        mean_final_price = np.mean(final_prices)\n",
    "        min_final_price = np.min(final_prices)\n",
    "        max_final_price = np.max(final_prices)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        lower_90 = np.percentile(final_prices, 5)\n",
    "        upper_90 = np.percentile(final_prices, 95)\n",
    "        lower_95 = np.percentile(final_prices, 2.5)\n",
    "        upper_95 = np.percentile(final_prices, 97.5)\n",
    "        \n",
    "        # Calculate simulated volatility\n",
    "        simulated_volatility = np.std(simulations) / np.mean(simulations) * 100  # Percentage format\n",
    "        \n",
    "        # Historical volatility (standard deviation of daily returns)\n",
    "        historical_volatility = df['Daily_Return'].std()\n",
    "        \n",
    "        # Create the summary statistics table\n",
    "        summary_stats = [\n",
    "            ['Current Price', f\"${current_price:.2f}\"],\n",
    "            ['Predicted Price (in 100 days)', f\"${mean_final_price:.2f}\"],\n",
    "            ['Average Predicted Price (100 days)', f\"${np.mean(final_prices):.2f}\"],\n",
    "            ['Prediction Range', f\"${min_final_price:.2f} - ${max_final_price:.2f}\"],\n",
    "            ['Historical Volatility (%)', f\"{historical_volatility:.2f}%\"],\n",
    "            ['Simulated Volatility (%)', f\"{simulated_volatility:.2f}%\"],\n",
    "            ['90% Confidence Interval (Final Price)', f\"${lower_90:.2f} - ${upper_90:.2f}\"],\n",
    "            ['95% Confidence Interval (Final Price)', f\"${lower_95:.2f} - ${upper_95:.2f}\"]\n",
    "        ]\n",
    "        \n",
    "        # Print the summary statistics using tabulate\n",
    "        print(\"\\nFuture Prediction Statistics:\")\n",
    "        return(tabulate(summary_stats, headers=['Metric', 'Value'], tablefmt='psql'))\n",
    "     \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ticker = NotImplemented #TODO replace with Apple Inc. ticker\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=2 * 365)\n",
    "\n",
    "    stock_analysis = StockDataAnalysis(ticker, start_date, end_date)\n",
    "\n",
    "    stock_analysis.get_stock_data()\n",
    "    stock_analysis.assign_states()\n",
    "    stock_analysis.get_transition_matrix()\n",
    "    stock_analysis.analyze_markov_chain()\n",
    "\n",
    "    simulations = stock_analysis.perform_simulations(n_simulations=1000, forecast_days=100)\n",
    "    backtest_results = stock_analysis.backtest(df = stock_analysis.df, transition_matrix=stock_analysis.transition_matrix, n_simulations=100)\n",
    "\n",
    "    # TODO: Call plot_results and generate_summary_stats methods\n",
    "    current_price = NotImplemented # Extract the current closing stock price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
