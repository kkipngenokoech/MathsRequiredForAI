\documentclass[a3paper,12pt]{extarticle} % Use extarticle for A3 paper size
\usepackage{amsmath}
\usepackage{amssymb} % Include this package for \mathbb
\usepackage[margin=1in]{geometry} % Adjust the margin as needed

\begin{document}

\author{kipngeno koech - bkoech}
\title{Homework 3 - Applied Stochastic Processes}   
\maketitle

\medskip

\maketitle
\begin{center}
    \large \textbf{Question 1: Random Vectors and Principal Component Analysis}
\end{center}
Reading: Random vectors are fundamental constructs in probability and statistics, allowing researchers
and practitioners to analyze relationships among multiple variables simultaneously. Each component of a
random vector can represent a different feature or measurement, and the joint distribution encapsulates
the uncertainty inherent in those variables.

For instance, consider a random vector \(X\) - \(X_1, X_2,...,X_n\) where each \(X_i\) is a random variable. The covariance matrix of \(X\) plays a crucial role in understanding the linear relationships among the
components, guiding decisions in fields such as finance, machine learning, and signal processing.
Sampling from random vectors introduces excitement in multivariate analyses, where one can explore
properties like independence, marginal distributions, and conditional relationships. Moreover, techniques
such as principal component analysis (PCA) leverage the variance structure of these vectors to reduce
dimensionality while preserving essential information.
\begin{enumerate}
    \item \textbf{5 points} let \(X = \begin{pmatrix}X_1\\X_2 \\X_3\end{pmatrix}\), and \(Y = \begin{pmatrix}Y_1\\Y_2 \\Y_3\end{pmatrix}\)  are related by \(Y - AX\) where
    \[A = \begin{bmatrix}1 & 0 & 0\\-1 & 1 & 0\\0 & -1 & 1\end{bmatrix}\]
    The joint PMF of X is given by:
    \[
    P_X(X) = 
    \begin{cases} 
    (1-p)p^{x_3} & \text{if } X_1 < X_2 < X_3 \\
    0 & \text{otherwise}
    \end{cases}
    \]
    where \(x_1, x_2, x_3 \in \{0, 1, 2, \ldots\}\) and \(0 < p < 1\).
    \\ Find the joint PMF \(P_Y(y)\) of the transformed random vector \(Y\).
    \[
    Y = AX = \begin{bmatrix}1 & 0 & 0\\-1 & 1 & 0\\0 & -1 & 1\end{bmatrix} \begin{pmatrix}X_1\\X_2 \\X_3\end{pmatrix} = \begin{pmatrix}X_1\\X_2 - X_1 \\X_3 - X_2\end{pmatrix}
    \]
    \[
    X_1 = Y_1, X_2 = Y_1 + Y_2, X_3 = Y_1 + Y_2 + Y_3
    \]
    \[
    P_Y(y) = P_X(A^{-1}y) = P_X\left(\begin{pmatrix}y_1\\y_2 \\y_3\end{pmatrix}\right) = P_X\left(\begin{pmatrix}y_1\\y_1 + y_2 \\y_1 + y_2 + y_3\end{pmatrix}\right)
    \]
    conditions for \(X_1 < X_2 < X_3\) to hold:
    \[
    y_1 < y_1 + y_2 < y_1 + y_2 + y_3
    \]
    \[
    0 < y_2 < y_3
    \]
    \[
    P_Y(y) = (1-p)p^{y_1 + y_2 + y_3} = (1-p)p^{y_1}p^{y_2}p^{y_3}
    \]
    \[
    P_Y(y) = (1-p)p^{y_1}p^{y_2}p^{y_3}
    \]
    \item You are working as a data analyst for a startup that collects various statistics from users’ activities
    on its platform. The startup wants to reduce the dimensionality of its collected data without
    losing significant information. Your goal is to apply Principal Component Analysis (PCA) to the
    dataset to retain as much variance (information) as possible while reducing the dimensionality.
    This exercise will take you from the conceptual understanding of random vectors and covariance
    matrices to the practical application of PCA using Python.
    \subitem \textbf{Part 1: understanding the covariance Matrix of Random Vectors (12 points)}
    \\ You are given a random vector \(X = [X_1, X_2, X_3, X_4]^{T}\), representing four features of platforms users. The covariance matrix of this random vector is:
    \[
    \Sigma_x = \begin{bmatrix}5 & 1.2 & 0.8 & 0.6\\1.2 & 4 & 0.5 & 0.3\\0.8 & 0.5 & 3 & 0.2\\0.6 & 0.3 & 0.2 & 2\end{bmatrix}
    \]
    \\ \subitem \textbf{Intepretation of the covariance matrix}
    \begin{enumerate}
        \item \textbf{(2 points)} What do the diagonal elements of the covariance matrix represent?
        \[
            \text{The diagonal elements of the covariance matrix represent the variance of the individual features.}
        \]
        \item \textbf{(2 points)} What do the off-diagonal elements signify in terms of the relationship between the features?
        \[
            \text{The off-diagonal elements signify the covariance between the features.}
        \]
    \end{enumerate}
    \subitem \textbf{Random Vector and Variance}
    \begin{enumerate}
        \item \textbf{(2 points)} Calculate the total variance of the random vector \(X\).
        \[
        \text{Total Variance} = \text{Trace}(\Sigma_x) = 5 + 4 + 3 + 2 = \textbf{14}
        \]
        \item \textbf{(2 points)} How would you compute the variance captured by a single feature ( e.g, the first feature \(X_1\))?
        \[
        \text{Variance of } X_1 = \Sigma_{11} = \textbf{5}
        \]
    \end{enumerate}
    \subitem \textbf{Eigenvalues and Eigenvectors of the Covariance Matrix}
    \begin{enumerate}
        \item \textbf{(2 points)} Calculate the eigenvalues and eigenvectors of the covariance matrix  \(\Sigma_x\) by hand
        \\ method used to calculate eigenvalues and eigenvectors is eigen decomposition:
        \\ The characteristic equation is given by:
        \[
        \text{det}(\Sigma_x - \lambda I) = 0
        \]
        \[
        \begin{vmatrix}5-\lambda & 1.2 & 0.8 & 0.6\\1.2 & 4-\lambda & 0.5 & 0.3\\0.8 & 0.5 & 3-\lambda & 0.2\\0.6 & 0.3 & 0.2 & 2-\lambda\end{vmatrix} = 0
        \]
        \[
            \lambda^4 - 14\lambda^3 + 68.18\lambda^2 - 139.254\lambda + 101.356 = 0
        \]
        \[
            \lambda_1 = \textbf{6.20306}, \lambda_2 = \textbf{3.20619}, \lambda_3 = \textbf{2.71066}, \lambda_4 = \textbf{1.88009}
        \]
        To calculate the eigen vectors, we substitute the eigen values into the equation:
        \\ The eigen vectors are:
        \\ for eigen value \(\lambda_1 = 6.20306\)
        \[
        \begin{bmatrix}-1.20306 & 1.2 & 0.8 & 0.6\\1.2 & -2.20306 & 0.5 & 0.3\\0.8 & 0.5 & -3.20306 & 0.2\\0.6 & 0.3 & 0.2 & -4.20306\end{bmatrix} \begin{bmatrix}v_1\\v_2\\v_3\\v_4\end{bmatrix} = \begin{bmatrix}0\\0\\0\\0\end{bmatrix}
        \]
        The augmented matrix is:
        \[
        \begin{bmatrix}-1.20306 & 1.2 & 0.8 & 0.6 & | & 0\\1.2 & -2.20306 & 0.5 & 0.3 & | & 0\\0.8 & 0.5 & -3.20306 & 0.2 & | & 0\\0.6 & 0.3 & 0.2 & -4.20306 & | & 0\end{bmatrix}
        \]
        \[
        R_1 = \frac{1}{-1.20306}R_1 = \begin{bmatrix}1 & -0.997 & -0.66497 & -0.4987 & | & 0\\1.2 & -2.20306 & 0.5 & 0.3 & | & 0\\0.8 & 0.5 & -3.20306 & 0.2 & | & 0\\0.6 & 0.3 & 0.2 & -4.20306 & | & 0\end{bmatrix}
        \]
        \[
        R_2 = R_2 - 1.2R_1 = \begin{bmatrix}1 & -0.997 & -0.66497 & -0.4987 & | & 0\\0 & -1.00611 & 1.297965 & 0.89847 & | & 0\\0.8 & 0.5 & -3.20306 & 0.2 & | & 0\\0.6 & 0.3 & 0.2 & -4.20306 & | & 0\end{bmatrix}
        \]
        \item \textbf{(2 points)} List the eigenvalues in descending order and explain what they represent in terms of variance
        \[
        \lambda_1 = \textbf{6.20306}, \lambda_2 = \textbf{3.20619}, \lambda_3 = \textbf{2.71066}, \lambda_4 = \textbf{1.88009}
        \]
        The eigen values represent the variance of the data along the principal components. The first eigen value \(\lambda_1 = 6.20306\) represents the variance of the data along the first principal component, the second eigen value \(\lambda_2 = 3.20619\) represents the variance of the data along the second principal component, the third eigen value \(\lambda_3 = 2.71066\) represents the variance of the data along the third principal component, and the fourth eigen value \(\lambda_4 = 1.88009\) represents the variance of the data along the fourth principal component.
    \end{enumerate} 
    \subitem \textbf{Part 2: Principal Component Analysis (PCA) (8 points)}
    \\ Now that you have a grasp of the covariance matrix and its eigenvalues, you will apply PCA to a random vector
    \\ \subitem \textbf{Principal Component Directions}
    \begin{enumerate}
    \item \textbf{(2 points)} Using the eigenvectors, describe the principal component directions. What do these directions represent in terms of variance in the data?
    \item \textbf{(2 points)} Explain the concept of orthogonality in PCA and why is it important?
    \\\\ Orthogonality in PCA means that the principal components are perpendicular to each other. This is important because it ensures that the principal components are independent of each other. This means that the variance of the data is maximized along the principal components.
    \end{enumerate}
    \subitem \textbf{Transformation of Random Vector}
    \\ Let the eigenvector matrix be \(P\) and defined the transformed random vector \(Y\) by \(Y = P^TX\)
    \begin{enumerate}
    \item \textbf{(2 points)} What is the covariance matrix of the  vector \(Y\)?
    \[
    \Sigma_Y = P^T\Sigma_xP
    \]
    \item \textbf{(2 points)} How does this Transformation affect the correlation between the transformed features?
    \\ The transformation affects the correlation between the transformed features by making them uncorrelated. The covariance matrix of the transformed random vector \(Y\) is a diagonal matrix, which means that the transformed features are uncorrelated.c
    \end{enumerate}
    \subitem \textbf{Part 3: Performing PCA by Hand on a Simple Dataset (8 points)}
    consider a simple dataset represented by the following 2-dimensional random vector \(Y = [Y_1, Y_2]^{T}\):
    \[
    Y = \begin{bmatrix}1.2 & 2.8\\0.8 & 2.4\\1.6 & 3.2\\1.4 & 2.9\end{bmatrix}
    \]
    \subitem \textbf{Mean Centering}
    \begin{enumerate}
        \item \textbf{(2 points)} Calculate the mean of the dataset for each feature \(Y_1 \text{ and } Y_2\)
        \[
        \text{Mean of } Y_1 = \frac{1.2 + 0.8 + 1.6 + 1.4}{4} = \textbf{1.25}
        \]
        \[
        \text{Mean of } Y_2 = \frac{2.8 + 2.4 + 3.2 + 2.9}{4} = \textbf{2.825}
        \]
        \item \textbf{(2 points)} Subtract the mean from each feature to center the data
        \[
        \text{Centered Data} = \begin{bmatrix}1.2 - 1.25 & 2.8 - 2.825\\0.8 - 1.25 & 2.4 - 2.825\\1.6 - 1.25 & 3.2 - 2.825\\1.4 - 1.25 & 2.9 - 2.825\end{bmatrix} = \begin{bmatrix}-0.05 & -0.025\\-0.45 & -0.425\\0.35 & 0.375\\0.15 & 0.075\end{bmatrix}
        \]
    \end{enumerate}
    \subitem \textbf{Covariance Matrix (2 points)}
    \begin{enumerate}
        \item Calculate the covariance matrix of the centered data
    \[
    \text{Covariance Matrix} = \frac{1}{n-1} \text{Centered Data}^T \text{Centered Data}
    \]
    \[
    \text{Covariance Matrix} = \frac{1}{4-1} \begin{bmatrix}-0.05 & -0.025\\-0.45 & -0.425\\0.35 & 0.375\\0.15 & 0.075\end{bmatrix}^T \begin{bmatrix}-0.05 & -0.025\\-0.45 & -0.425\\0.35 & 0.375\\0.15 & 0.075\end{bmatrix}
    \]
    \[
    \text{Covariance Matrix} = \frac{1}{3} \begin{bmatrix}-0.05 & -0.45 & 0.35 & 0.15\\-0.025 & -0.425 & 0.375 & 0.075\end{bmatrix} \begin{bmatrix}-0.05 & -0.025\\-0.45 & -0.425\\0.35 & 0.375\\0.15 & 0.075\end{bmatrix}
    \]
    \[
    \text{Covariance Matrix} = \frac{1}{3} \begin{bmatrix}0.35  & 0.335\\0.335 & 0.328\end{bmatrix}
    \]
    \[
    \text{Covariance Matrix} = \begin{bmatrix}0.1167  & 0.1117\\0.1117 & 0.1093\end{bmatrix}
    \]
    \end{enumerate}
    \subitem \textbf{Eigenvalue decomposition (2 points)}
    \begin{enumerate}
        \item Manually compute the eigenvalues and eigenvectors of the covariance matrix
        \\ The Eigen values are obtained by solving the characteristic equation:
        \[
        \text{Characteristic Equation: } \text{det}(\Sigma - \lambda I) = 0
        \]
        \[
        \begin{vmatrix}0.1167 - \lambda & 0.1117\\0.1117 & 0.1093 - \lambda\end{vmatrix} = 0
        \]
        \[
        \lambda^2 - 0.226\lambda + 0.0003 = 0
        \]
        \[
        \lambda_1 = 0.225, \lambda_2 = 0.0013
        \]
        The Eigen vectors are obtained by solving the equation:
        \\ for eigen value \(\lambda_1 = 0.225\)
        \[
            \begin{bmatrix}0.1167 - 0.225 & 0.1117\\0.1117 & 0.1093 - 0.225\end{bmatrix} \begin{bmatrix}v_1\\v_2\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix}
        \]
        \[
            \begin{bmatrix}-0.1083 & 0.1117\\0.1117 & -0.1157\end{bmatrix} \begin{bmatrix}v_1\\v_2\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix}
        \]
        \[
            R_1 = \frac{1}{-0.1083}R_1 = \begin{bmatrix}1 & -1.031 & | & 0\\0.1117 & -0.1157 & | & 0\end{bmatrix}
        \]
        \[
            R_2 = R_2 - 0.1117R_1 = \begin{bmatrix}1 & -1.031 & | & 0\\0 & -0.0005 & | & 0\end{bmatrix}
        \]
        \[
        R_2 = \frac{1}{-0.0005}R_2 = \begin{bmatrix}1 & -1.031 & | & 0\\0 & 1 & | & 0\end{bmatrix}
        \]
        \[
        v_1 = 1.031v_2
        \]
        \[
        v_1 = \begin{bmatrix}1.031\\1\end{bmatrix}
        \]
        for eigen value \(\lambda_2 = 0.0013\)
        \[
            \begin{bmatrix}0.1167 - 0.0013 & 0.1117\\0.1117 & 0.1093 - 0.0013\end{bmatrix} \begin{bmatrix}v_1\\v_2\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix}
        \]
        \[
            \begin{bmatrix}0.1154 & 0.1117\\0.1117 & 0.1080\end{bmatrix} \begin{bmatrix}v_1\\v_2\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix}
        \]
        \[
            R_1 = \frac{1}{0.1154}R_1 = \begin{bmatrix}1 & 0.968 & | & 0\\0.1117 & 0.1080 & | & 0\end{bmatrix}
        \]
        \[
            R_2 = R_2 - 0.1117R_1 = \begin{bmatrix}1 & 0.968 & | & 0\\0 & -0.0001256 & | & 0\end{bmatrix}
        \]
        \[
        R_2 = \frac{1}{-0.0001256}R_2 = \begin{bmatrix}1 & 0.968 & | & 0\\0 & 1 & | & 0\end{bmatrix}
        \]
        \[
        v_1 = -0.968v_2
        \]
        \[
        v_1 = \begin{bmatrix}-0.968\\1\end{bmatrix}
        \]
        so the eigen vectors are as follows:
        \[
        \lambda_1 = 0.225, v_1 = \begin{bmatrix}1.031\\1\end{bmatrix}
        \]
        \[
        \lambda_2 = 0.0013, v_2 = \begin{bmatrix}-0.968\\1\end{bmatrix}
        \]
    \end{enumerate}
    \subitem \textbf{Project the data (2 points)}
    \begin{enumerate}
        \item Using the principal component corresponding to the largest eigenvalue, project the original data onto the principal component axis
        \[
        \text{Projection} = \text{Centered Data} \times v_1
        \]
        \[
        \text{Projection} = \begin{bmatrix}-0.05 & -0.025\\-0.45 & -0.425\\0.35 & 0.375\\0.15 & 0.075\end{bmatrix} \begin{bmatrix}1.031\\1\end{bmatrix}
        \]
        \[
        \text{Projection} = \begin{bmatrix}-0.05 \times 1.031 + -0.025 \times 1\\-0.45 \times 1.031 + -0.425 \times 1\\0.35 \times 1.031 + 0.375 \times 1\\0.15 \times 1.031 + 0.075 \times 1\end{bmatrix}
        \]
        \[
        \text{Projection} = \begin{bmatrix}-0.07655\\-0.88895\\0.73585\\0.22965\end{bmatrix}
        \]
        \item Show the final transformed data in 1D (along the principal component axis)
        \[
        \text{Final Transformed Data} = \begin{bmatrix}-0.07655\\-0.88895\\0.73585\\0.22965\end{bmatrix}
        \]
    \end{enumerate}
    \subitem \textbf{PCA in Practice with a Large Dataset (10 points)} 
    \\ You are now provided with a dataset consisting of 500 users, where each user has four features: Usage time, Interactions, Activity type 1, Activity type 2. You will apply PCA using python to reduce the dimensionality
    \[
     \textbf{[ in the notebook ]}
    \]
    \subitem \textbf{Intepretation \& Business Insights (20 points)}
    \\ \subitem \textbf{Part 1: Feature Intepretation in PCA:}
    \begin{enumerate}
        \item \textbf{(5 points)} Based on the principal components directions, explain which features (original dimensions) contribute most to the first and second principal components.
        \[
        \]
        \item \textbf{(5 points)} How would you explain the reduced features to a non-technical team in terms of the user behavior patterns?
    \end{enumerate}
\end{enumerate}
\newpage
\begin{center}
    \large \textbf{Question 2: SUM OF RANDOM VARIABLES, CENTRAL lIMIT THEOREM \& PROBABILITY BOUNDS (5O marks)}
\end{center}
    Reading: The sum of random variables is a fundamental concept in probability and statistics, shedding
    light on the behavior of combined outcomes under uncertainty. When adding two random variables, \(X\)
    and \(Y\) \((i.e., Z = X + Y )\), we analyze the distribution of Z. For independent variables, the distribution
    of Z can be derived by convolving the individual distributions.
    \\ For example, if both X and Y are normally distributed with means \(\mu_X\) and \(\mu_Y\) and variances \(\sigma^2_X\) and \(\sigma^2_Y\), then Z will also be normally distributed, with mean \(\mu_Z = \mu_X + \mu_Y\) and variance \(\sigma^2_Z = \sigma^2_X + \sigma^2_Y\)
    which is beneficial for statistical modeling.
    \\ When X and Y are not independent, we must include covariance, which accounts for how the variables
    change together, in the variance of Z: \(\sigma^2_Z = \sigma^2_X + \sigma^2_Y + 2\text{Cov}(X, Y)\)
    + 2Cov(X, Y ).
    \\ This concept applies to any number of random variables. For independent variables \(X_1,X_2, . . . ,X_n\), the
    sum \(Z = X_1 + X_2 + . . . + X_n\) is analyzed similarly. The Central Limit Theorem indicates that as the
    number of independent variables increases, their standardized sum approaches a normal distribution.
    Understanding the sum of random variables is essential in finance, insurance, and natural sciences for
    risk assessment, forecasts, and decision-making, revealing insights into complex systems.
    Probability bounds are crucial in statistics, quantifying the likelihood of events within specified limits,
    particularly in finance and engineering.
    \\ Markov’s inequality estimates the probability that a non-negative random variable X exceeds a certain
    value a, stating that for any \(a > 0\), the probability that \(X \geq a\) is at most the expected value of X divided
    by a. This demonstrates that limited information about X’s distribution can provide useful probability
    estimates.
    \\ Chebyshev’s inequality extends Markov’s by considering variance. It states that for any random variable
    X with mean \(\mu\) and finite variance \(\sigma^2\), the probability that X deviates from its mean by more than k
    standard deviations is at most 1
    k2 . This finding is fundamental for statistical inference.
    Hoeffding’s inequality offers bounds for sums of independent random variables, ensuring exponential
    decay in tail probabilities, which is especially valuable in large sample scenarios to keep observed averages
    close to expected values.
    \\ Overall, these probability bounds enhance decision-making and deepen our understanding of stochastic
    processes, facilitating robust conclusions across various fields.
    \\ \subitem \textbf{Part 1: Mobile Network Data Analysis (10 points)}
    \\ In a study conducted by a telecommunication company in Rwanda, mobile network Clls are classified as either voice (V) when someone is speaking or data (D) when there is a modem or fax transmission. Based on observed data, the probabilities are:
    \[
    P(V) = 0.6 \text{ (60\% voice calls)}
    \]
    \[
    P(D) = 0.4 (40\% \text{ data calls })
    \]
    Assume data calls and voice calls occur independently of each other, and let the random variable \(K_n\) represent the number of data calls in a collection of \(n\) calls.
    \begin{enumerate}
        \item \textbf{(2 points)} What is the \(E[K_{100}]\), the expected number of voice calls in a set of 100 calls?
        \[
        E[K_{100}] = n \times P[D] = 100 \times 0.4 = \textbf{40}
        \]
        \item \textbf{(2 points)} What is \(\sigma_{k_{100}}\), the standard deviation of the number of voice calls in a set of 100 calls?
        \[
        \sigma_{k_{100}} = \sqrt{n \times P[D] \times P[V]} = \sqrt{100 \times 0.4 \times 0.6} = \textbf{4.9}
        \]
        \item \textbf{(2 points)} Using Central Limit  Theorem, estimate \(P[K_{100} \geq  18]\), the probability of at least 18 data calls in a set of 100 calls
        \[
        P[K_{100} \geq 18] = 1 - P[K_{100} < 18] = 1 - P[K_{100} \leq 17]
        \]
        \[
        P[K_{100} \leq 17] = P\left[\frac{K_{100} - E[K_{100}]}{\sigma_{K_{100}}} \leq \frac{17 - 40}{4.9}\right] = P[Z \leq -4.69]
        \]
        \[
        P[Z \leq -4.69] = 0
        \]
        \[
        P[K_{100} \geq 18] = 1 - 0 = \textbf{1}
        \]
        \item \textbf{(2 points)} Using the CLT, estimate \(P[16 \leq K_{100} \leq  24]\), the probability of between 16 and 24 data calls in a set of 100 calls
        \[
        P[16 \leq K_{100} \leq 24] = P[K_{100} \leq 24] - P[K_{100} \leq 16]
        \]
        \[
        P[K_{100} \leq 24] = P\left[\frac{K_{100} - E[K_{100}]}{\sigma_{K_{100}}} \leq \frac{24 - 40}{4.9}\right] = P[Z \leq -3.27]
        \]
        \[
        P[Z \leq -3.27] = 0.0005
        \]
        \[
        P[K_{100} \leq 16] = P\left[\frac{K_{100} - E[K_{100}]}{\sigma_{K_{100}}} \leq \frac{16 - 40}{4.9}\right] = P[Z \leq -4.9]
        \]
        \[
        P[Z \leq -4.9] = 0
        \]
        \[
        P[16 \leq K_{100} \leq 24] = 0.0005 - 0 = \textbf{0.0005}
        \]
        \item \textbf{(2 points)} Based on your calculations, what can you infer about the likelyhood of high data traffic during a given period? How might this information help a telecom optimize their resources for voice and data services?
        \\\\ The likelihood of high data traffic during a given period is very low. This information can help a telecom optimize their resources for voice and data services by ensuring that they have enough capacity to handle voice calls, which are more likely to occur. They can also allocate resources to handle data calls, but they do not need to allocate as many resources since data calls are less likely to occur.
    \end{enumerate}
    \subitem \textbf{Part 2: Chernoff Bound \& Gaussian Random Variables (4 points)}
    \\ Use the Chernoff bound to show that for a Gaussian (Normal) random variable \(X\) with mean \(\mu\) and
    standard deviation \(\sigma\), the probability that X exceeds a certain threshold c can be bounded by:
    \[
    P[X \geq c] \leq e^{-\frac{(c-\mu)^2}{2\sigma^2}}
    \]
    proof:
    \[
    P[X \geq c] = P[e^{tX} \geq e^{tc}]
    \]
    \[
    P[X \geq c] = P[e^{tX} \geq e^{tc}] \leq \frac{E[e^{tX}]}{e^{tc}}
    \]
    \[
    E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx
    \]
    \[
    E[e^{tX}] = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}\sigma} e^{tx - \frac{(x-\mu)^2}{2\sigma^2}} dx
    \]
    \[
    E[e^{tX}] = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}\sigma} e^{tx - \frac{x^2 - 2x\mu + \mu^2}{2\sigma^2}} dx
    \]
    but the integral is the Gaussian integral:
    \[
    \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}\sigma} e^{tx - \frac{x^2 - 2x\mu + \mu^2}{2\sigma^2}} dx = e^{\frac{t^2\sigma^2}{2} + t\mu}
    \]
    solving for \(t\):
    \[
    e^{\frac{t^2\sigma^2}{2} + t\mu} = e^{tc}
    \]
    \[
    \frac{t^2\sigma^2}{2} + t\mu = tc
    \]
    \[
    \frac{t\sigma^2}{2} + \mu = c
    \]
    \[
    t = \frac{2(c-\mu)}{\sigma^2}
    \]
    substituting back into the Chernoff bound:
    \[
    P[X \geq c] \leq \frac{e^{\frac{2(c-\mu)}{\sigma^2}X}}{e^{tc}}
    \]
    \[
    P[X \geq c] \leq e^{-\frac{(c-\mu)^2}{2\sigma^2}}
    \]
    Given this result, how would you use it to provide a worst-case scenario estimate in a real-world context, such as predicting an extreme event like an abnormally high network traffic spike or stock price surge?
    \subitem\textbf{Part 3: Soccer Tournament Performance (11 points)}
    \\ Manchester United is competing in a knockout-style tournament, where each game can result in a win,
    loss, or tie. For every win, they earn 3 points, for every tie 1 point, and for a loss 0 points. The outcome
    of each game is independent of the others, and each game result is equally likely (win, loss, or tie). Let
    \(X_i\) be the number of points earned in game i, and Y represent the total number of points earned over
    the course of the tournament.
    \begin{enumerate}
        \item \textbf{( 3 points )} Derive the moment generating function (MGF) of \(\phi_Y\)(s)
        \\ random variables \(X_i\) are independent and identically distributed, so the MGF of the sum of the random variables is the product of the MGFs of the individual random variables
        \\ A random variable \(X_i\) can take on the values 0, 1, 3 with probabilities \(\frac{1}{3}\) each
        \[
        \phi_{X_i}(s) = E[e^{sX_i}] = \frac{1}{3}e^{0} + \frac{1}{3}e^{s} + \frac{1}{3}e^{3s}
        \]
        \[
        \phi_{X_i}(s) = \frac{1}{3} + \frac{1}{3}e^{s} + \frac{1}{3}e^{3s}
        \]
        \[
        \phi_{X_i}(s) = \frac{1}{3}(1 + e^{s} + e^{3s})
        \]
        \[
        \phi_{Y}(s) = \phi_{X_1}(s) \times \phi_{X_2}(s) \times \phi_{X_3}(s) \times \phi_{X_4}(s) \dots \times \phi_{X_n}(s)
        \]
        \[
        \phi_{Y}(s) = \left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)^n
        \]
        \item \textbf{( 5 points )} Find E[Y] and Var[Y], the expected total points and variance.
        \\ Our moment generating function is:
        \[
        \phi_{Y}(s) = \left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)^n
        \]
        to get the expected value, we differentiate the MGF with respect to s and evaluate at s = 0:
        \[
        E[Y] = \phi_{Y}'(s) = n\left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)^{n-1} \times \frac{d}{ds}\left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)
        \]
        \[
        E[Y] = n\left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)^{n-1} \times \frac{1}{3}(e^{s} + 3e^{3s})
        \]
        set s = 0:
        \[
        E[Y] = n\left(\frac{1}{3}(1 + 1 + 1)\right)^{n-1} \times \frac{1}{3}(1 + 3)
        \]
        \[
        E[Y] = n\left(\frac{3}{3}\right)^{n-1} \times \frac{4}{3}
        \]
        so the expected value of Y is:
        \[
        E[Y] = \mathbf{\frac{4n}{3}}
        \]
        The variance is obtained by differentiating the MGF twice with respect to s and evaluating at s = 0:
        \[
        Var[Y] = \phi_{Y}''(s) = n(n-1)\left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)^{n-2} \times \frac{d^2}{ds^2}\left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)
        \]
        \[
        Var[Y] = n(n-1)\left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)^{n-2} \times \frac{d}{ds}\left(\frac{1}{3}(e^{s} + 3e^{3s})\right)
        \]
        \[
        Var[Y] = n(n-1)\left(\frac{1}{3}(1 + e^{s} + e^{3s})\right)^{n-2} \times \frac{1}{3}(e^{s} + 9e^{3s})
        \]
        set s = 0:
        \[
        Var[Y] = n(n-1)\left(\frac{1}{3}(1 + 1 + 1)\right)^{n-2} \times \frac{1}{3}(1 + 9)
        \]
        \[
        Var[Y] = n(n-1)\left(\frac{3}{3}\right)^{n-2} \times \frac{10}{3}
        \]
        so the variance of Y is:
        \[
        Var[Y] = \mathbf{\frac{10n(n-1)}{9}}
        \]
        \item \textbf{( 3 points )} Based on your calculations, what can you infer about the likely performance of Manchester United over the course of multiple tournaments? How might the expected points impact their overall ranking or their chances of advancing in the competition?
        \\\\ Based on the calculations, we can infer that the expected performance of Manchester United over the course of multiple tournaments is to earn an average of \(\frac{4n}{3}\) points with a variance of \(\frac{10n(n-1)}{9}\). This information can help predict their overall ranking in the competition and their chances of advancing. Teams with higher expected points are more likely to rank higher and advance in the competition, while teams with lower expected points are less likely to do so.
    \end{enumerate}
    \subitem \textbf{Part 4: Course Enrollment and Resource Planning (6 points)}
    \\ The number of students enrolling in a popular data science course is modelled as a poisson random variable with a mean of 100 students. The Professor has decided that if 120 students enroll, he will split the class into two sectioons, otherwise , he will teach all the students in a single Section
    \begin{enumerate}
        \item \textbf{( 3 points )} What is the probability that the professor will need to teach two sections?
        \\ using Markov's inequality:
        \[
        P[X \geq 120] \leq \frac{E[X]}{120}
        \]
        \[
        P[X \geq 120] \leq \frac{100}{120}
        \]
        \[
        P[X \geq 120] \leq 0.833
        \]
        \[
        P[X \geq 120] = \textbf{0.833}
        \]
        \item \textbf{( 3 points )} Based on Probability, what recommendations would you make regarding resource planning for future courses? Should the professor prepare for two sections or allocate resources differently based on expected enrollments?
        \\\\ Based on the probability that the professor will need to teach two sections, it is recommended that the professor prepare for two sections. Since the probability is 0.833, which is greater than 0.5, it is more likely that 120 students will enroll. By preparing for two sections, the professor can ensure that there are enough resources to accommodate the students and provide a better learning experience.
    \end{enumerate}
    \subitem \textbf{Part 5: Comparison of Markov, Chebyshev and Chernoff Inequalities (19 points)}
    consider a Gaussian Random Variable X with mean \(\mu = 0\) and variance \(\sigma = 1\). We are interested in comparing how well three probability probability bounds - \textbf{Markov, Chebyshev and Chernoff} - estimate the probability that X exceeds a certain threshold c. i.e \(P[X \geq c]\)
    \begin{enumerate}
        \item \textbf{ Markov Inequality}
        \\ For a non-negative random variable X, Markov's inequality states that for any \(c > 0\), the probability that \(X \geq a\) is at most the expected value of X divided by a. This provides a simple bound on the tail probability of a random variable.
        \[
        P[X \geq c] \leq \frac{E[X]}{c}
        \]
        For this comparison, we will apply Markov's inequality to the positive part of the Gaussian random variable \(X\), considering \(X^{+} = \max(X, 0)\).
        \item \textbf{ Chebyshev Inequality}
        \\ Chebyshev's inequality, applicable to any random variable with finite variance, states that for any random variable X with mean \(\mu\) and finite variance \(\sigma^2\), the probability that X deviates from its mean by more than k standard deviations is at most 1/k2. This provides a more refined bound on the tail probability of a random variable.
        \[
        P[|X - \mu| \geq c] \leq \frac{\sigma^2}{c^2}
        \]
        We will apply this inequality to the Gaussian random variable
        \item \textbf{ Chernoff bound}
        \\ For a Gaussian random Variable X, the Chernoff bound provides a tighter bound for tail probabilities
        \[
        P[X \geq c] \leq \exp(-\frac{(c-\mu)^2}{2\sigma^2})
        \]
        The bound is especially for normally distributed data.
        \item \textbf{ Comparison}
        \[
        \textbf{[ in the notebook ]}
        \]
    \end{enumerate}
    \newpage
    \begin{center}
        \large \textbf{Question 3: ESTIMATION \& HYPOTHESIS TESTING, CONFIDENCE INTERVALS (130 marks)}
    \end{center}
    Reading: Estimation and hypothesis Testing provide a framework for making inference about populations based on sample data. \textbf{Estimation} is the process of inferring population parameters (such as means or proportions) from sample statistics. There are two main types of estimations:
    \subitem \textbf{Point Estimation} 
    \\ Provides a single value as the estimate of a population parameter. 
    \\ Example: The sample mean \(\bar{X}\) is a point estimate of the population mean \(\mu\).
    \\ Formula:
    \[
    \bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i
    \]
    \subitem \textbf{Interval Estimation}
    \\ Offers a range of values within which the population parameter is likely to fall. These values are in arange called confidence interval, believed to contain the parameter based with a specific level of confindence:
    \\ Example: A 95\% confidence interval for the population mean \(\mu\) is given by \(\bar{X} \pm 1.96\frac{\sigma}{\sqrt{n}}\)
    \\ Formula:
    \[
    \text{Confidence Interval} = \bar{X} \pm z \times \frac{\sigma}{\sqrt{n}}
    \]
    where Z is the z-score corresponding to the desired confidence level.
    \\ \\ \subitem \textbf{Hypothesis Testing}
    \\ Hypothesis testing is a statistical method used to make inferences about population parameters based on sample data. You use it to make probabilistic decisions about the population based on sample data. The process involves:
    \subitem \textbf{Step 1: Formulate Hypotheses}
    \\ The null hypothesis (\(H_0\)) is the default assumption that there is no effect or no difference. 
    \\ The alternative hypothesis (\(H_1\) or \(H_a\)) is the claim that there is an effect or difference. The claim to be tested
    \subitem \textbf{Step 2: Set the Significance Level}
    \\ The significance level (\(\alpha\)) is the probability of rejecting the null hypothesis when it is true. Common values are 0.05 or 0.01. This is the threshold of rejecting the null hypothesis.
    \subitem \textbf{Step 3: Calculate the Test Statistic}
    \\ The test statistic is a numerical summary of the data that measures the difference between the sample data and the null hypothesis. It is used to determine whether the null hypothesis should be rejected.
    \[
    \text{Test Statistic} = \frac{\text{Observed Value} - \text{Expected Value}}{\text{Standard Error}}
    \]
    \[
    \text{Standard Error} = \frac{\text{Standard Deviation}}{\sqrt{n}}
    \]
    \[
    \text{Observed Value} = \text{Sample Mean}
    \]
    \[
    \text{Expected Value} = \text{Population Mean}
    \]
    \[
    \text{Standard Deviation} = \text{Population Standard Deviation}
    \]
    \[
    \text{n} = \text{Sample Size}
    \]
    the final formula is:
    \[
    \text{Test Statistic} = \frac{\bar{X} - \mu}{\frac{\sigma}{\sqrt{n}}}
    \]
    \subitem \textbf{Step 4: Make a Decision}
    \\ Compare the test statistic to the critical value. If the test statistic falls in the rejection region, reject the null hypothesis. If it falls in the non-rejection region, do not reject the null hypothesis.
    \[
    \text{Rejection Region} = \text{Critical Value}
    \]
    \[
    \text{ if } Z \geq \text{Critical Value} \text{, reject } H_0
    \]
    where the critical value is determined by the significance level and the type of test (one-tailed or two-tailed).
    
    \subitem \textbf{Step 5: outcome}:
    \\ Based on the decision, you can either reject or fail to reject the null hypothesis. This decision provides insight into the population parameter and the relationship between the sample and the population.
    \\ if the null hypothesis is rejected, it suggests strong evidence in favor of the alternative hypothesis
    \\ if not rejected, there isn't sufficient evidence to support the alternative hypothesis

    \subitem \textbf{Central Limit Theorem}
    \\ It plays a crucial role in both estimation and hypothesis testing stating that the sample mean \(\bar(X)\) will approach a normal distribution as the sample size(n), increases, regardless of the population's distribution
    \\ This Theorem provides normal approximation techniques in practice, facilitating easier calculations and Intepretation in estimation and hypothesis testing..
    \\ \subitem \textbf{Part 1: Point Estimation: Estimating the Average Battery Life ( 20 marks )}
    \\ A company is producing a new smartphone model and wants to estimate the average Battery life. From a sample of 20 smartphones, the following battery life data (in hours) is collected:
    \[
    \text{Battery Life} = [8.2, 8.5, 8.9, 9.0, 7.8, 8.6, 8.4, 8.1, 9.1, 8.7, 9.2, 8.8, 8.3, 9.3, 8.0, 8.9, 8.4, 8.6, 8.7, 8.2]
    \]
    \begin{enumerate}
        \item \textbf{( 2 points )} Calculate the sample mean and sample variance.
        \[
        \text{Sample Mean} = \bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i
        \]
        \[
        \text{Sample Mean} = \frac{1}{20}(8.2 + 8.5 + 8.9 + 9.0 + 7.8 + 8.6 + 8.4 + 8.1 + 9.1 + 8.7 + 9.2 + 8.8 + 8.3 + 9.3 + 8.0 + 8.9 + 8.4 + 8.6 + 8.7 + 8.2)
        \]
        \[
        \text{Sample Mean} = \frac{1}{20}(171.7) = \textbf{8.585}
        \]
        sample variance:
        \[
        \text{Sample Variance} = \frac{1}{n-1}\sum_{i=1}^{n}(X_i - \bar{X})^2
        \]
        \[
        \text{Sample Variance} = \frac{1}{19}((8.2 - 8.585)^2 + (8.5 - 8.585)^2 + (8.9 - 8.585)^2 + (9.0 - 8.585)^2 + (7.8 - 8.585)^2 + (8.6 - 8.585)^2 + (8.4 - 8.585)^2 + (8.1 - 8.585)^2 
        \]
        \[
        + (9.1 - 8.585)^2 + (8.7 - 8.585)^2 + (9.2 - 8.585)^2 + (8.8 - 8.585)^2 + (8.3 - 8.585)^2 + (9.3 - 8.585)^2 + (8.0 - 8.585)^2 + (8.9 - 8.585)^2 + (8.4 - 8.585)^2 + (8.6 - 8.585)^2 
        \]
        \[
        + (8.7 - 8.585)^2 + (8.2 - 8.585)^2)
        \]
        \[
        \text{Sample Variance} = \frac{1}{19}(3.0725425) = \textbf{0.1617}
        \]
        \item \textbf{( 3 points )} What does the sample mean estimate in this case, and determine if the sample mean is unbiased estimator of the population \(\mu\). Explain your resonings.
        \\ The sample mean estimates the average battery life of the smartphones in the sample. The sample mean is an unbiased estimator of the population mean \(\mu\) if the expected value of the sample mean is equal to the population mean. In this case, the sample mean is an unbiased estimator of the population mean \(\mu\) because the expected value of the sample mean is equal to the population mean.
        \item \textbf{( 3 points )} Compute the mean square error (MSE) of the sample mean and variance.
        \[
        \text{MSE} = \text{Bias}(\bar{X})^2 + \text{Variance}(\bar{X})
        \]
        \[
        \text{Bias}(\bar{X}) = E[\bar{X}] - \mu
        \]
        \[
        \text{Variance}(\bar{X}) = 0.1617
        \]
        \[
        \text{MSE} = (8.585 - 8.585)^2 + 0.1617 = \textbf{0.1617}
        \]
        \item (2 points)Explain how the error of the mean and the sample variance might change if the
        sample size were smaller or larger.
        \\ \\ If the sample size were smaller, the error of the mean and the sample variance would likely be larger because the sample mean and sample variance would be less representative of the population. If the sample size were larger, the error of the mean and the sample variance would likely be smaller because the sample mean and sample variance would be more representative of the population.
        \item (2 points) What effect do outliers have on the sample mean and variance?
        \\ \\ If there are outliers in the sample, they can skew the sample mean and variance, making them less representative of the population. Outliers can also increase the variance of the sample, making it less accurate as an estimate of the population variance.
        \item (8 points) Dynamically simulate different sample sizes \(n \in [5, 1000, step=5]\) and generate samples
        from a normal distribution with a known population mean \(\mu = 8.5\) and standard deviation \(\sigma\) = 0.5.
        Vary n and compare how the sample mean and variance behave as the sample size changes
    \end{enumerate}
    \subitem \textbf{Part 2: Confidence Intervals; Estimating the True Mean Height ( 16  marks )}
    \\ You are studying the average height of adult males in a city. A random sample of 30 men yields a sample mean height of \(\textbf{176 cm}\) and a standard deviation of \(\textbf{7 cm}\). 
    \begin{enumerate}
        \item  \textbf{( 2 points )} Calculate a 95\% confidence interval for the population mean height, assuming that the population standard deviation is unkown
        \[
        \text{Confidence Interval} = \bar{X} \pm z \times \frac{\sigma}{\sqrt{n}}
        \]
        \[
        \text{Confidence Interval} = 176 \pm 1.96 \times \frac{7}{\sqrt{30}}
        \]
        \[
        \text{Confidence Interval} = 176 \pm 1.96 \times \frac{7}{5.48}
        \]
        \[
        \text{Confidence Interval} = 176 \pm 1.96 \times 1.28
        \]
        \[
        \text{Confidence Interval} = 176 \pm 2.51
        \]
        \[
        \text{Confidence Interval} = [173.49, 178.51]
        \]
        \item \textbf{( 2 points )} If the population standard deviation was known to be 7 cm, how would this change the confidence interval?
        \[
        \text{Confidence Interval} = \bar{X} \pm z \times \frac{\sigma}{\sqrt{n}}
        \]
        \[
        \text{Confidence Interval} = 176 \pm 1.96 \times \frac{7}{\sqrt{30}}
        \]
        \item \textbf{( 2 points )} What does the confidence interval mean in practical terms?
        \item \textbf{( 8 points )} Simulate the construction of confidence intervals for different sample sizes, and confidence levels,  Vary \(n \in [10, 500, \text{step}=10]\) and \(\alpha \in [0.01, 0.2, \text{step}=0.01]\) dynamically and observe how the confidence interval width changes.
        \[
        \textbf{[ in the notebook ]}
        \]
    \end{enumerate}
    \subitem \textbf{Part 3: Hypothesis Testing; Comparing Two Webpage Designs (A/B Testing) (19 points)}
    \\ You are running an A/B test for two different webpage designs to see which one generates more clicks.
    Out of 600 visitors, 240 clicked on Version A, and 290 clicked on Version B.

    \begin{enumerate}
        \item \textbf{( 2 points )} Calculate the proportions of clicks for each version 
        \[
        \text{Proportion of Clicks for Version A} = \frac{240}{600} = \textbf{0.4}
        \]
        \[
        \text{Proportion of Clicks for Version B} = \frac{290}{600} = \textbf{0.4833}
        \]
        \item \textbf{( 2 points )} Perform a hypothesis test to determine whether there is a significant difference between the click-through rates (CTR) of the two versions. Assume \(\alpha\) = 0.05 and compute the z-statistic
        by hand
        \[
        \text{Null Hypothesis} (H_0): \text{There is no significant difference between the click-through rates of the two versions}
        \]
        \[
        \text{Alternative Hypothesis} (H_1): \text{There is a significant difference between the click-through rates of the two versions}
        \]
        \[
        \text{Test Statistic} = \frac{p_1 - p_2}{\sqrt{p(1-p)(\frac{1}{n_1} + \frac{1}{n_2})}}
        \]
        \[
        p = \frac{240 + 290}{600 + 600} = \frac{530}{1200} = 0.4417
        \]
        \[
        \text{Standard Error} = \sqrt{0.4417(1-0.4417)(\frac{1}{600} + \frac{1}{600})} = 0.032
        \]
        \[
        \text{Z-Statistic} = \frac{0.4 - 0.4833}{0.032} = -2.604
        \]
        \[
        \text{Critical Value} = -1.96
        \]
        \[
        \text{ if } Z \leq -1.96 \text{, reject } H_0
        \]
        \[
        \text{ if } -2.604 \leq -1.96 \text{, reject } H_0
        \]
        \[
        \text{Reject } H_0
        \]
        \item \textbf{( 2 points )} What does the p-value mean in the context of this A/B test?
        \[
        \text{P-Value} = 0.0049
        \]
        it represents the probability of observing a test statistic as extreme as the one calculated, assuming the null hypothesis is true. In this case, the p-value is less than the significance level of 0.05, indicating that the observed difference in click-through rates is statistically significant.
        \item \textbf{( 3 points )} How would you interpret the results to your marketing team?
        \item \textbf{( 6 points )} Use a dynamic simulation to vary the sample sizes \(n \in [100, 1000, \text{step}=50]\) and click-through rates \(c \in [50, 600, \text{step}=10]\) for Versions A and B. Observe how the p-value and hypothesis test decision change as you modify these parameters.
    \end{enumerate}
    \subitem \textbf{Part 4: True Positives and False Positives in Medical Testing (23 points)}
    \\ A medical test for a rare disease is conducted on 1000 people. The test correctly identifies 80 true positives and 900 true negatives. However, it also produces 10 false positives and 10 false negatives.
    \begin{enumerate}
        \item (5 points) Construct a confusion matrix from the data provided.
        \[
        \begin{array}{|c|c|c|}
        \hline
        & \text{Actual Positive} & \text{Actual Negative} \\
        \hline
        \text{Predicted Positive} & 80 & 10 \\
        \hline
        \text{Predicted Negative} & 10 & 900 \\
        \hline
        \end{array}
        \]
        \item (2 points) Compute the sensitivity, specificity, positive predictive value (PPV), and negative
        predictive value (NPV) of the test.
    \end{enumerate}
\end{document}