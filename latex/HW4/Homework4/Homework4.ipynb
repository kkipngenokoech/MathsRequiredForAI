{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Real estate.csv\")\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "df.drop(\"No\", axis = 1, inplace = True)\n",
    "column_maping = {}\n",
    "for i in df.columns:\n",
    "    new_column = i[3:].capitalize().replace(' ', '_')\n",
    "    column_maping[i] = new_column\n",
    "# Now we will rename the column using the dictinary \n",
    "df.rename(columns = column_maping, inplace = True)\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "X  = df.drop(['Transaction_date', \"House_price_of_unit_area\"], axis = 1)\n",
    "y = df['House_price_of_unit_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error using SVD 7.387891796775459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "# Do not change the code Above\n",
    "\n",
    "# Add a column of ones to X_train and X_test to account for the bias term (intercept)\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Perform SVD decomposition on the training data\n",
    "U, s, VT = np.linalg.svd(X_train_b, full_matrices=False)\n",
    "\n",
    "# Create diagonal matrix for Sigma\n",
    "S_diag = np.diag(s)\n",
    "\n",
    "# Compute the pseudo-inverse of the training data\n",
    "X_train_pinv = VT.T @ np.linalg.inv(S_diag) @ U.T\n",
    "\n",
    "# Calculate the weights (regression coefficients), including the bias term (intercept)\n",
    "w = X_train_pinv @ y_train\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = X_test_b @ w\n",
    "\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error using SVD {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error Ridge 7.387798950430715\n"
     ]
    }
   ],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "     Ridge Regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y, alpha=0):\n",
    "        \"\"\"\n",
    "        Fits the ridge regression model to the training data.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p independent variables\n",
    "        y: response variable vector for n examples\n",
    "        alpha: regularization parameter.\n",
    "        \"\"\"\n",
    "      \n",
    "        intercept = np.ones((len(X),1))\n",
    "        X_b = np.c_[intercept,X]\n",
    "        \n",
    "        I = np.identity(X_b.shape[1])\n",
    "        \n",
    "        betha_optim = ##TODO What is the optimal Beta you solved ?\n",
    "        self.betas = betha_optim\n",
    "        return betha_optim\n",
    "   \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the dependent variable of new data using the model.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        response variable vector for n examples\n",
    "        \"\"\"\n",
    "       \t# Your code here\n",
    "        X_predictor = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.predictions = ##TODO how do we use the learned parameters to make a prediction\n",
    "        return self.predictions\n",
    "\n",
    "    def rmse(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the RMSE(Root Mean Squared Error) when the model is validated.\n",
    "            \n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "        y: response variable vector for n examples\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        RMSE when model is used to predict y\n",
    "        \"\"\"\n",
    "        y_predict = self.predict(X=X)\n",
    "        se = (y_predict-y) ** 2\n",
    "        mse = se.mean()\n",
    "        rmse = mse**0.5\n",
    "        return rmse\n",
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "my_model = Model()\n",
    "my_model.fit(X=X_train, y=y_train, alpha=0.01)\n",
    "rmse = my_model.rmse(X=X_test, y=y_test)\n",
    "print(f\"Root Mean Squre Error Ridge {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(331, 1)\n",
      "Root Mean Squre Error 1 Layer MLP 15.69657819926082\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return #TODO\n",
    "\n",
    "# Mean Squared Error loss\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return #TODO\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_o, b_o):\n",
    "    a_h1 = #TODO what is the output of the hidden layer\n",
    "    z_h1 = #TODO apply activation to the output of the hidden layer \n",
    "    y_pred = #TODO use ouput of activation as input to the output layer (it is just similar to first layer but we don't apply activation)\n",
    "    return y_pred, z_h1, a_h1\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, W_h1, W_o):\n",
    "    \n",
    "    # Derivative of loss with respect to y_pred\n",
    "    dL_dy_pred = #TODO What type of loss function are we using, what is it's derivative ?\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dL_dW_o = #TODO remember the output layer is just a dense layer\n",
    "    dL_db_o = #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    dL_dz_h1 = #TODO derivative with respect to the output of activation layer\n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    dL_da_h1 = #TODO derivative with respect to the activation layer\n",
    "    \n",
    "    # Gradients for hidden layer\n",
    "    dL_dW_h1 = #TODO\n",
    "    dL_db_h1 = #TODO \n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer_size = 20 # Number of neurons in layer\n",
    "output_size = 1 # predicted variable\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer_size))\n",
    "W_o = np.random.randn(hidden_layer_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "#To save the weights which give the lowest loss\n",
    "lowest_loss = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1 = forward_pass(X_train, W_h1, b_h1, W_o, b_o)\n",
    "    loss = mse_loss(y_train, y_pred)\n",
    "\n",
    "    if loss < lowest_loss:\n",
    "        lowest_loss = loss\n",
    "        # Save the best weights and biases\n",
    "        best_weights = (W_h1.copy(), b_h1.copy(), W_o.copy(), b_o.copy())\n",
    "\n",
    "    # Backward pass to get gradients\n",
    "    dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, W_h1, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases\n",
    "    W_h1 -= #TODO\n",
    "    b_h1 -= #TODO\n",
    "    W_o -= #TODO\n",
    "    b_o -= #TODO\n",
    "\n",
    "\n",
    "W_h1_best, b_h1_best, W_o_best, b_o_best = best_weights\n",
    "y_pred_test, _, _ = forward_pass(X_test, W_h1_best, b_h1_best, W_o_best, b_o_best)\n",
    "se = (y_pred_test-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 1 Layer MLP {rmse}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Lets add one more hidden layer (10 Points) \n",
    "### You must write down all gradients and complete the code below to get full bonus points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error 1 Layer MLP 13.11351371204041\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_h2, b_h2, W_o, b_o):\n",
    "    a_h1 = #TODO compute ouput of first hidden layer\n",
    "    z_h1 = sigmoid(a_h1) # apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    a_h2 = #TODO compute ouput of second hidden layer\n",
    "    z_h2 = sigmoid(a_h2) #apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    y_pred = # compute ouput of output layer, why don't we apply activation ?\n",
    "    \n",
    "    return y_pred, z_h1, a_h1, z_h2, a_h2\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o):\n",
    "    \n",
    "    # Derivative of loss with respect to y_pred, what kind of loss are we using ?\n",
    "    dL_dy_pred = #TODO\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    dL_dW_o = #TODO\n",
    "    dL_db_o = #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h2\n",
    "    dL_dz_h2 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to a_h2\n",
    "    dL_da_h2 =  #TODO\n",
    "    \n",
    "    # Gradients for second hidden layer\n",
    "    dL_dW_h2 =  #TODO\n",
    "    dL_db_h2 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    dL_dz_h1 =  #TODO\n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    dL_da_h1 =  #TODO\n",
    "    \n",
    "    # Gradients for first hidden layer\n",
    "    dL_dW_h1 =  #TODO\n",
    "    dL_db_h1 =  #TODO\n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "# Random input and true output (modify these according to your dataset)\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer1_size = 100\n",
    "hidden_layer2_size = 20\n",
    "output_size = 1\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer1_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer1_size))\n",
    "W_h2 = np.random.randn(hidden_layer1_size, hidden_layer2_size) * 0.001\n",
    "b_h2 = np.zeros((1, hidden_layer2_size))\n",
    "W_o = np.random.randn(hidden_layer2_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "#Training loop\n",
    "for i in range(200):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1, z_h2, a_h2 = forward_pass(X_train, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "\n",
    "    #TODO Compute the loss\n",
    "    # Backward pass to get gradients\n",
    "    dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases for each layer\n",
    "    \n",
    "    W_h1 -= #TODO\n",
    "    b_h1 -= #TODO\n",
    "    W_h2 -= #TODO\n",
    "    b_h2 -= #TODO\n",
    "    W_o -= #TODO\n",
    "    b_o -= #TODO\n",
    "\n",
    "y_pred, _, _, _, _ = forward_pass(X_test, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 2 Layer MLP {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 400)               2400      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 400)               160400    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 400)               160400    \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 401       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 323601 (1.23 MB)\n",
      "Trainable params: 323601 (1.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform',  activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform', activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(400, kernel_initializer = 'he_uniform',activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 1s 12ms/step - loss: 59.0233 - val_loss: 38.2307\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 60.5859 - val_loss: 38.7539\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5126 - val_loss: 39.4410\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.5206 - val_loss: 41.2996\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.8750 - val_loss: 36.7995\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.7921 - val_loss: 42.3474\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.6418 - val_loss: 38.9120\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.3855 - val_loss: 34.8828\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5909 - val_loss: 38.9493\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3074 - val_loss: 46.1832\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3386 - val_loss: 48.0670\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 65.7009 - val_loss: 47.4946\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 61.9314 - val_loss: 36.4857\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.1176 - val_loss: 39.1615\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.7488 - val_loss: 45.9356\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1246 - val_loss: 42.3506\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.2204 - val_loss: 38.4740\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.5598 - val_loss: 40.0011\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.5267 - val_loss: 41.0102\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.7600 - val_loss: 41.7729\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 65.8730 - val_loss: 40.3871\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 60.0501 - val_loss: 39.3643\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.8604 - val_loss: 39.8595\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 61.1329 - val_loss: 38.9457\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.6057 - val_loss: 41.5118\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.1321 - val_loss: 35.7910\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1891 - val_loss: 39.2262\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3964 - val_loss: 36.2836\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.9470 - val_loss: 45.4145\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.6735 - val_loss: 37.7727\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.3257 - val_loss: 48.5326\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 64.4544 - val_loss: 43.9641\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.6239 - val_loss: 46.4309\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 58.7664 - val_loss: 38.9237\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.0663 - val_loss: 40.5066\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 54.7116 - val_loss: 43.4164\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.9722 - val_loss: 44.4749\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.3571 - val_loss: 47.2762\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.0204 - val_loss: 53.4110\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5805 - val_loss: 40.7375\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.9992 - val_loss: 40.2645\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1041 - val_loss: 42.0701\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.9374 - val_loss: 46.8338\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 54.1358 - val_loss: 45.7144\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.6320 - val_loss: 39.5554\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 71.7491 - val_loss: 54.7381\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.1944 - val_loss: 45.8185\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 59.1702 - val_loss: 45.4068\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "history = model.fit(X_train, y_train,  epochs = 200, validation_data=(X_test, y_test), \n",
    "                    callbacks = EarlyStopping(monitor = 'val_loss',patience = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 45.4068\n",
      "RMSE error using deeper neural network 6.738456462848816\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE error using deeper neural network {model.evaluate(X_test, y_test)**0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
