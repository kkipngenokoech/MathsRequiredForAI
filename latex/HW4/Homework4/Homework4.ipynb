{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import ipdb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Real estate.csv\")\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "df.drop(\"No\", axis = 1, inplace = True)\n",
    "column_maping = {}\n",
    "for i in df.columns:\n",
    "    new_column = i[3:].capitalize().replace(' ', '_')\n",
    "    column_maping[i] = new_column\n",
    "# Now we will rename the column using the dictinary \n",
    "df.rename(columns = column_maping, inplace = True)\n",
    "df.rename({'Ouse_price_of_unit_area': 'House_price_of_unit_area'}, axis = 1, inplace = True)\n",
    "X  = df.drop(['Transaction_date', \"House_price_of_unit_area\"], axis = 1)\n",
    "y = df['House_price_of_unit_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error using SVD 7.387891796775459\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "# Do not change the code Above\n",
    "\n",
    "# Add a column of ones to X_train and X_test to account for the bias term (intercept)\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "# Perform SVD decomposition on the training data\n",
    "U, s, VT = np.linalg.svd(X_train_b, full_matrices=False)\n",
    "\n",
    "# Create diagonal matrix for Sigma\n",
    "S_diag = np.diag(s)\n",
    "\n",
    "# Compute the pseudo-inverse of the training data\n",
    "X_train_pinv = VT.T @ np.linalg.inv(S_diag) @ U.T\n",
    "\n",
    "# Calculate the weights (regression coefficients), including the bias term (intercept)\n",
    "w = X_train_pinv @ y_train\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = X_test_b @ w\n",
    "\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error using SVD {rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error Ridge 7.387798950430715\n",
      "7.387798950430715\n",
      "Root Mean Squre Error Ridge 7.38697328534338\n",
      "7.38697328534338\n",
      "Root Mean Squre Error Ridge 7.379693013437837\n",
      "7.379693013437837\n",
      "Root Mean Squre Error Ridge 7.39620540310947\n",
      "7.39620540310947\n",
      "Root Mean Squre Error Ridge 11.238394958870039\n",
      "11.238394958870039\n",
      "Root Mean Squre Error Ridge 29.033427420684916\n",
      "29.033427420684916\n",
      "Root Mean Squre Error Ridge 37.290048013604675\n",
      "37.290048013604675\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/00lEQVR4nO3de1yUdf7//+cAMoCcVU4KngsVLdM009wOlKnb0UqzWm37dVRL3c9WfiqrT58+uvXdamvL2l3LDpjlrtrWlq5p6lYeklQ0jdJUPACaAgMoCMz794cyOoEKOsM1Mzzut9vccuZ6z/Cay1vyvF3X63pdNmOMEQAAgB8KsroAAACAM0WQAQAAfosgAwAA/BZBBgAA+C2CDAAA8FsEGQAA4LcIMgAAwG8RZAAAgN8iyAAAAL9FkAGaMZvNpqeeeuqM3zt+/HjPFgQAjUSQAQLUa6+9JpvNpv79+1tdis/asWOHbDab6xEUFKT4+HgNHTpUK1eurLP+qaeecq3btWtXne0Oh0Ph4eH1hrz9+/froYceUnp6usLDw5WQkKB+/frpkUceUVlZmWvd2LFj3Wo68REWFub5nQD4uRCrCwDgHVlZWerQoYPWrFmjrVu3qkuXLlaX5LNuvfVWDRs2TDU1Nfrhhx/02muv6bLLLtM333yjnj171llvt9v1/vvv6+GHH3Z7fd68efV+/sGDB9W3b185HA799re/VXp6ug4cOKCcnBzNmDFD999/vyIjI90+/29/+1udzwkODj7LbwoEHoIMEIC2b9+ur7/+WvPmzdO9996rrKwsPfnkk1aX5bMuuOAC3X777a7nl1xyiYYOHaoZM2botddeq7N+2LBh9QaZ2bNna/jw4frHP/7h9vrMmTOVl5enr776ShdffLHbNofDodDQULfXQkJC3OoBcHKcWgICUFZWluLi4jR8+HDddNNNysrKatD7ak+dfP/997rlllsUHR2tVq1a6aGHHlJFRUW971mwYIEyMjJkt9vVo0cPLVy40G37zp079cADD+jcc89VeHi4WrVqpZtvvlk7duw4ZS1VVVWKj4/XnXfeWWebw+FQWFiY/uu//sv12iuvvKIePXooIiJCcXFx6tu3r2bPnt2g7/1Ll1xyiSRp27Zt9W4fPXq01q9fr++//971WkFBgZYuXarRo0fXWb9t2zYFBwfroosuqrMtOjqaU0bAWSDIAAEoKytLN954o0JDQ3Xrrbfqxx9/1DfffNPg999yyy2qqKjQtGnTNGzYML388su655576qz78ssv9cADD2jUqFF67rnnVFFRoREjRujAgQOuNd98842+/vprjRo1Si+//LLuu+8+LVmyRJdeeqkOHTp00hpatGihG264QQsWLNCRI0fcti1YsECVlZUaNWqUJOmvf/2rHnzwQXXv3l0vvfSSnn76aZ1//vlavXp1g7/ziWpDVlxcXL3bBw8erHbt2rkFpQ8++ECRkZEaPnx4nfXt27dXTU2N3n333QbX8PPPP9d5OByOxn0RoDkwAALK2rVrjSSzePFiY4wxTqfTtGvXzjz00EN11koyTz75pOv5k08+aSSZa6+91m3dAw88YCSZDRs2uL03NDTUbN261fXahg0bjCTzyiuvuF47dOhQnZ+7cuVKI8m88847p/wuixYtMpLMxx9/7Pb6sGHDTKdOnVzPr7vuOtOjR49TflZ9tm/fbiSZp59+2uzfv98UFBSY//znP+bCCy80kszcuXPd1tfun/3795v/+q//Ml26dHFtu/DCC82dd95pjDm6b8aNG+faVlBQYNq0aWMkmfT0dHPfffeZ2bNnm+Li4jo1jRkzxkiq9zFkyJBGf0cg0HFEBggwWVlZSkxM1GWXXSbp6GXSI0eO1Jw5c1RTU9Ogzxg3bpzb8wkTJkiSPv30U7fXMzMz1blzZ9fzXr16KTo6Wj/99JPrtfDwcNefq6qqdODAAXXp0kWxsbH69ttvT1nH5ZdfrtatW+uDDz5wvVZUVKTFixdr5MiRrtdiY2O1e/fuRh11OtGTTz6pNm3aKCkpSZdccom2bNmiP/7xj7rppptO+p7Ro0dr69at+uabb1z/re+0kiQlJiZqw4YNuu+++1RUVKTXX39do0ePVkJCgp555hkZY9zWh4WFafHixXUe06dPP6PvBwQyggwQQGpqajRnzhxddtll2r59u7Zu3aqtW7eqf//+Kiws1JIlSxr0OV27dnV73rlzZwUFBdXpa0lLS6vz3ri4OBUVFbmeHz58WFOnTlVqaqrsdrtat26tNm3aqLi4WCUlJaesIyQkRCNGjNBHH32kyspKSUevDKqqqnILMo888ogiIyPVr18/de3aVePGjdNXX33VoO8qSffcc48WL16sjz/+WJMmTdLhw4dPG/p69+6t9PR0zZ49W1lZWUpKStLll19+0vXJycmaMWOG8vPzlZubq5dffllt2rTR1KlTNXPmTLe1wcHByszMrPM4//zzG/ydgOaCIAMEkKVLlyo/P19z5sxR165dXY9bbrlFkhrc9PtLNput3tdPdjnwiUcYJkyYoGeffVa33HKLPvzwQ/373//W4sWL1apVKzmdztP+7FGjRqm0tFSfffaZJOnDDz9Uenq6zjvvPNeabt26KTc3V3PmzNGgQYP0j3/8Q4MGDWrwlVpdu3ZVZmamfv3rX+uFF17QpEmT9Oijj2rt2rWnfN/o0aP1wQcfaPbs2Ro5cqSCgk7/T6rNZtM555yjCRMmaMWKFQoKCjrjvxcABBkgoGRlZSkhIUFz586t87j11ls1f/58HT58+LSf8+OPP7o937p1q5xOpzp06NDomv7+979rzJgxrlM1V155pQYNGqTi4uIGvX/w4MFKTk7WBx98oJ9//llLly51OxpTq2XLlho5cqTeeust5eXlafjw4Xr22WdPerXVqTz22GOKiorS448/fsp1o0ePVn5+vn744YeTnlY6lU6dOikuLk75+fmNfi+Ao5gjAwSIw4cPa968ebr55pvr7e1ISUnR+++/r3/+85/1BoETvfrqq7rqqqtcz1955RVJ0tChQxtdV3BwcJ0ekFdeeaXB/TpBQUG66aab9Oabb6pfv36qrq6uU/+BAwfUqlUr1/PQ0FB1795dn332maqqqhp9eXNsbKzuvfdePffcc1q/fv1JT+l07txZL730kg4fPqx+/fqd9PNWr16tjIwMtWzZ0u31NWvW6MCBAxo4cGCj6gNwHEEGCBD//Oc/VVpaqmuvvbbe7RdddJHatGmjrKys0waZ7du369prr9XVV1+tlStX6r333tPo0aPdTuc01K9//Wu9++67iomJUffu3bVy5Up9/vnnbsHjdEaOHKlXXnlFTz75pHr27Klu3bq5bb/qqquUlJSkgQMHKjExUVu2bNGf//xnDR8+XFFRUY2uWZIeeughvfTSS5o+fbrmzJlzynWn8+677yorK0s33HCD+vTpo9DQUG3ZskVvvvmmwsLC9N///d9u66urq/Xee+/V+1k33HBDnUAENGcEGSBAZGVlKSwsTFdeeWW924OCgjR8+HBlZWXVOYLxSx988IGmTp2qRx99VCEhIRo/fryef/75M6rrT3/6k4KDg5WVlaWKigoNHDhQn3/+uYYMGdLgz7j44ouVmpqqXbt21RvCaqcXv/DCCyorK1O7du304IMPnvbU0KmkpKRo9OjRevfdd7Vt2za3q7Ma695771VERISWLFmijz76SA6HQ23atNFVV12lKVOmqHfv3m7rKysrdccdd9T7Wdu3byfIACewmV8e8wXQbD311FN6+umntX//frVu3drqcgDgtGj2BQAAfosgAwAA/BZBBgAA+C16ZAAAgN/iiAwAAPBbBBkAAOC3An6OjNPp1N69exUVFXXS+8UAAADfYoxRaWmpUlJSTnkfs4APMnv37lVqaqrVZQAAgDOwa9cutWvX7qTbAz7I1I4n37Vrl6Kjoy2uBgAANITD4VBqauppbzMS8EGm9nRSdHQ0QQYAAD9zurYQmn0BAIDfIsgAAAC/RZABAAB+iyADAAD8FkEGAAD4LYIMAADwWwQZAADgtwgyAADAbxFkAACA3yLIAAAAv0WQAQAAfosgAwAA/BZBBgAAnJEap9GWfIccFVWW1RDwd78GAACeUVZZrfV5xVq786CydxZpfV6xSiur9dptF2hYz2RLaiLIAACAOowx2l10WN/mFWntjiJl7yzS9wUOOY37upahwTpQVmlNkSLIAAAASVU1Tn2316HsnUXKPnbEpdBRN6C0iwtXn/Zx6ts+The0j1N6UrSCg2wWVHwUQQYAgGao+NCRY6GlSGt3Filnd7Eqqpxua0KCbOrRNkZ90uLUt0Oc+rSPU2J0mEUV148gAwBAgDPG6Kefy5W9oza4HNS2/eV11sVGtFCftKNHWvq2j1OvdrEKDw22oOKGI8gAABBgKqpqlLO7RGt3HtS3x466FB2qe2VRpzYt1bf90SMtfdrHq1Prlgqy8DTRmSDIAADg5/Y5KlyniLJ3Fum7vSWqqnHvyrWHBOm8drGuoy0XtI9TfMtQiyr2HIIMAAB+pMZplFtQquy8ImXvOKjsvCLtOni4zro2UfYTjrbEqUdKjEJDAm98HEEGAAAfVlZZrXV5Ra7G3HV5xSqrrHZbY7NJ5yZGuRpy+7aPV7u4cNls/nWa6EwQZAAA8BG1s1tOvJoo9ySzW3qnHT/a0jstVlFhLawp2mIEGQAALFI7u2XtjoP69thRF3+Y3eJLCDIAADSRovIjrsDiz7NbfAlBBgAALwjk2S2+hCADAIAHNKfZLb6EIAMAwBlozOyWPh3iXEddAmF2iy8hyAAAcBrMbvFdBBkAAH6hobNb0pOi1ad9rPq2j1ef9nHNZnaLLyHIAACatTOZ3dK3Q5zOT22+s1t8CUEGANCsHKl2anP+8dkta3cUaV9p/bNbTmzKPTcpitktPoggAwAIaLWzW2qbcjfsKlZldf2zW07sb2F2i38gyAAAAoYxRtv2l+vbY3NbsncWnXJ2S+3VROelxiqsBbNb/BFBBgDgtyqqarRhV/Gxq4mK9G1ew2a3dG7TkqbcAGFpkJkxY4ZmzJihHTt2SJJ69OihqVOnaujQoZKkSy+9VMuXL3d7z7333qvXX3+9qUsFAPiAE2e3rN1ZpO/2lKjayeyW5szSINOuXTtNnz5dXbt2lTFGb7/9tq677jqtW7dOPXr0kCTdfffd+p//+R/XeyIiIqwqFwDQhJjdgoawNMhcc801bs+fffZZzZgxQ6tWrXIFmYiICCUlJVlRHgCgCZVWVGn9rmJmt6BRfKZHpqamRnPnzlV5ebkGDBjgej0rK0vvvfeekpKSdM011+iJJ57gqAwA+LkTZ7ccbcotrnd2S6Q9RL3TYnVBGrNbUD/Lg8zGjRs1YMAAVVRUKDIyUvPnz1f37t0lSaNHj1b79u2VkpKinJwcPfLII8rNzdW8efNO+nmVlZWqrDw+D8DhcHj9OwAATo3ZLfAWmzHGnH6Z9xw5ckR5eXkqKSnR3//+d/3tb3/T8uXLXWHmREuXLtUVV1yhrVu3qnPnzvV+3lNPPaWnn366zuslJSWKjo72eP0AgLqY3YKz5XA4FBMTc9rf35YHmV/KzMxU586d9cYbb9TZVl5ersjISC1cuFBDhgyp9/31HZFJTU0lyACAlzC7Bd7Q0CBj+amlX3I6nW5B5ETr16+XJCUnJ5/0/Xa7XXa73RulAQBUd3ZLdl6RiuuZ3dK5Tcuj9yVqH68L2scxuwVeYWmQmTJlioYOHaq0tDSVlpZq9uzZWrZsmRYtWqRt27Zp9uzZGjZsmFq1aqWcnBxNmjRJgwcPVq9evawsGwCalX2OCtcpolPObkmNPRZc4tQ7jdktaBqWBpl9+/bpN7/5jfLz8xUTE6NevXpp0aJFuvLKK7Vr1y59/vnneumll1ReXq7U1FSNGDFCjz/+uJUlA0BAc81uOXaKaO3OIu0uqju7JSHKrr4d4o5dTRSv7snRzG6BJXyuR8bTGnqODQCao9rZLWuPjfevb3ZLkE06NynarSmX2S3wNr/tkQEAeAezWxCICDIAEKCOVDv13d4S16Tc7J3MbkHgIcgAQIAoKj9yNLAcu5pow25mtyDwEWQAwA/Vzm45sSn3J2a3oBkiyACAH6id3bJ2Z5G+3cnsFqAWQQYAfNCZzG65IC1OccxuQTNDkAEAizG7BThzBBkAaGLMbgE8hyADAF7U2NkttaGF2S1AwxBkAMCDGjq7JTU+/NjVRPHqkxbH7BbgDBFkAOAsNGR2S4tgm3qkxBxvymV2C+AxBBkAaKAzmd3St328erWLYXYL4CUEGQA4icNHapSzm9ktgC8jyADAMYWOiqNHWnYcDS2nmt1SezURs1sAaxFkADRLNU6j7wsc+vbYKaLs08xu6dM+Xn3axzG7BfAxBBkAzUJpRZXW5RW7riRal1ek8iM1bmuY3QL4H4IMgIBTO7tlbW1T7o4i5RaWyjC7BQg4BBkAfu+Xs1vW7izSfma3AM0CQQaA3zlYfsTV2/LtzobNbunTPk4JzG4BAg5BBoBPOzq7pcztaqL6ZrfERbQ4ehVRe2a3AM0JQQaAT2nM7Ja+x64k6tMhTp1aM7sFaI4IMgAsxewWAGeDIAOgyTC7BYCnEWQAeE1DZ7ekJ0UfbcrtcPRoC7NbADQUQQaAR5zJ7Ja+7eN1XmoMs1sAnDGCDIAz0pjZLbU3U+zbPk7nJDK7BYDnEGQANEhjZrecOOKf2S0AvIkgA6AOp9Pop5+Z3QLA9xFkAOjwkRpt2H28KfdbZrcA8BMEGaAZKnRUHD3SsrNI2TsP6ru9Dma3APBLBBkgwNXObnE15e4o0p5iZrcACAwEGSDA1M5uqW3KZXYLgEBGkAH8GLNbADR3BBnAjzC7BQDcEWQAH8bsFgA4NYIM4CPqzG7ZWaSffj757JbaplxmtwBozggygEUaOrulS0Kk+qQdndvSpz2zWwDgRAQZoIk0ZHZLWIsgndcu1nU1Ue9UZrcAwKkQZAAvOJPZLX3bx6kbs1sAoFEIMoAHOCqqtJ7ZLQDQ5AgyQCMZY7Tr4GFl5x10nSo61eyW2nsTMbsFADyPIAOcxpFqpzbtLTl6GfSxO0HXN7slLT7CdflzH2a3AECTIMgAv3Cw/IirtyV750Hl7C5hdgsA+CiCDJq12tktx68mYnYLAPgTggyaFWa3AEBgIcggoBWUVBy7J9FBfbuziNktABBgLA0yM2bM0IwZM7Rjxw5JUo8ePTR16lQNHTpUklRRUaHf/e53mjNnjiorKzVkyBC99tprSkxMtLBq+KrqGqe+LyjVt3nHR/zXN7slMdrudkPF7inRahHM7BYA8EeWBpl27dpp+vTp6tq1q4wxevvtt3Xddddp3bp16tGjhyZNmqR//etfmjt3rmJiYjR+/HjdeOON+uqrr6wsGz6iMbNb+nY43pTbNpbZLQAQKGzG/HL6hbXi4+P1/PPP66abblKbNm00e/Zs3XTTTZKk77//Xt26ddPKlSt10UUXNejzHA6HYmJiVFJSoujoaG+WDi8609kt56fFKtLOGVQA8DcN/f3tM//C19TUaO7cuSovL9eAAQOUnZ2tqqoqZWZmutakp6crLS3tlEGmsrJSlZXHZ3w4HA6v1w7PY3YLAKAhLA8yGzdu1IABA1RRUaHIyEjNnz9f3bt31/r16xUaGqrY2Fi39YmJiSooKDjp502bNk1PP/20l6uGp/1ydsuG3SU6Us/sloy2MeqTdnzEP7NbAKB5szzInHvuuVq/fr1KSkr097//XWPGjNHy5cvP+POmTJmiyZMnu547HA6lpqZ6olR4SONmt8S7ribq2ZbZLQAAd5YHmdDQUHXp0kWS1KdPH33zzTf605/+pJEjR+rIkSMqLi52OypTWFiopKSkk36e3W6X3W73dtlohF/ObsneWaSSw/XPbunbPs51NVFHZrcAAE7D8iDzS06nU5WVlerTp49atGihJUuWaMSIEZKk3Nxc5eXlacCAARZXiVNpzOyW2quJLkiLU2wEs1sAAI1jaZCZMmWKhg4dqrS0NJWWlmr27NlatmyZFi1apJiYGN11112aPHmy4uPjFR0drQkTJmjAgAENvmIJ3sfsFgCAlSwNMvv27dNvfvMb5efnKyYmRr169dKiRYt05ZVXSpJefPFFBQUFacSIEW4D8WAdR0WV1uUVu5py1+cVM7sFAGAZn5sj42nMkTlztbNb1u486OptqW92S5Q9ROczuwUA4EF+N0cG1qusrtF3ex3Krr2aiNktAAAfR5Bpxmpnt9Q25TK7BQDgbwgyzUhldY3+uX6v1mw/eNLZLfEtQ3VBWhyzWwAAfoEg04zM+mqHpn32vdtrzG4BAPgzgkwz8s2OIknS1T2SdMuF7ZjdAgDwewSZZmTjnmJJ0v93SUf17RBvbTEAAHgAE8maiUJHhQodlQqySd1TuAwdABAYCDLNxMbdJZKkrglRigjlQBwAIDAQZJqJnD1Hg0zPdjEWVwIAgOcQZJqJjbuLJUm9CDIAgABCkGkGjDHaWHtEpi1BBgAQOAgyzUB+SYV+LjuikCCbuiXT6AsACBwEmWYg51ij7zmJUUzpBQAEFIJMM1A7P4b+GABAoCHINAO1R2S4YgkAEGgIMgHuxEbfXm1jrS0GAAAPI8gEuN1Fh1V8qEqhwUE6JynS6nIAAPAogkyAqz2tlJ4cJXsIjb4AgMBCkAlwOccafZkfAwAIRASZAFd7jyWuWAIABCKCTABzOk+c6BtrbTEAAHgBQSaA7Tx4SKUV1bKHBKlrIo2+AIDAQ5AJYDnHbhTZPSVaLYL5qwYABB5+uwUwV38Mjb4AgABFkAlgObX9Me1irS0EAAAvIcgEqBqn0Xd7uGIJABDYCDIBavvPZSo/UqPwFsHq3IZGXwBAYCLIBKjaib4ZbaMVHGSzuBoAALyDIBOgXHe8Zn4MACCAEWQC1Eb6YwAAzQBBJgBV1zj13d7aK5YIMgCAwEWQCUBb95eposqpSHuIOrZqaXU5AAB4DUEmAG08odE3iEZfAEAAI8gEoOP9MbHWFgIAgJcRZALQ8SuW6I8BAAQ2gkyAqapxanO+QxJXLAEAAh9BJsD8UFiqI9VORYeFKC0+wupyAADwKoJMgHHd8bpdrGw2Gn0BAIGNIBNgau94nUF/DACgGSDIBJjjR2QIMgCAwEeQCSCV1TX6vuBooy9XLAEAmgOCTADJLShVVY1RXEQLtYsLt7ocAAC8jiATQFzzY2j0BQA0EwSZAOLqj+G0EgCgmSDIBJDaK5a44zUAoLmwNMhMmzZNF154oaKiopSQkKDrr79eubm5bmsuvfRS2Ww2t8d9991nUcW+q6KqRj8UlkriiiUAQPNhaZBZvny5xo0bp1WrVmnx4sWqqqrSVVddpfLycrd1d999t/Lz812P5557zqKKfdfmfIdqnEatI+1Kig6zuhwAAJpEiJU/fOHChW7PZ82apYSEBGVnZ2vw4MGu1yMiIpSUlNTU5fmVE+fH0OgLAGgufKpHpqTk6C/j+Ph4t9ezsrLUunVrZWRkaMqUKTp06NBJP6OyslIOh8Pt0Rxwx2sAQHNk6RGZEzmdTk2cOFEDBw5URkaG6/XRo0erffv2SklJUU5Ojh555BHl5uZq3rx59X7OtGnT9PTTTzdV2T5j455iSfTHAACaF5sxxlhdhCTdf//9+uyzz/Tll1+qXbt2J123dOlSXXHFFdq6das6d+5cZ3tlZaUqKytdzx0Oh1JTU1VSUqLo6Giv1G618spq9XxqkZxGWvPfVyiBHhkAgJ9zOByKiYk57e9vnzgiM378eH3yySdasWLFKUOMJPXv31+SThpk7Ha77Ha7V+r0VZvzHXIaKSk6jBADAGhWLA0yxhhNmDBB8+fP17Jly9SxY8fTvmf9+vWSpOTkZC9X5z+OT/TltBIAoHmxNMiMGzdOs2fP1kcffaSoqCgVFBRIkmJiYhQeHq5t27Zp9uzZGjZsmFq1aqWcnBxNmjRJgwcPVq9evaws3ads3F0siYm+AIDmx9IgM2PGDElHh96d6K233tLYsWMVGhqqzz//XC+99JLKy8uVmpqqESNG6PHHH7egWt/FRF8AQHNl+amlU0lNTdXy5cubqBr/VFpRpZ/2Hx0gyKXXAIDmxqfmyKDxNu05OienbWy4WkU2ryZnAAAIMn6O+TEAgOaMIOPnuGIJANCcEWT83MZjjb692sZaWwgAABYgyPixkkNV2nng6H2naPQFADRHBBk/Vns0pn2rCMVEtLC4GgAAmh5Bxo/lHGv05WgMAKC5alSQ2bdv3ym3V1dXa82aNWdVEBpu47FGX65YAgA0V40KMsnJyW5hpmfPntq1a5fr+YEDBzRgwADPVYdTcl2xRKMvAKCZalSQ+eUk3h07dqiqquqUa+AdB8uPaE/xYUlSRtuT394cAIBA5vEeGZvN5umPRD1qG307tWmpqDAafQEAzRPNvn6KO14DANDIm0babDaVlpYqLCxMxhjZbDaVlZXJ4Th6v5/a/8L7jk/0jbW2EAAALNSoIGOM0TnnnOP2vHfv3m7PObXUNFwTfbliCQDQjDUqyHzxxRfeqgONsK+0QvklFQqySd2TafQFADRfjQoyv/rVr7xVBxph07GjMV0SItXS3qi/QgAAAkqjfgtWV1erpqZGdrvd9VphYaFef/11lZeX69prr9WgQYM8XiTc1fbHZNDoCwBo5hoVZO6++26FhobqjTfekCSVlpbqwgsvVEVFhZKTk/Xiiy/qo48+0rBhw7xSLI5yTfQlyAAAmrlGXX791VdfacSIEa7n77zzjmpqavTjjz9qw4YNmjx5sp5//nmPF4njjDHK2cMVSwAASI0MMnv27FHXrl1dz5csWaIRI0YoJubokYExY8bou+++82yFcFPoqNT+0koFB9lo9AUANHuNCjJhYWE6fPiw6/mqVavUv39/t+1lZWWeqw515BwbhNc1IVLhocHWFgMAgMUaFWTOP/98vfvuu5Kk//znPyosLNTll1/u2r5t2zalpKR4tkK4YX4MAADHNarZd+rUqRo6dKg+/PBD5efna+zYsUpOTnZtnz9/vgYOHOjxInEcE30BADiu0XNksrOz9e9//1tJSUm6+eab3baff/756tevn0cLxHHGmONHZLhiCQCAxgUZSerWrZu6detW77Z77rnnrAvCye0pPqyD5UfUItim9OQoq8sBAMByjQoyK1asaNC6wYMHn1ExOLXa+THnJkXJHkKjLwAAjQoyl156qeumkMaYetfYbDbV1NScfWWowzU/pm2stYUAAOAjGhVk4uLiFBUVpbFjx+qOO+5Q69atvVUX6uGa6MsVSwAASGrk5df5+fn6wx/+oJUrV6pnz56666679PXXXys6OloxMTGuBzzPGOOaIdOTRl8AACQ1MsiEhoZq5MiRWrRokb7//nv16tVL48ePV2pqqh577DFVV1d7q85mL+/gITkqqhUaEqRzEmn0BQBAamSQOVFaWpqmTp2qzz//XOecc46mT58uh8Phydpwgtr5Md2SoxUacsZ/bQAABJQz+o1YWVmp2bNnKzMzUxkZGWrdurX+9a9/KT4+3tP14RjmxwAAUFejmn3XrFmjt956S3PmzFGHDh1055136sMPPyTANAFXfwyNvgAAuDQqyFx00UVKS0vTgw8+qD59+kiSvvzyyzrrrr32Ws9UB0mS02m0ac/R03ZcsQQAwHGNnuybl5enZ5555qTbmSPjedsPlKusslphLYLUpU2k1eUAAOAzGhVknE7nadccOnTojItB/Wrnx/RIiVFIMI2+AADU8thvxcrKSr3wwgvq1KmTpz4Sx7jueE2jLwAAbhoVZCorKzVlyhT17dtXF198sRYsWCBJevPNN9WxY0e9+OKLmjRpkjfqbNY27imWRH8MAAC/1KhTS1OnTtUbb7yhzMxMff3117r55pt15513atWqVXrhhRd08803KziYmxl6Ug2NvgAAnFSjgszcuXP1zjvv6Nprr9WmTZvUq1cvVVdXa8OGDa6bScKztu0v0+GqGrUMDVbH1jT6AgBwokadWtq9e7frsuuMjAzZ7XZNmjSJEONFtf0xPdrGKDiI/QwAwIkaFWRqamoUGhrqeh4SEqLISI4SeNPGY4PwmOgLAEBdjTq1ZIzR2LFjZbfbJUkVFRW677771LJlS7d18+bN81yFzVzOsVsTMNEXAIC6GhVkxowZ4/b89ttv92gxcFdd49TmvbWNvrHWFgMAgA9qVJB56623vFUH6vHjvjJVVjsVFRai9vERVpcDAIDPsXRM7LRp03ThhRcqKipKCQkJuv7665Wbm+u2pqKiQuPGjVOrVq0UGRmpESNGqLCw0KKKm9bGEwbhBdHoCwBAHZYGmeXLl2vcuHFatWqVFi9erKqqKl111VUqLy93rZk0aZI+/vhjzZ07V8uXL9fevXt14403Wlh108k5NgiP/hgAAOrX6JtGetLChQvdns+aNUsJCQnKzs7W4MGDVVJSopkzZ2r27Nm6/PLLJR09vdWtWzetWrVKF110kRVlN5naIzK92sZaWwgAAD7Kp+5AWFJy9Bd3fHy8JCk7O1tVVVXKzMx0rUlPT1daWppWrlxZ72dUVlbK4XC4PfzRkWqntuSXSmKiLwAAJ+MzQcbpdGrixIkaOHCgMjIyJEkFBQUKDQ1VbGys29rExEQVFBTU+znTpk1TTEyM65Gamurt0r3ih8JSHalxKjaihdrFhVtdDgAAPslngsy4ceO0adMmzZkz56w+Z8qUKSopKXE9du3a5aEKm9aJd7xmcjIAAPWztEem1vjx4/XJJ59oxYoVateunev1pKQkHTlyRMXFxW5HZQoLC5WUlFTvZ9ntdtfAPn9We8frnkz0BQDgpCw9ImOM0fjx4zV//nwtXbpUHTt2dNvep08ftWjRQkuWLHG9lpubq7y8PA0YMKCpy21StUdk6I8BAODkLD0iM27cOM2ePVsfffSRoqKiXH0vMTExCg8PV0xMjO666y5NnjxZ8fHxio6O1oQJEzRgwICAvmKpoqpGuQVHG317MtEXAICTsjTIzJgxQ5J06aWXur3+1ltvaezYsZKkF198UUFBQRoxYoQqKys1ZMgQvfbaa01cadP6vqBU1U6jVi1DlRITZnU5AAD4LEuDjDHmtGvCwsL06quv6tVXX22CinxD7R2ve7aj0RcAgFPxmauWcJyrP4ZGXwAATokg44M27jl26TX9MQAAnBJBxsccPlKjHwqZ6AsAQEMQZHzM5vwSOY2UEGVXYjSNvgAAnApBxscwPwYAgIYjyPiYja5bE8RaWwgAAH6AIONjcvZwRAYAgIYiyPiQsspqbdtfJknK4NJrAABOiyDjQ77bUyJjpJSYMLWJ8v8bXwIA4G0EGR9yfH4MR2MAAGgIgowPOX7FUqy1hQAA4CcIMj7EdUSG/hgAABqEIOMjSg5XafvP5ZIIMgAANBRBxkd8d+xoTGp8uOJahlpcDQAA/oEg4yNc82MYhAcAQIMRZHyEa6IvVywBANBgBBkfkbOnWJLUi/4YAAAajCDjA4rKj2jXwcOSpB4EGQAAGowg4wNqL7vu2LqlYsJbWFwNAAD+gyDjA5gfAwDAmSHI+ICc3cWSuOM1AACNRZDxAZv2OCRxRAYAgMYiyFjs57JK7Sk+LJuNRl8AABqLIGOx2v6Yzm0iFWkPsbgaAAD8C0HGYrWD8JgfAwBA4xFkLJbDRF8AAM4YQcZiG2sn+hJkAABoNIKMhQodFSp0VCrIJnVPJsgAANBYBBkL1fbHnJMYpfDQYIurAQDA/xBkLJTDRF8AAM4KQcZCG49N9KXRFwCAM0OQsYgxhnssAQBwlggyFskvqdDPZUcUEmRTt+Roq8sBAMAvEWQsknNCo29YCxp9AQA4EwQZizA/BgCAs0eQsQgTfQEAOHsEGQuc2Ojbq22stcUAAODHCDIW2F10WMWHqhQaHKRzkiKtLgcAAL9FkLFA7Wml9OQo2UNo9AUA4EwRZCyQc6zRl/kxAACcHYKMBWrvscQVSwAAnB2CTBNzOk+c6BtrbTEAAPg5gkwT23nwkEorqmUPCVLXRBp9AQA4GwSZJpZz7EaR3VOi1SKY3Q8AwNngN2kTc/XH0OgLAMBZszTIrFixQtdcc41SUlJks9m0YMECt+1jx46VzWZze1x99dXWFOshObX9Me1irS0EAIAAYGmQKS8v13nnnadXX331pGuuvvpq5efnux7vv/9+E1boWTVOo+/2cMUSAACeEmLlDx86dKiGDh16yjV2u11JSUlNVJF3bf+5TOVHahTeIlid29DoCwDA2fL5Hplly5YpISFB5557ru6//34dOHDglOsrKyvlcDjcHr6idqJvRttoBQfZLK4GAAD/59NB5uqrr9Y777yjJUuW6A9/+IOWL1+uoUOHqqam5qTvmTZtmmJiYlyP1NTUJqz41Fx3vGZ+DAAAHmHpqaXTGTVqlOvPPXv2VK9evdS5c2ctW7ZMV1xxRb3vmTJliiZPnux67nA4fCbMbKQ/BgAAj/LpIzK/1KlTJ7Vu3Vpbt2496Rq73a7o6Gi3hy+ornHqu721VywRZAAA8AS/CjK7d+/WgQMHlJycbHUpjbZ1f5kqqpyKtIeoY6uWVpcDAEBAsPTUUllZmdvRle3bt2v9+vWKj49XfHy8nn76aY0YMUJJSUnatm2bHn74YXXp0kVDhgyxsOozc2KjbxCNvgAAeISlQWbt2rW67LLLXM9re1vGjBmjGTNmKCcnR2+//baKi4uVkpKiq666Ss8884zsdrtVJZ+x43e8jrW2EAAAAoilQebSSy+VMeak2xctWtSE1XjX8Tte0x8DAICn+FWPjL+qqnFqc/7ReTZcsQQAgOcQZJrAD4WlOlLtVHRYiNLiI6wuBwCAgEGQaQIn9sfYbDT6AgDgKQSZJnD8jtecVgIAwJMIMk3AdUSGRl8AADyKIONlldU1+r7gaKMvR2QAAPAsgoyX5RaUqqrGKL5lqNrGhltdDgAAAYUg42XH73gdQ6MvAAAeRpDxso27GYQHAIC3EGS8jCuWAADwHoKMF1VU1eiHwlJJTPQFAMAbCDJetDnfoRqnUetIu5Kiw6wuBwCAgEOQ8aLjE31p9AUAwBsIMl6UQ6MvAABeRZDxoo17iiXRHwMAgLcQZLykvLJaW/eVSeKIDAAA3kKQ8ZLN+Q45jZQUHaYEGn0BAPAKgoyXuPpjOK0EAIDXEGS8ZOPuYknc8RoAAG8iyHgJE30BAPA+gowXlFZU6af95ZJo9AUAwJsIMl6waY9DktQ2NlytIu0WVwMAQOAiyHgB82MAAGgaBBkv4IolAACaBkHGCzYea/Tt1TbW2kIAAAhwBBkPKzlUpZ0HDkmi0RcAAG8jyHhY7dGY9q0iFBPRwuJqAAAIbAQZD8s51ujL0RgAALyPIONhG481+nLFEgAA3keQ8TDXFUs0+gIA4HUEGQ86UFapPcWHJUkZbaMtrgYAgMBHkPGg2kbfTm1aKiqMRl8AALyNIONBrv4YGn0BAGgSBBkP2ui643WstYUAANBMEGQ8yDXRlyuWAABoEgQZDykqP6L8kgpJUvdkGn0BAGgKBBkPKXAcDTGtWoaqpT3E4moAAGgeCDIesr+0UpLUJspucSUAADQfBBkPIcgAAND0CDIesr/sWJCJJMgAANBUCDIewhEZAACaHkHGQwgyAAA0PYKMhxBkAABoegQZD6FHBgCApkeQ8ZB9x+bIJEQTZAAAaCqWBpkVK1bommuuUUpKimw2mxYsWOC23RijqVOnKjk5WeHh4crMzNSPP/5oTbGnUFFVI0dFtSSpTWSYxdUAANB8WBpkysvLdd555+nVV1+td/tzzz2nl19+Wa+//rpWr16tli1basiQIaqoqGjiSk/t52OnlUKDgxQdzlRfAACaiqW/dYcOHaqhQ4fWu80Yo5deekmPP/64rrvuOknSO++8o8TERC1YsECjRo1qylJP6cRGX5vNZnE1AAA0Hz7bI7N9+3YVFBQoMzPT9VpMTIz69++vlStXnvR9lZWVcjgcbg9vqw0yrbliCQCAJuWzQaagoECSlJiY6PZ6YmKia1t9pk2bppiYGNcjNTXVq3VKXLEEAIBVfDbInKkpU6aopKTE9di1a5fXfyYzZAAAsIbPBpmkpCRJUmFhodvrhYWFrm31sdvtio6Odnt4G0EGAABr+GyQ6dixo5KSkrRkyRLXaw6HQ6tXr9aAAQMsrKwuggwAANaw9KqlsrIybd261fV8+/btWr9+veLj45WWlqaJEyfqf//3f9W1a1d17NhRTzzxhFJSUnT99ddbV3Q96JEBAMAalgaZtWvX6rLLLnM9nzx5siRpzJgxmjVrlh5++GGVl5frnnvuUXFxsQYNGqSFCxcqLMy3hs5xRAYAAGvYjDHG6iK8yeFwKCYmRiUlJV7plzHGKP2Jhaqsduo/D1+m1PgIj/8MAACam4b+/vbZHhl/UVpZrcpqpySpNaeWAABoUgSZs1R7WinKHqLw0GCLqwEAoHkhyJwl+mMAALAOQeYscXsCAACsQ5A5SxyRAQDAOgSZs8QMGQAArEOQOUu1R2QSogkyAAA0NYLMWXKdWuKIDAAATY4gc5bokQEAwDoEmbO0jyADAIBlCDJnocZpdLCcIAMAgFUIMmfhQHmlnEYKskmtWhJkAABoagSZs1DbHxPf0q7gIJvF1QAA0PwQZM4Cjb4AAFiLIHMWCDIAAFiLIHMWmOoLAIC1CDJngSMyAABYiyBzFggyAABYiyBzFggyAABYiyBzFuiRAQDAWgSZs8ARGQAArEWQOUNlldUqraiWRJABAMAqBJkz9JuZq11/jg4LsbASAACaL4LMGRp/eRe1jQ3X5CvPkc3G7QkAALAChxLO0OXpibr80USrywAAoFnjiAwAAPBbBBkAAOC3CDIAAMBvEWQAAIDfIsgAAAC/RZABAAB+iyADAAD8FkEGAAD4LYIMAADwWwQZAADgtwgyAADAbxFkAACA3yLIAAAAv0WQAQAAfivE6gK8zRgjSXI4HBZXAgAAGqr293bt7/GTCfggU1paKklKTU21uBIAANBYpaWliomJOel2mzld1PFzTqdTe/fuVVRUlGw2m8c+1+FwKDU1Vbt27VJ0dLTHPhd1sa+bBvu5abCfmwb7uWl4cz8bY1RaWqqUlBQFBZ28Eybgj8gEBQWpXbt2Xvv86Oho/idpIuzrpsF+bhrs56bBfm4a3trPpzoSU4tmXwAA4LcIMgAAwG8RZM6Q3W7Xk08+KbvdbnUpAY993TTYz02D/dw02M9Nwxf2c8A3+wIAgMDFERkAAOC3CDIAAMBvEWQAAIDfIsgAAAC/RZA5Q6+++qo6dOigsLAw9e/fX2vWrLG6JJ81bdo0XXjhhYqKilJCQoKuv/565ebmuq2pqKjQuHHj1KpVK0VGRmrEiBEqLCx0W5OXl6fhw4crIiJCCQkJ+v3vf6/q6mq3NcuWLdMFF1wgu92uLl26aNasWd7+ej5r+vTpstlsmjhxous19rNn7NmzR7fffrtatWql8PBw9ezZU2vXrnVtN8Zo6tSpSk5OVnh4uDIzM/Xjjz+6fcbBgwd12223KTo6WrGxsbrrrrtUVlbmtiYnJ0eXXHKJwsLClJqaqueee65Jvp+vqKmp0RNPPKGOHTsqPDxcnTt31jPPPON27x32deOtWLFC11xzjVJSUmSz2bRgwQK37U25T+fOnav09HSFhYWpZ8+e+vTTTxv/hQwabc6cOSY0NNS8+eab5rvvvjN33323iY2NNYWFhVaX5pOGDBli3nrrLbNp0yazfv16M2zYMJOWlmbKyspca+677z6TmppqlixZYtauXWsuuugic/HFF7u2V1dXm4yMDJOZmWnWrVtnPv30U9O6dWszZcoU15qffvrJREREmMmTJ5vNmzebV155xQQHB5uFCxc26ff1BWvWrDEdOnQwvXr1Mg899JDrdfbz2Tt48KBp3769GTt2rFm9erX56aefzKJFi8zWrVtda6ZPn25iYmLMggULzIYNG8y1115rOnbsaA4fPuxac/XVV5vzzjvPrFq1yvznP/8xXbp0Mbfeeqtre0lJiUlMTDS33Xab2bRpk3n//fdNeHi4eeONN5r0+1rp2WefNa1atTKffPKJ2b59u5k7d66JjIw0f/rTn1xr2NeN9+mnn5rHHnvMzJs3z0gy8+fPd9veVPv0q6++MsHBwea5554zmzdvNo8//rhp0aKF2bhxY6O+D0HmDPTr18+MGzfO9bympsakpKSYadOmWViV/9i3b5+RZJYvX26MMaa4uNi0aNHCzJ0717Vmy5YtRpJZuXKlMebo/3hBQUGmoKDAtWbGjBkmOjraVFZWGmOMefjhh02PHj3cftbIkSPNkCFDvP2VfEppaanp2rWrWbx4sfnVr37lCjLsZ8945JFHzKBBg0663el0mqSkJPP888+7XisuLjZ2u928//77xhhjNm/ebCSZb775xrXms88+MzabzezZs8cYY8xrr71m4uLiXPu99mefe+65nv5KPmv48OHmt7/9rdtrN954o7ntttuMMexrT/hlkGnKfXrLLbeY4cOHu9XTv39/c++99zbqO3BqqZGOHDmi7OxsZWZmul4LCgpSZmamVq5caWFl/qOkpESSFB8fL0nKzs5WVVWV2z5NT09XWlqaa5+uXLlSPXv2VGJiomvNkCFD5HA49N1337nWnPgZtWua29/LuHHjNHz48Dr7gv3sGf/85z/Vt29f3XzzzUpISFDv3r3117/+1bV9+/btKigocNtHMTEx6t+/v9t+jo2NVd++fV1rMjMzFRQUpNWrV7vWDB48WKGhoa41Q4YMUW5uroqKirz9NX3CxRdfrCVLluiHH36QJG3YsEFffvmlhg4dKol97Q1NuU899W8JQaaRfv75Z9XU1Lj9Qy9JiYmJKigosKgq/+F0OjVx4kQNHDhQGRkZkqSCggKFhoYqNjbWbe2J+7SgoKDefV677VRrHA6HDh8+7I2v43PmzJmjb7/9VtOmTauzjf3sGT/99JNmzJihrl27atGiRbr//vv14IMP6u2335Z0fD+d6t+IgoICJSQkuG0PCQlRfHx8o/4uAt2jjz6qUaNGKT09XS1atFDv3r01ceJE3XbbbZLY197QlPv0ZGsau88D/u7X8C3jxo3Tpk2b9OWXX1pdSsDZtWuXHnroIS1evFhhYWFWlxOwnE6n+vbtq//7v/+TJPXu3VubNm3S66+/rjFjxlhcXWD58MMPlZWVpdmzZ6tHjx5av369Jk6cqJSUFPY1XDgi00itW7dWcHBwnSs9CgsLlZSUZFFV/mH8+PH65JNP9MUXX6hdu3au15OSknTkyBEVFxe7rT9xnyYlJdW7z2u3nWpNdHS0wsPDPf11fE52drb27dunCy64QCEhIQoJCdHy5cv18ssvKyQkRImJiexnD0hOTlb37t3dXuvWrZvy8vIkHd9Pp/o3IikpSfv27XPbXl1drYMHDzbq7yLQ/f73v3cdlenZs6fuuOMOTZo0yXXEkX3teU25T0+2prH7nCDTSKGhoerTp4+WLFnies3pdGrJkiUaMGCAhZX5LmOMxo8fr/nz52vp0qXq2LGj2/Y+ffqoRYsWbvs0NzdXeXl5rn06YMAAbdy40e1/nsWLFys6Otr1S2XAgAFun1G7prn8vVxxxRXauHGj1q9f73r07dtXt912m+vP7OezN3DgwDrjA3744Qe1b99ektSxY0clJSW57SOHw6HVq1e77efi4mJlZ2e71ixdulROp1P9+/d3rVmxYoWqqqpcaxYvXqxzzz1XcXFxXvt+vuTQoUMKCnL/NRUcHCyn0ymJfe0NTblPPfZvSaNag2GMOXr5td1uN7NmzTKbN28299xzj4mNjXW70gPH3X///SYmJsYsW7bM5Ofnux6HDh1yrbnvvvtMWlqaWbp0qVm7dq0ZMGCAGTBggGt77WXBV111lVm/fr1ZuHChadOmTb2XBf/+9783W7ZsMa+++mqzuiy4PidetWQM+9kT1qxZY0JCQsyzzz5rfvzxR5OVlWUiIiLMe++951ozffp0Exsbaz766COTk5NjrrvuunovX+3du7dZvXq1+fLLL03Xrl3dLl8tLi42iYmJ5o477jCbNm0yc+bMMREREQF7SXB9xowZY9q2beu6/HrevHmmdevW5uGHH3atYV83XmlpqVm3bp1Zt26dkWReeOEFs27dOrNz505jTNPt06+++sqEhISY//f//p/ZsmWLefLJJ7n8uim98sorJi0tzYSGhpp+/fqZVatWWV2Sz5JU7+Ott95yrTl8+LB54IEHTFxcnImIiDA33HCDyc/Pd/ucHTt2mKFDh5rw8HDTunVr87vf/c5UVVW5rfniiy/M+eefb0JDQ02nTp3cfkZz9Msgw372jI8//thkZGQYu91u0tPTzV/+8he37U6n0zzxxBMmMTHR2O12c8UVV5jc3Fy3NQcOHDC33nqriYyMNNHR0ebOO+80paWlbms2bNhgBg0aZOx2u2nbtq2ZPn2617+bL3E4HOahhx4yaWlpJiwszHTq1Mk89thjbpf0sq8b74svvqj33+QxY8YYY5p2n3744YfmnHPOMaGhoaZHjx7mX//6V6O/j82YE0YkAgAA+BF6ZAAAgN8iyAAAAL9FkAEAAH6LIAMAAPwWQQYAAPgtggwAAPBbBBkAAOC3CDIAfM6yZctks9nq3BfqVJ566imdf/75XqsJgG8iyACwzMqVKxUcHKzhw4dbXQoAP0WQAWCZmTNnasKECVqxYoX27t1rdTkA/BBBBoAlysrK9MEHH+j+++/X8OHDNWvWrJOunTVrlmJjY7VgwQJ17dpVYWFhGjJkiHbt2lVn7bvvvqsOHTooJiZGo0aNUmlpqWvbwoULNWjQIMXGxqpVq1b69a9/rW3btnnj6wFoIgQZAJb48MMPlZ6ernPPPVe333673nzzTZ3q1m+HDh3Ss88+q3feeUdfffWViouLNWrUKLc127Zt04IFC/TJJ5/ok08+0fLlyzV9+nTX9vLyck2ePFlr167VkiVLFBQUpBtuuEFOp9Nr3xOAd4VYXQCA5mnmzJm6/fbbJUlXX321SkpKtHz5cl166aX1rq+qqtKf//xn9e/fX5L09ttvq1u3blqzZo369esnSXI6nZo1a5aioqIkSXfccYeWLFmiZ599VpI0YsQIt89888031aZNG23evFkZGRne+JoAvIwjMgCaXG5urtasWaNbb71VkhQSEqKRI0dq5syZJ31PSEiILrzwQtfz9PR0xcbGasuWLa7XOnTo4AoxkpScnKx9+/a5nv/444+69dZb1alTJ0VHR6tDhw6SpLy8PE99NQBNjCMyAJrczJkzVV1drZSUFNdrxhjZ7Xb9+c9/PuPPbdGihdtzm83mdtrommuuUfv27fXXv/5VKSkpcjqdysjI0JEjR874ZwKwFkdkADSp6upqvfPOO/rjH/+o9evXux4bNmxQSkqK3n///ZO+b+3ata7nubm5Ki4uVrdu3Rr0cw8cOKDc3Fw9/vjjuuKKK9StWzcVFRV55DsBsA5HZAA0qU8++URFRUW66667FBMT47ZtxIgRmjlzpp5//vk672vRooUmTJigl19+WSEhIRo/frwuuugiV3/M6cTFxalVq1b6y1/+ouTkZOXl5enRRx/1yHcCYB2OyABoUjNnzlRmZmadECMdDTJr165VTk5OnW0RERF65JFHNHr0aA0cOFCRkZH64IMPGvxzg4KCNGfOHGVnZysjI0OTJk2qNzAB8C82c6rrHQHAB8yaNUsTJ05s1C0LADQPHJEBAAB+iyADAAD8FqeWAACA3+KIDAAA8FsEGQAA4LcIMgAAwG8RZAAAgN8iyAAAAL9FkAEAAH6LIAMAAPwWQQYAAPgtggwAAPBb/z/WUsHee83E7QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class Model(object):\n",
    "    \"\"\"\n",
    "     Ridge Regression.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y, alpha=0):\n",
    "        \"\"\"\n",
    "        Fits the ridge regression model to the training data.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p independent variables\n",
    "        y: response variable vector for n examples\n",
    "        alpha: regularization parameter.\n",
    "        \"\"\"\n",
    "      \n",
    "        intercept = np.ones((len(X),1))\n",
    "        X_b = np.c_[intercept,X]\n",
    "        \n",
    "        I = np.identity(X_b.shape[1])\n",
    "        \n",
    "        betha_optim = np.linalg.inv(X_b.T.dot(X_b) + alpha*I).dot(X_b.T).dot(y)\n",
    "        self.betas = betha_optim\n",
    "        return betha_optim\n",
    "   \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the dependent variable of new data using the model.\n",
    "\n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        response variable vector for n examples\n",
    "        \"\"\"\n",
    "       \t# Your code here\n",
    "        X_predictor = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.predictions = X_predictor.dot(self.betas)\n",
    "        return self.predictions\n",
    "\n",
    "    def rmse(self, X, y):\n",
    "        \"\"\"\n",
    "        Returns the RMSE(Root Mean Squared Error) when the model is validated.\n",
    "            \n",
    "        Arguments\n",
    "        ----------\n",
    "        X: nxp matrix of n examples with p covariates\n",
    "        y: response variable vector for n examples\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        RMSE when model is used to predict y\n",
    "        \"\"\"\n",
    "        y_predict = self.predict(X=X)\n",
    "        se = (y_predict-y) ** 2\n",
    "        mse = se.mean()\n",
    "        rmse = mse**0.5\n",
    "        return rmse\n",
    "\n",
    "# Do not change the code below\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "my_model = Model()\n",
    "#! TODO: USE DIFFERENT SETS OF ALPHA VALUES TO FIND THE BEST ALPHA\n",
    "alphas = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "RMSES = []\n",
    "for alpha in alphas:\n",
    "    my_model.fit(X=X_train, y=y_train, alpha=alpha)\n",
    "    rmse = my_model.rmse(X=X_test, y=y_test)\n",
    "    RMSES.append(rmse)\n",
    "    print(f\"Root Mean Squre Error Ridge {rmse}\")\n",
    "    print(my_model.rmse(X=X_test, y=y_test))\n",
    "\n",
    "# plot different alpha values against RMSE\n",
    "plt.plot(alphas, RMSES)\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Alpha vs RMSE')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error 1 Layer MLP 13.113716195628152\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Mean Squared Error loss\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "def mse_loss_derivative(y_true, y_pred):\n",
    "    return -2 * (y_true - y_pred)/ y_true.size\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_o, b_o):\n",
    "    #TODO what is the output of the hidden layer\n",
    "    a_h1 =  np.dot(x, W_h1) + b_h1\n",
    "    #TODO apply activation to the output of the hidden layer \n",
    "    z_h1 = sigmoid(a_h1)\n",
    "    #TODO use ouput of activation as input to the output layer (it is just similar to first layer but we don't apply activation)\n",
    "    y_pred = np.dot(z_h1, W_o) + b_o\n",
    "    return y_pred, z_h1, a_h1\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, W_h1, W_o):\n",
    "    # Derivative of loss with respect to y_pred\n",
    "    #TODO What type of loss function are we using, what is it's derivative ?\n",
    "    dL_dy_pred = mse_loss_derivative(y_true, y_pred)\n",
    "    \n",
    "    \n",
    "    # Gradients for output layer\n",
    "    #TODO remember the output layer is just a dense layer\n",
    "    dL_dW_o = np.dot(z_h1.T, dL_dy_pred)\n",
    "    \n",
    "    dL_db_o = np.sum(dL_dy_pred, axis=0, keepdims=True)\n",
    "\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    #TODO derivative with respect to the output of activation layer\n",
    "    \n",
    "    dL_dz_h1 = np.dot(dL_dy_pred, W_o.T)\n",
    "    \n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    #TODO derivative with respect to the activation layer\n",
    "    dL_da_h1 = dL_dz_h1 * sigmoid_derivative(a_h1)\n",
    "    \n",
    "    # Gradients for hidden layer\n",
    "    dL_dW_h1 = np.dot(x.T, dL_da_h1)\n",
    "    dL_db_h1 = np.sum(dL_da_h1, axis=0, keepdims=True)\n",
    "\n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer_size = 20 # Number of neurons in layer\n",
    "output_size = 1 # predicted variable\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer_size))\n",
    "W_o = np.random.randn(hidden_layer_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "learning_rate = 0.2\n",
    "\n",
    "#To save the weights which give the lowest loss\n",
    "lowest_loss = float('inf')\n",
    "best_weights = None\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1 = forward_pass(X_train, W_h1, b_h1, W_o, b_o)\n",
    "    loss = mse_loss(y_train, y_pred)\n",
    "\n",
    "    if loss < lowest_loss:\n",
    "        lowest_loss = loss\n",
    "        # Save the best weights and biases\n",
    "        best_weights = (W_h1.copy(), b_h1.copy(), W_o.copy(), b_o.copy())\n",
    "\n",
    "    # Backward pass to get gradients\n",
    "    \n",
    "    dL_dW_h1, dL_db_h1, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, W_h1, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases\n",
    "    W_h1 -= learning_rate * dL_dW_h1\n",
    "    b_h1 -= learning_rate * dL_db_h1\n",
    "    W_o -= learning_rate * dL_dW_o\n",
    "    b_o -= learning_rate * dL_db_o\n",
    "\n",
    "\n",
    "W_h1_best, b_h1_best, W_o_best, b_o_best = best_weights\n",
    "y_pred_test, _, _ = forward_pass(X_test, W_h1_best, b_h1_best, W_o_best, b_o_best)\n",
    "se = (y_pred_test-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 1 Layer MLP {rmse}\")\n",
    "\n",
    "#(331, 1)\n",
    "#Root Mean Squre Error 1 Layer MLP 15.69657819926082"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Lets add one more hidden layer (10 Points) \n",
    "### You must write down all gradients and complete the code below to get full bonus points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squre Error 2 Layer MLP 13.113716211174902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Forward pass\n",
    "def forward_pass(x, W_h1, b_h1, W_h2, b_h2, W_o, b_o):\n",
    "    #TODO compute ouput of first hidden layer\n",
    "    a_h1 = np.dot(x, W_h1) + b_h1\n",
    "    z_h1 = sigmoid(a_h1) # apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    #TODO compute ouput of second hidden layer\n",
    "    a_h2 = np.dot(z_h1, W_h2) + b_h2\n",
    "    z_h2 = sigmoid(a_h2) #apply activation to ouputs of first hidden layer\n",
    "    \n",
    "    # compute ouput of output layer, why don't we apply activation ?\n",
    "    y_pred = np.dot(z_h2, W_o) + b_o\n",
    "    \n",
    "    return y_pred, z_h1, a_h1, z_h2, a_h2\n",
    "\n",
    "# Backward pass\n",
    "def backward_pass(x, y_true, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o):\n",
    "    \n",
    "    # Derivative of loss with respect to y_pred, what kind of loss are we using ?\n",
    "    #TODO\n",
    "    dL_dy_pred = mse_loss_derivative(y_true, y_pred)\n",
    "    \n",
    "    # Gradients for output layer\n",
    "    #TODO\n",
    "    dL_dW_o = np.dot(z_h2.T, dL_dy_pred)\n",
    "    #TODO\n",
    "    dL_db_o = np.sum(dL_dy_pred, axis=0, keepdims=True)\n",
    "    \n",
    "    # Derivative of loss with respect to z_h2\n",
    "    #TODO\n",
    "    dL_dz_h2 = np.dot(dL_dy_pred, W_o.T)\n",
    "    \n",
    "    # Derivative of loss with respect to a_h2\n",
    "    #TODO\n",
    "    dL_da_h2 =  dL_dz_h2 * sigmoid_derivative(a_h2)\n",
    "    \n",
    "    # Gradients for second hidden layer\n",
    "    #TODO\n",
    "    dL_dW_h2 =  np.dot(z_h1.T, dL_da_h2)\n",
    "    #TODO\n",
    "    dL_db_h2 =  np.sum(dL_da_h2, axis=0, keepdims=True)\n",
    "    \n",
    "    # Derivative of loss with respect to z_h1\n",
    "    #TODO\n",
    "    dL_dz_h1 =  np.dot(dL_da_h2, W_h2.T)\n",
    "    \n",
    "    # Derivative of loss with respect to a_h1\n",
    "    #TODO\n",
    "    dL_da_h1 =  dL_dz_h1 * sigmoid_derivative(a_h1)\n",
    "    \n",
    "    # Gradients for first hidden layer\n",
    "    #TODO\n",
    "    dL_dW_h1 =  np.dot(x.T, dL_da_h1)\n",
    "    #TODO\n",
    "    dL_db_h1 =  np.sum(dL_da_h1, axis=0, keepdims=True)\n",
    "    \n",
    "    return dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o\n",
    "\n",
    "\n",
    "\n",
    "# Random input and true output (modify these according to your dataset)\n",
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "\n",
    "# Network architecture\n",
    "input_size = 5 # Number of features\n",
    "hidden_layer1_size = 100\n",
    "hidden_layer2_size = 20\n",
    "output_size = 1\n",
    "\n",
    "# Initial random weights and biases for each layer\n",
    "W_h1 = np.random.randn(input_size, hidden_layer1_size) * 0.001\n",
    "b_h1 = np.zeros((1, hidden_layer1_size))\n",
    "W_h2 = np.random.randn(hidden_layer1_size, hidden_layer2_size) * 0.001\n",
    "b_h2 = np.zeros((1, hidden_layer2_size))\n",
    "W_o = np.random.randn(hidden_layer2_size, output_size) * 0.001\n",
    "b_o = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "#Training loop\n",
    "for i in range(200):\n",
    "    # Forward pass to get predictions\n",
    "    y_pred, z_h1, a_h1, z_h2, a_h2 = forward_pass(X_train, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "    #TODO Compute the loss\n",
    "    # Backward pass to get gradients\n",
    "    dL_dW_h1, dL_db_h1, dL_dW_h2, dL_db_h2, dL_dW_o, dL_db_o = backward_pass(X_train, y_train, y_pred, z_h1, a_h1, z_h2, a_h2, W_h1, W_h2, W_o)\n",
    "\n",
    "    # Now you would use the gradients to update the weights and biases for each layer\n",
    "    #TODO\n",
    "    W_h1 -= learning_rate * dL_dW_h1\n",
    "    #TODO\n",
    "    b_h1 -= learning_rate * dL_db_h1\n",
    "    \n",
    "    #TODO\n",
    "    W_h2 -= learning_rate * dL_dW_h2\n",
    "    #TODO\n",
    "    b_h2 -= learning_rate * dL_db_h2\n",
    "    #TODO\n",
    "    W_o -= learning_rate * dL_dW_o\n",
    "    #TODO\n",
    "    b_o -= learning_rate * dL_db_o\n",
    "\n",
    "y_pred, _, _, _, _ = forward_pass(X_test, W_h1, b_h1, W_h2, b_h2, W_o, b_o)\n",
    "# print(y_pred)\n",
    "se = (y_pred-y_test) ** 2\n",
    "mse = se.mean()\n",
    "rmse = mse**0.5\n",
    "print(f\"Root Mean Squre Error 2 Layer MLP {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: absl-py in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (0.5.0)\n",
      "Requirement already satisfied: packaging in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from optree->keras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (73.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
      "  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m467.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: rich in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.5/615.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hUsing cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.67.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wheel-0.45.0-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorboard-data-server, protobuf, opt-einsum, numpy, markdown, grpcio, google-pasta, gast, tensorboard, ml-dtypes, astunparse, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.0\n",
      "    Uninstalling numpy-2.1.0:\n",
      "      Successfully uninstalled numpy-2.1.0\n",
      "  Attempting uninstall: ml-dtypes\n",
      "    Found existing installation: ml_dtypes 0.5.0\n",
      "    Uninstalling ml_dtypes-0.5.0:\n",
      "      Successfully uninstalled ml_dtypes-0.5.0\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.25 gast-0.6.0 google-pasta-0.2.0 grpcio-1.67.1 libclang-18.1.1 markdown-3.7 ml-dtypes-0.4.1 numpy-2.0.2 opt-einsum-3.4.0 protobuf-5.28.3 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 termcolor-2.5.0 werkzeug-3.1.3 wheel-0.45.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 14:21:26.172352: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-11 14:21:26.297861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731327686.328111  448831 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731327686.334264  448831 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-11 14:21:26.379476: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1731327693.648209  448831 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1768 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2050, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1731327694.256563  486188 gpu_backend_lib.cc:579] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/kip/projects/MathsRequiredForAI/EnvMaths/lib/python3.12/site-packages/tensorflow/python/platform/../../cuda\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">401</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │         \u001b[38;5;34m2,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │       \u001b[38;5;34m160,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │       \u001b[38;5;34m160,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m401\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">323,601</span> (1.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m323,601\u001b[0m (1.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">323,601</span> (1.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m323,601\u001b[0m (1.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform',  activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(400, input_dim = 5, kernel_initializer = 'he_uniform', activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(400, kernel_initializer = 'he_uniform',activation = 'relu')) #\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 1s 12ms/step - loss: 59.0233 - val_loss: 38.2307\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 60.5859 - val_loss: 38.7539\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5126 - val_loss: 39.4410\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.5206 - val_loss: 41.2996\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.8750 - val_loss: 36.7995\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.7921 - val_loss: 42.3474\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.6418 - val_loss: 38.9120\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.3855 - val_loss: 34.8828\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5909 - val_loss: 38.9493\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3074 - val_loss: 46.1832\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3386 - val_loss: 48.0670\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 65.7009 - val_loss: 47.4946\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 61.9314 - val_loss: 36.4857\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.1176 - val_loss: 39.1615\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.7488 - val_loss: 45.9356\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1246 - val_loss: 42.3506\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.2204 - val_loss: 38.4740\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.5598 - val_loss: 40.0011\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.5267 - val_loss: 41.0102\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.7600 - val_loss: 41.7729\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 65.8730 - val_loss: 40.3871\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 60.0501 - val_loss: 39.3643\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.8604 - val_loss: 39.8595\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 61.1329 - val_loss: 38.9457\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.6057 - val_loss: 41.5118\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.1321 - val_loss: 35.7910\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1891 - val_loss: 39.2262\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.3964 - val_loss: 36.2836\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.9470 - val_loss: 45.4145\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 55.6735 - val_loss: 37.7727\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.3257 - val_loss: 48.5326\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 64.4544 - val_loss: 43.9641\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 56.6239 - val_loss: 46.4309\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 58.7664 - val_loss: 38.9237\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 59.0663 - val_loss: 40.5066\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 54.7116 - val_loss: 43.4164\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.9722 - val_loss: 44.4749\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.3571 - val_loss: 47.2762\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 57.0204 - val_loss: 53.4110\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.5805 - val_loss: 40.7375\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.9992 - val_loss: 40.2645\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 62.1041 - val_loss: 42.0701\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 63.9374 - val_loss: 46.8338\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 54.1358 - val_loss: 45.7144\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 53.6320 - val_loss: 39.5554\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 71.7491 - val_loss: 54.7381\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 58.1944 - val_loss: 45.8185\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 59.1702 - val_loss: 45.4068\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scale = scaler.fit_transform(X)\n",
    "X_scale = np.asarray(X_scale)\n",
    "y = np.asarray(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "history = model.fit(X_train, y_train,  epochs = 200, validation_data=(X_test, y_test), \n",
    "                    callbacks = EarlyStopping(monitor = 'val_loss',patience = 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step - loss: 45.4068\n",
      "RMSE error using deeper neural network 6.738456462848816\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE error using deeper neural network {model.evaluate(X_test, y_test)**0.5}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvMaths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
