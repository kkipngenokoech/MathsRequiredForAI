{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam ham classifier\n",
    "Consider a text classification problem. In this case, you will try to classify text as either spam or ham. To do this, you will apply concepts of Likelihood, prior, and posterior given a dataset comprising pairs of text and labels. There are two types of labels: 1 (spam) and 0 (ham). Your goal is to create a simple classifier that, when given, determines if the text is spam or ham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kip/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('spam_ham_dataset.csv')\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "df =data\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipykernel_1420/2336932942.py:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  text = [re.sub('\\W+','', word) for word in text]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: enron methanol ; meter # : 988291\n",
      "this is a follow up to the note i gave you on monday , 4 / 3 / 00 { preliminary\n",
      "flow data provided by daren } .\n",
      "please override pop ' s daily volume { presently zero } to reflect daily\n",
      "activity you can obtain from gas control .\n",
      "this change is needed asap for economics purposes .\n",
      "subject enron methanol  meter   988291 follow note gave monday  4  3  00  preliminary flow data provided daren   please override pop  daily volume  presently zero  reflect daily activity obtain gas control  change needed asap economics purposes \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject enron methanol  meter   988291 follow ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject hpl nom january 9  2001  see attached ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject neon retreat ho ho ho   around wonderf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>subject photoshop  windows  office  cheap  mai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject  indian springs deal book teco pvr rev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  subject enron methanol  meter   988291 follow ...   \n",
       "1        2349   ham  subject hpl nom january 9  2001  see attached ...   \n",
       "2        3624   ham  subject neon retreat ho ho ho   around wonderf...   \n",
       "3        4685  spam  subject photoshop  windows  office  cheap  mai...   \n",
       "4        2030   ham  subject  indian springs deal book teco pvr rev...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    # remove special characters\n",
    "    text = [re.sub('\\W+','', word) for word in text]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "txt1 = df.text[0]\n",
    "print(txt1)\n",
    "# remove the filler words\n",
    "txt1 = remove_stopwords(txt1)\n",
    "print(txt1)\n",
    "# apply the function to the entire dataset\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the data into training and testing. We will derive the probabilities from the training data and then use them to predict the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets split the data into training and testing\n",
    "X = df.text\n",
    "y = df.label_num\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) \n",
    "Find the priors. What are the priors in this distribution? i.e find P (ham) and P (spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(ham): 0.7084139264990329\n",
      "P(spam): 0.2915860735009671\n"
     ]
    }
   ],
   "source": [
    "# Calculate priors\n",
    "P_ham = y_train.value_counts()[0] / len(y_train)\n",
    "P_spam = y_train.value_counts()[1] / len(y_train)\n",
    "\n",
    "print(f\"P(ham): {P_ham}\")\n",
    "print(f\"P(spam): {P_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) \n",
    "Find the likelihoods for each word. For each word in the dataset, find the likelihood that the word\n",
    "is in spam and ham. This will represent the conditional probability P (w|spam) and P (w|ham) for\n",
    "w where w âˆˆ V . V is the vocabulary of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods for spam words: [('subject', 0.008392657843131073), ('message', 0.001513326733547301), ('hey', 0.00020808242586275387), ('julie', 1.2611056112894174e-05), ('_', 0.004785895794843339), ('turned', 8.827739279025922e-05), ('18', 0.00030266534670946016), ('high', 0.0008134131192816742), ('school', 8.827739279025922e-05), ('senior', 8.197186473381214e-05)]\n",
      "Likelihoods for ham words: [('subject', 0.01633299063026014), ('april', 0.0013167581802791138), ('activity', 0.0006810818173857485), ('surveys', 4.216220774292729e-05), ('starting', 0.00016216233747279728), ('collect', 3.8918960993471346e-05), ('data', 0.000535135713660231), ('attached', 0.002737300256540818), ('survey', 0.00037945986968634563), ('drives', 1.6216233747279726e-05)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Initialize counters for spam and ham words\n",
    "spam_words = Counter()\n",
    "ham_words = Counter()\n",
    "\n",
    "# Separate spam and ham texts\n",
    "spam_texts = X_train[y_train == 1]\n",
    "ham_texts = X_train[y_train == 0]\n",
    "\n",
    "# Count words in spam and ham texts\n",
    "for text in spam_texts:\n",
    "    for word in text.split():\n",
    "        spam_words[word] += 1\n",
    "\n",
    "for text in ham_texts:\n",
    "    for word in text.split():\n",
    "        ham_words[word] += 1\n",
    "\n",
    "# Calculate total number of words in spam and ham texts\n",
    "total_spam_words = sum(spam_words.values())\n",
    "total_ham_words = sum(ham_words.values())\n",
    "\n",
    "# Calculate likelihoods\n",
    "likelihoods_spam = {word: (count / total_spam_words) for word, count in spam_words.items()}\n",
    "likelihoods_ham = {word: (count / total_ham_words) for word, count in ham_words.items()}\n",
    "\n",
    "print(\"Likelihoods for spam words:\", list(likelihoods_spam.items())[:10])\n",
    "print(\"Likelihoods for ham words:\", list(likelihoods_ham.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) \n",
    "Define a function that, when given a text sequence, returns the probability of the text being in\n",
    "spam. I.e., it returns P (spam|text). Note that this function calculates the likelihood using the\n",
    "Bayes rule. Do the same for ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam|text): -26.23636612458862\n",
      "P(ham|text): -29.381719006113546\n"
     ]
    }
   ],
   "source": [
    "def calculate_posterior(text, priors, likelihoods, total_words):\n",
    "    words = text.split()\n",
    "    posterior = np.log(priors)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in likelihoods:\n",
    "            posterior += np.log(likelihoods[word])\n",
    "        else:\n",
    "            # Apply Laplace smoothing for unseen words\n",
    "            posterior += np.log(1 / (total_words + len(likelihoods)))\n",
    "    \n",
    "    return posterior\n",
    "\n",
    "def predict_spam(text):\n",
    "    P_spam_given_text = calculate_posterior(text, P_spam, likelihoods_spam, total_spam_words)\n",
    "    return P_spam_given_text\n",
    "\n",
    "def predict_ham(text):\n",
    "    P_ham_given_text = calculate_posterior(text, P_ham, likelihoods_ham, total_ham_words)\n",
    "    return P_ham_given_text\n",
    "\n",
    "# Example usage\n",
    "text_example = \"free money now\"\n",
    "print(f\"P(spam|text): {predict_spam(text_example)}\")\n",
    "print(f\"P(ham|text): {predict_ham(text_example)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) \n",
    "Perform inference, i.e., given a string of text, determine if it is ham or spam based on the poste-\n",
    "rior probabilities calculated from the previous steps. Your function will determine the posterior\n",
    "probability of your text being in ham and spam and classify it as being the larger of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text 'free money now' is classified as: spam\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text):\n",
    "    P_spam_given_text = predict_spam(text)\n",
    "    P_ham_given_text = predict_ham(text)\n",
    "    \n",
    "    if P_spam_given_text > P_ham_given_text:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "# Example usage\n",
    "text_example = \"free money now\"\n",
    "classification = classify_text(text_example)\n",
    "print(f\"The text '{text_example}' is classified as: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5)\n",
    "Evaluate the data based on your test set and report the accuracy of your classifier. Your accuracy\n",
    "must be greater than 85%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = X_test.apply(classify_text)\n",
    "\n",
    "# Convert predictions to numerical labels\n",
    "y_pred_num = y_pred.apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_num)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvMaths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
