{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam ham classifier\n",
    "Consider a text classification problem. In this case, you will try to classify text as either spam or ham. To do this, you will apply concepts of Likelihood, prior, and posterior given a dataset comprising pairs of text and labels. There are two types of labels: 1 (spam) and 0 (ham). Your goal is to create a simple classifier that, when given, determines if the text is spam or ham.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kip/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('spam_ham_dataset.csv')\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "df =data\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "/tmp/ipykernel_108095/2336932942.py:4: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  text = [re.sub('\\W+','', word) for word in text]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: enron methanol ; meter # : 988291\n",
      "this is a follow up to the note i gave you on monday , 4 / 3 / 00 { preliminary\n",
      "flow data provided by daren } .\n",
      "please override pop ' s daily volume { presently zero } to reflect daily\n",
      "activity you can obtain from gas control .\n",
      "this change is needed asap for economics purposes .\n",
      "subject enron methanol  meter   988291 follow note gave monday  4  3  00  preliminary flow data provided daren   please override pop  daily volume  presently zero  reflect daily activity obtain gas control  change needed asap economics purposes \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject enron methanol  meter   988291 follow ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject hpl nom january 9  2001  see attached ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject neon retreat ho ho ho   around wonderf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>subject photoshop  windows  office  cheap  mai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>subject  indian springs deal book teco pvr rev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 label                                               text  \\\n",
       "0         605   ham  subject enron methanol  meter   988291 follow ...   \n",
       "1        2349   ham  subject hpl nom january 9  2001  see attached ...   \n",
       "2        3624   ham  subject neon retreat ho ho ho   around wonderf...   \n",
       "3        4685  spam  subject photoshop  windows  office  cheap  mai...   \n",
       "4        2030   ham  subject  indian springs deal book teco pvr rev...   \n",
       "\n",
       "   label_num  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          1  \n",
       "4          0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    # remove special characters\n",
    "    text = [re.sub('\\W+','', word) for word in text]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "txt1 = df.text[0]\n",
    "print(txt1)\n",
    "# remove the filler words\n",
    "txt1 = remove_stopwords(txt1)\n",
    "print(txt1)\n",
    "# apply the function to the entire dataset\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the data into training and testing. We will derive the probabilities from the training data and then use them to predict the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets split the data into training and testing\n",
    "X = df.text\n",
    "y = df.label_num\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4136,), (1035,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5132    subject april activity surveys starting collec...\n",
       "2067    subject message subject hey  julie  _   turned...\n",
       "4716    subject txu fuels  sds nomination may 2001 att...\n",
       "4710    subject  richardson volumes nov 99 dec 99 mete...\n",
       "2268    subject new era online medical care  new era o...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) \n",
    "Find the priors. What are the priors in this distribution? i.e find P (ham) and P (spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(ham): 0.7084139264990329\n",
      "P(spam): 0.2915860735009671\n"
     ]
    }
   ],
   "source": [
    "# Calculate priors\n",
    "P_ham = y_train.value_counts()[0] / len(y_train)\n",
    "P_spam = y_train.value_counts()[1] / len(y_train)\n",
    "\n",
    "print(f\"P(ham): {P_ham}\")\n",
    "print(f\"P(spam): {P_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) \n",
    "Find the likelihoods for each word. For each word in the dataset, find the likelihood that the word\n",
    "is in spam and ham. This will represent the conditional probability P (w|spam) and P (w|ham) for\n",
    "w where w âˆˆ V . V is the vocabulary of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihoods for spam words: [('subject', 0.008392657843131073), ('message', 0.001513326733547301), ('hey', 0.00020808242586275387), ('julie', 1.2611056112894174e-05), ('_', 0.004785895794843339), ('turned', 8.827739279025922e-05), ('18', 0.00030266534670946016), ('high', 0.0008134131192816742), ('school', 8.827739279025922e-05), ('senior', 8.197186473381214e-05)]\n",
      "Likelihoods for ham words: [('subject', 0.01633299063026014), ('april', 0.0013167581802791138), ('activity', 0.0006810818173857485), ('surveys', 4.216220774292729e-05), ('starting', 0.00016216233747279728), ('collect', 3.8918960993471346e-05), ('data', 0.000535135713660231), ('attached', 0.002737300256540818), ('survey', 0.00037945986968634563), ('drives', 1.6216233747279726e-05)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Initialize counters for spam and ham words\n",
    "spam_words = Counter()\n",
    "ham_words = Counter()\n",
    "\n",
    "# Separate spam and ham texts\n",
    "spam_texts = X_train[y_train == 1]\n",
    "ham_texts = X_train[y_train == 0]\n",
    "\n",
    "# Count words in spam and ham texts\n",
    "for text in spam_texts:\n",
    "    for word in text.split():\n",
    "        spam_words[word] += 1\n",
    "\n",
    "for text in ham_texts:\n",
    "    for word in text.split():\n",
    "        ham_words[word] += 1\n",
    "\n",
    "# Calculate total number of words in spam and ham texts\n",
    "total_spam_words = sum(spam_words.values())\n",
    "total_ham_words = sum(ham_words.values())\n",
    "\n",
    "# Calculate likelihoods\n",
    "likelihoods_spam = {word: (count / total_spam_words) for word, count in spam_words.items()}\n",
    "likelihoods_ham = {word: (count / total_ham_words) for word, count in ham_words.items()}\n",
    "\n",
    "print(\"Likelihoods for spam words:\", list(likelihoods_spam.items())[:10])\n",
    "print(\"Likelihoods for ham words:\", list(likelihoods_ham.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) \n",
    "Define a function that, when given a text sequence, returns the probability of the text being in\n",
    "spam. I.e., it returns P (spam|text). Note that this function calculates the likelihood using the\n",
    "Bayes rule. Do the same for ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam|text): -65.67705077597867\n",
      "P(ham|text): -70.95047223507962\n"
     ]
    }
   ],
   "source": [
    "def calculate_posterior(text, priors, likelihoods, total_words):\n",
    "    '''\n",
    "    Calculate the posterior probability of a text being spam or ham\n",
    "    given the text and the likelihoods of spam and ham words.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): the text to classify\n",
    "    priors (tuple): the prior probabilities of spam and ham\n",
    "    likelihoods (dict): the likelihoods of spam and ham words\n",
    "    total_words (int): the total number of words in the training set\n",
    "    \n",
    "    returns:\n",
    "    float: the posterior probability of the text being spam or ham\n",
    "    '''\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Initialize posterior as the log of the priors\n",
    "    posterior = np.log(priors)\n",
    "    \n",
    "    # Calculate the posterior for spam and ham of the text\n",
    "    for word in words:\n",
    "        # If the word is in the likelihoods dictionary, add the log likelihood to the posterior\n",
    "        if word in likelihoods:\n",
    "            posterior += np.log(likelihoods[word])\n",
    "        #  If the word is not in the likelihoods dictionary, apply Laplace smoothing to avoid zero probabilities\n",
    "        else:\n",
    "            # Apply Laplace smoothing for unseen words\n",
    "            posterior += np.log(1 / (total_words + len(likelihoods)))\n",
    "    \n",
    "    # Return the final posterior probability\n",
    "    return posterior\n",
    "\n",
    "def predict_spam(text):\n",
    "    '''\n",
    "    Predict whether a text is spam given the text.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): the text to classify\n",
    "    \n",
    "    returns:\n",
    "    float: the posterior probability of the text being spam\n",
    "    '''\n",
    "    P_spam_given_text = calculate_posterior(text, P_spam, likelihoods_spam, total_spam_words)\n",
    "    return P_spam_given_text\n",
    "\n",
    "def predict_ham(text):\n",
    "    '''\n",
    "    Predict whether a text is ham given the text.\n",
    "    \n",
    "    Parameters:\n",
    "    text (str): the text to classify\n",
    "    \n",
    "    returns:\n",
    "    float: the posterior probability of the text being ham\n",
    "    '''\n",
    "    P_ham_given_text = calculate_posterior(text, P_ham, likelihoods_ham, total_ham_words)\n",
    "    return P_ham_given_text\n",
    "\n",
    "# Example usage\n",
    "text_example = \"Congratulations, spin and win money now\"\n",
    "print(f\"P(spam|text): {predict_spam(text_example)}\")\n",
    "print(f\"P(ham|text): {predict_ham(text_example)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) \n",
    "Perform inference, i.e., given a string of text, determine if it is ham or spam based on the poste-\n",
    "rior probabilities calculated from the previous steps. Your function will determine the posterior\n",
    "probability of your text being in ham and spam and classify it as being the larger of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text 'Congratulations, spin and win money now' is classified as: spam\n"
     ]
    }
   ],
   "source": [
    "def classify_text(text):\n",
    "    P_spam_given_text = predict_spam(text)\n",
    "    P_ham_given_text = predict_ham(text)\n",
    "    \n",
    "    if P_spam_given_text > P_ham_given_text:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "# Example usage\n",
    "text_example = \"Congratulations, spin and win money now\"\n",
    "classification = classify_text(text_example)\n",
    "print(f\"The text '{text_example}' is classified as: {classification}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5)\n",
    "Evaluate the data based on your test set and report the accuracy of your classifier. Your accuracy\n",
    "must be greater than 85%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.00%\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for the test set\n",
    "y_pred = X_test.apply(classify_text)\n",
    "\n",
    "# Convert predictions to numerical labels\n",
    "y_pred_num = y_pred.apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_num)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FROM SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.94%\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Convert text data to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')  # Removing common English stopwords\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)  # Increase max_iter if convergence is not achieved\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.56%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Logistic regression equation: z = X * weights + bias\n",
    "# Sigmoid function: sigmoid(z) = 1 / (1 + exp(-z))\n",
    "\n",
    "\n",
    "class LogisticRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000, init_method=\"random\", lambda_=0.1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.init_method = init_method\n",
    "        self.lambda_ = lambda_  # Regularization strength\n",
    "\n",
    "    def initialize_weights(self, n_features):\n",
    "        if self.init_method == \"random\":\n",
    "            self.weights = np.random.normal(0, 0.01, size=n_features)\n",
    "        elif self.init_method == \"xavier\":\n",
    "            limit = np.sqrt(1 / n_features)\n",
    "            self.weights = np.random.uniform(-limit, limit, size=n_features)\n",
    "        elif self.init_method == \"he\":\n",
    "            limit = np.sqrt(2 / n_features)\n",
    "            self.weights = np.random.normal(0, limit, size=n_features)\n",
    "        else:\n",
    "            self.weights = np.zeros(n_features)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Numerical stability\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.initialize_weights(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            # Linear combination (use sparse matrix's dot method)\n",
    "            z = X.dot(self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(z)\n",
    "\n",
    "            # Gradients with L2 regularization\n",
    "            dw = (1 / n_samples) * X.T.dot(y_pred - y) + (self.lambda_ / n_samples) * self.weights  # Regularization term\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = X.dot(self.weights) + self.bias\n",
    "        probabilities = self.sigmoid(z)\n",
    "        return [1 if prob >= 0.5 else 0 for prob in probabilities]\n",
    "\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)  # Prevent centering for sparse matrices\n",
    "X_train_scaled = scaler.fit_transform(X_train_tfidf)\n",
    "X_test_scaled = scaler.transform(X_test_tfidf)\n",
    "\n",
    "\n",
    "# Train and evaluate\n",
    "regressor = LogisticRegressionScratch(learning_rate=0.001, n_iters=10000, init_method=\"xavier\", lambda_=0.1)\n",
    "regressor.fit(X_train_scaled, y_train)\n",
    "predictions = regressor.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[734   8]\n",
      " [ 38 255]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97       742\n",
      "           1       0.97      0.87      0.92       293\n",
      "\n",
      "    accuracy                           0.96      1035\n",
      "   macro avg       0.96      0.93      0.94      1035\n",
      "weighted avg       0.96      0.96      0.95      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvMaths",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
